{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github README Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"./dataset/data/esol_iupac.csv\"\n",
    "raw_data = pd.read_csv(data_path)\n",
    "\n",
    "def query2IUPAC(text):\n",
    "  try:\n",
    "    '''This function queries the one given molecule name and returns a SMILES string from the record'''\n",
    "    #query the PubChem database\n",
    "    r = requests.get('https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/smiles/' + text + '/property/IUPACName/JSON')\n",
    "    data = r.json()\n",
    "    smi = data[\"PropertyTable\"][\"Properties\"][0][\"IUPACName\"]\n",
    "    return smi\n",
    "  except:\n",
    "    return None\n",
    "\n",
    "# raw_data[\"IUPAC\"] = raw_data[\"SMILES\"].map(lambda sml: query2IUPAC(sml))\n",
    "raw_data = raw_data[[\"IUPAC\", \"measured log(solubility:mol/L)\"]]\n",
    "raw_data = raw_data.dropna()\n",
    "raw_data[50:65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### README example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bolift\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "asktell = bolift.AskTellFewShotTopk(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "asktell.tell(\"3-chloroaniline\", -1.37)\n",
    "asktell.tell(\"nitromethane\", 0.26)\n",
    "asktell.tell(\"1-bromobutane\", -2.43)\n",
    "asktell.tell(\"3-chlorophenol\", -0.7)\n",
    "\n",
    "yhat = asktell.predict(\"penta-1,4-diene\t\")\n",
    "print(yhat.mean(), yhat.std())\n",
    "\n",
    "pool_list = [\n",
    "  \"1,5-dimethylnaphthalene\",\n",
    "  \"2-aminophenol\",\n",
    "  \"1hexa-1,5-diene\",\n",
    "  \"1,1,2,3,4,4-hexachlorobuta-1,3-diene\"\n",
    "]\n",
    "pool=bolift.Pool(pool_list)\n",
    "print(asktell.ask(pool))\n",
    "\n",
    "asktell.tell(\"phenol\", -0.5)\n",
    "yhat = asktell.predict(\"penta-1,4-diene\")\n",
    "print(yhat.mean(), yhat.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bolift\n",
    "from bolift.llm_model import GaussDist, DiscreteDist\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import itertools\n",
    "import os\n",
    "import openai\n",
    "import glob\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "# @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def run_ablation_experiment(asktell, train_data, test_data, system_message=\"\"):\n",
    "    if isinstance(asktell, bolift.AskTellGPR) or isinstance(asktell, bolift.AskTellFinetuning):\n",
    "        i=-1 #Hack to pass the case we have len(train_data) == 1\n",
    "        for i in range(len(train_data)-1):\n",
    "            asktell.tell(train_data.iloc[i, 0], float(train_data.iloc[i, 1]), train=False)\n",
    "        asktell.tell(train_data.iloc[i+1, 0], float(train_data.iloc[i+1, 1]), train=True)\n",
    "    else:\n",
    "        for i in range(len(train_data)):\n",
    "            asktell.tell(train_data.iloc[i, 0], float(train_data.iloc[i, 1]))\n",
    "    x    = []\n",
    "    y    = []\n",
    "    yhat = []\n",
    "    for j in range(len(test_data)):\n",
    "        x.append(test_data.iloc[j, 0])\n",
    "        y.append(float(test_data.iloc[j, 1]))\n",
    "        if isinstance(asktell, bolift.AskTellNearestNeighbor):\n",
    "            yhat.append(asktell.predict(test_data.iloc[j, 0]))\n",
    "        else:\n",
    "            yhat.append(asktell.predict(test_data.iloc[j, 0], system_message=system_message))\n",
    "\n",
    "    x_filter = [xi for xi, yhi in zip(x, yhat)]# if len(yhi.values) > 0]\n",
    "    y_filter = [yi for yi, yhi in zip(y, yhat)]# if len(yhi.values) > 0]\n",
    "    yhat_filter = [yhi for yi, yhi in zip(y, yhat)]# if len(yhi.values) > 0]\n",
    "    return x_filter, y_filter, yhat_filter\n",
    "\n",
    "def save_csv(filename, x, y, yhat, data, model, T, k, N, model_class, tokens):\n",
    "    if not os.path.exists(filename):\n",
    "        f = open(filename, \"w\")\n",
    "        f.write(\"y;yhat;yprobs;data;model;Temperature;k_selected;N_train;model_class;n_tokens;x\\n\")\n",
    "    else:\n",
    "        f = open(filename, \"a\")\n",
    "\n",
    "    for xi, yi, yhi in zip(x, y, yhat):\n",
    "        # print(yi, yhi, data, model, T, k, N, model_class, tokens, xi)\n",
    "        if isinstance(yhi, DiscreteDist):\n",
    "            if len(yhi.values) > 0:\n",
    "                for v,p in zip(yhi.values, yhi.probs):\n",
    "                    f.write(f\"{yi};{v};{p:.4f};{data};{model};{T};{k};{N};{model_class};{tokens};{xi}\\n\")\n",
    "        if isinstance(yhi, GaussDist):\n",
    "            # f.write(f\"{yi};{yhi.mean()};{yhi.std():.4f};{data};{model};{T};{k};{N};{model_class};{tokens};{xi}\\n\")\n",
    "            f.write(f\"{yi};{yhi.mean()};{0};{data};{model};{T};{k};{N};{model_class};{tokens};{xi}\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(data: str, N: int, split=0.8):\n",
    "    match data:\n",
    "        case \"in-house\":\n",
    "            data_path = \"./dataset/data/71023_BO_ready_pool.csv\"\n",
    "            raw_data = pd.read_csv(data_path)\n",
    "\n",
    "            raw_data['Catalyst'] = raw_data['Prompt'].str.extract(r'(\\b[A-Z][a-z]?:[A-Z][a-z]?:[A-Z][a-z]?\\b)')\n",
    "            unique_cat = raw_data['Catalyst'].unique()\n",
    "            c = {c: 0.2+m*(5/len(unique_cat)) for m, c in enumerate(unique_cat)}\n",
    "            raw_data['dummy_Completion'] = raw_data['Catalyst'].apply(lambda x: np.random.normal(c[x], 0.05))\n",
    "\n",
    "            x_name = \"Prompt\"\n",
    "            y_name = \"dummy_Completion\"\n",
    "        case \"ocm\":\n",
    "            data_path = \"./dataset/data/12708_ocm_dataset.csv\"\n",
    "            raw_data = pd.read_csv(data_path, sep=\";\")\n",
    "            raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "            x_name = \"prompt\"\n",
    "            y_name = \"completion\"\n",
    "        case \"biasfree_ocm\":\n",
    "            data_path = \"./dataset/data/bias_free_ocmdataset_p_comp.csv\"\n",
    "            raw_data = pd.read_csv(data_path, sep=\",\")\n",
    "            raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "            x_name = \"prompt\"\n",
    "            y_name = \"completion\"\n",
    "        case \"sol\":\n",
    "            data_path = \"./dataset/data/esol_iupac.csv\"\n",
    "            raw_data = pd.read_csv(data_path)\n",
    "            raw_data = raw_data.dropna()\n",
    "            raw_data = raw_data[[\"IUPAC\", \"measured log(solubility:mol/L)\"]]\n",
    "            x_name = \"IUPAC\"\n",
    "            y_name = \"measured log(solubility:mol/L)\"\n",
    "        case _:\n",
    "            raise ValueError(\"Unknown data\")\n",
    "        \n",
    "    n_data = raw_data.shape[0]\n",
    "    indexes = np.random.choice(n_data, int(n_data), replace=False)\n",
    "    train = np.random.choice(n_data, int(n_data * split), replace=False)\n",
    "    test = np.setdiff1d(np.arange(n_data), train)\n",
    "    test = np.random.choice(test, min(200, len(test)), replace=False) # limiting too large test set to avoid expense with OpenAI requests\n",
    "\n",
    "    if N > len(train):\n",
    "        raise ValueError(f\"N must be less than the training set size. Trainin set size: {len(train)}\")\n",
    "    train_data = raw_data.iloc[train, :].reset_index(drop=True)[:N]\n",
    "    test_data = raw_data.iloc[test, :].reset_index(drop=True)\n",
    "    # print(f\"Dataset size:  \\t{n_data}\")\n",
    "    # print(f\"Training size: \\t{len(train_data)}\")\n",
    "    # print(f\"Test size:     \\t{len(test_data)}\")\n",
    "\n",
    "    return raw_data, train_data, test_data, indexes, x_name, y_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_asktell(model: str, kwargs: dict = {}, pool: bolift.Pool = None, knn: int = 1):\n",
    "    match model:\n",
    "        case \"gpt-3.5-turbo-instruct\":\n",
    "            kwargs['model']=\"gpt-3.5-turbo-instruct\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-3.5-turbo-0125\":\n",
    "            kwargs['model']=\"gpt-3.5-turbo-0125\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4\":\n",
    "            kwargs['model']=\"gpt-4\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4o\":\n",
    "            kwargs['model']=\"gpt-4o-2024-05-13\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4o-mini\":\n",
    "            kwargs['model']=\"gpt-4o-mini-2024-07-18\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4-0125-preview\":\n",
    "            kwargs['model']=\"gpt-4-0125-preview\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"davinci\":\n",
    "            kwargs['model']=\"davinci-002\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpr\":\n",
    "            s = kwargs['selector_k']\n",
    "            kwargs['selector_k'] = 0\n",
    "            kwargs['pool'] = pool if pool else None\n",
    "            kwargs['n_components'] = 32\n",
    "            model = bolift.AskTellGPR(**kwargs) \n",
    "            del kwargs['pool']\n",
    "            del kwargs['n_components']\n",
    "            kwargs['selector_k'] = s\n",
    "            return model\n",
    "        case \"knn\":\n",
    "            s = kwargs['selector_k']\n",
    "            del kwargs['selector_k']\n",
    "            kwargs['knn'] = knn\n",
    "            model = bolift.AskTellNearestNeighbor(**kwargs)\n",
    "            del kwargs['knn']\n",
    "            kwargs['selector_k'] = s\n",
    "            return model\n",
    "        case \"krr\":\n",
    "            kwargs['alpha'] = 0.5\n",
    "            model = bolift.AskTellRidgeKernelRegression(**kwargs)\n",
    "            del kwargs['alpha']\n",
    "            return model\n",
    "        case \"finetune\":\n",
    "            s = kwargs['selector_k']\n",
    "            del kwargs['selector_k']\n",
    "            kwargs['model']=\"gpt-3.5-turbo\"\n",
    "            model = bolift.AskTellFinetuning(**kwargs)\n",
    "            kwargs['selector_k'] = s\n",
    "            return model\n",
    "        case _:\n",
    "            raise ValueError(\"Unknown model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(exp, dataset, system_message=\"\", *args, **kwargs):\n",
    "    T_list = exp['T_list']\n",
    "    k_list = exp['k_list']\n",
    "    N_list = exp['N_list']\n",
    "    models_list = exp['models_list']\n",
    "    out_csv_file = exp['out_csv_file']\n",
    "    model_class=\"topk\"\n",
    "    for T, k, N, model in itertools.product(T_list, k_list, N_list, models_list):\n",
    "        if model == \"gpr\" and N <= 5: continue # GPR needs at least 5 data points\n",
    "        print(f\"Running {dataset} {model_class} regression with T={T}, k={k}, N={N}, model={model}\", end=\" \")\n",
    "        raw_data, train_data, test_data, indexes, x_name, y_name = get_dataset(dataset, N, split=0.8)\n",
    "        kwargs['temperature'] = T\n",
    "        kwargs['selector_k'] = k\n",
    "        pool=None\n",
    "        if model == \"gpr\":\n",
    "            pool = bolift.Pool(raw_data[x_name].tolist())\n",
    "            # pool = bolift.Pool(train_data[x_name].tolist())\n",
    "        asktell = get_asktell(model, kwargs=kwargs, pool=pool, knn=5)\n",
    "        x, y, yhat =  run_ablation_experiment(asktell, train_data, test_data, system_message=system_message)\n",
    "        save_csv(out_csv_file, x, y, yhat, dataset, model, T, k, N, model_class, asktell.tokens_used)\n",
    "        print(\" --> done\")\n",
    "\n",
    "def save_backup(out_csv_file):\n",
    "    if os.path.exists(f\"{out_csv_file}.csv\"):\n",
    "        i = 1\n",
    "        while os.path.exists(f\"{out_csv_file}{i}.csv\"):\n",
    "            i += 1\n",
    "        os.rename(f\"{out_csv_file}.csv\", f\"{out_csv_file}{i}.csv\")\n",
    "\n",
    "def merge_exps(out_csv_file):\n",
    "    save_backup(out_csv_file)\n",
    "    all_files = glob.glob(\"regression-results_exp*.csv\")\n",
    "    exps = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0, sep=\";\")\n",
    "        exps.append(df)\n",
    "\n",
    "    frame = pd.concat(exps, axis=0, ignore_index=True)\n",
    "    frame.to_csv(f\"{out_csv_file}.csv\", index=False, sep=\";\")\n",
    "\n",
    "    for filename in all_files:\n",
    "        os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config experiment\n",
    "\n",
    "These values and datasets should be loaded accordingly with the experiments that are being done.\n",
    "The union of all values considered in our experiments is available below.\n",
    "    \n",
    "```python\n",
    "T_list = [0.01, 0.1, 0.7, 1.0]\n",
    "k_list = [1, 2, 3, 4, 5]\n",
    "N_list = [1, 2, 5, 10, 50, 100, 250, 500, 700, 1000]\n",
    "models_list = [\"gpt-3.5-turbo-instruct\", \"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\", \"gpt-4o\" \"davinci-002\", \"KNN\", \"RNN\", \"GPR\", \"FineTunning\"]\n",
    "out_csv_file = \"regression_results.csv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = {\n",
    "    \"exp_1\" : {\n",
    "        \"T_list\" : [0.05],\n",
    "        \"k_list\" : [1, 2, 5, 10],\n",
    "        \"N_list\" : [1000],\n",
    "        \"models_list\" : [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\", \"gpt-4o\"],\n",
    "        \"out_csv_file\" : \"regression-results1.csv\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in-house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"in-house\"\n",
    "kwargs = dict(\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@\\nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@\\nA:\",\n",
    "    x_formatter=lambda x: f\"experimental procedure: {x}\",\n",
    "    y_name=\"CO STY\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"ocm\"\n",
    "# dataset=\"biasfree_ocm\"\n",
    "kwargs = dict(\n",
    "    # prefix=\"You are a bot who knows chemistry and catalysts. \" \\\n",
    "    #         \"Below, you'll see examples of experimental procedures to synthesize catalysts and the measured C2 yield in a oxidative methane coupling reaction. \" \\\n",
    "    #         \"The following question should be answered with a number and finished with ###\\n\",\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],   \n",
    "        template=\"Q: What is the {y_name} of {x}?@@@\\nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@\\nA:\",\n",
    "    x_formatter=lambda x: f\"experimental procedure: {x}\",\n",
    "    y_name=\"C2 yield\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    ")\n",
    "\n",
    "inv_system_message_path = \"./prompts/inv_prompt_1.txt\"\n",
    "system_message_path = \"./prompts/prompt_1.txt\"\n",
    "\n",
    "exps = {\n",
    "    \"exp_1\" : { # Experiment 1 -> Varying k\n",
    "       \"k_list\" : [1, 2, 10],\n",
    "       \"T_list\" : [0.05],\n",
    "       \"N_list\" : [10],\n",
    "        \"models_list\" : [\"gpt-3.5-turbo-0125\"],\n",
    "        \"out_csv_file\" : \"regression-results_exp1.csv\",\n",
    "    },\n",
    "    \"exp_2\": { # Experiment 2 -> Varying T\n",
    "        \"T_list\" : [0.01, 0.1, 0.5, 1.0, 1.5],\n",
    "        \"k_list\" : [5],\n",
    "        \"N_list\" : [10],\n",
    "        \"models_list\" : [\"gpt-3.5-turbo-0125\"],\n",
    "        \"out_csv_file\" : \"regression-results_exp2.csv\",\n",
    "    },\n",
    "    # \"exp_3\": { # Experiment 3 -> Varying N\n",
    "    #     \"T_list\" : [0.7],\n",
    "    #     \"k_list\" : [5],\n",
    "    #     \"N_list\" : [1, 5, 10, 25, 50, 100, 250, 500],\n",
    "    #     \"models_list\" : [\"gpt-3.5-turbo-0125\", \"krr\", \"knn\", \"gpr\", \"gpt-4o\"],\n",
    "    #     \"out_csv_file\" : \"regression-results_exp3.csv\",\n",
    "    # },\n",
    "    # \"exp_4\": { # Experiment 4 -> Varying the model\n",
    "    #     \"T_list\" : [0.7],\n",
    "    #     \"k_list\" : [5],\n",
    "    #     \"N_list\" : [1000],\n",
    "    #     \"models_list\" : [\"gpt-4o-mini\", \"finetune\", \"gpt-4-0125-preview\"],\n",
    "    #     \"out_csv_file\" : \"regression-results_exp4.csv\",\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solubility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"sol\"\n",
    "kwargs = dict(\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@\\nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@\\nA:\",\n",
    "    x_formatter=lambda x: f\"iupac name {x}\",\n",
    "    y_name=\"measured log solubility in mols per litre\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    ")\n",
    "\n",
    "inv_system_message_path = \"./prompts/inv_prompt_sol.txt\"\n",
    "system_message_path = \"./prompts/prompt_sol.txt\"\n",
    "\n",
    "exps = {\n",
    "    # \"exp_1\" : { # Experiment 1 -> Varying k\n",
    "    #    \"k_list\" : [1, 2, 5, 10],\n",
    "    #    \"T_list\" : [0.05],\n",
    "    #    \"N_list\" : [700],\n",
    "    #     \"models_list\" : [\"gpt-3.5-turbo-0125\"],\n",
    "    #     \"out_csv_file\" : \"regression-results_exp1.csv\",\n",
    "    # },\n",
    "    # \"exp_2\": { # Experiment 2 -> Varying T\n",
    "    #     \"T_list\" : [0.01, 0.1, 0.5, 1.0, 1.5],\n",
    "    #     \"k_list\" : [5],\n",
    "    #     \"N_list\" : [700],\n",
    "    #     \"models_list\" : [\"gpt-3.5-turbo-0125\"],\n",
    "    #     \"out_csv_file\" : \"regression-results_exp2.csv\",\n",
    "    # },\n",
    "    \"exp_3\": { # Experiment 3 -> Varying N\n",
    "        \"T_list\" : [0.7],\n",
    "        \"k_list\" : [5],\n",
    "        \"N_list\" : [1, 5, 10, 25, 50, 100, 250, 500],\n",
    "        \"models_list\" : [\"gpt-4o\"], #[\"gpt-3.5-turbo-0125\", \"krr\", \"knn\", \"gpr\", \"gpt-4o\"],\n",
    "        \"out_csv_file\" : \"regression-results_exp3.csv\",\n",
    "    },\n",
    "    # \"exp_4\": { # Experiment 4 -> Varying the model\n",
    "    #     \"T_list\" : [0.7],\n",
    "    #     \"k_list\" : [5],\n",
    "    #     \"N_list\" : [700],\n",
    "    #     \"models_list\" : [\"knn\", \"gpr\", \"krr\", \"finetune\", \"gpt-4o\", \"gpt-4o-mini\", \"gpt-4-0125-preview\"],\n",
    "    #     \"out_csv_file\" : \"regression-results_exp4.csv\",\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "from botorch.exceptions.warnings import InputDataWarning\n",
    "\n",
    "warnings.simplefilter('ignore', SparseEfficiencyWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "warnings.simplefilter('ignore', InputDataWarning)\n",
    "\n",
    "if os.path.exists(system_message_path):\n",
    "    with open(system_message_path, \"r\") as f:\n",
    "        system_message = f.read()\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Running experiment {i+1}\")\n",
    "    for e in exps:\n",
    "        exp = exps[e]\n",
    "        run_experiment(exp, dataset, system_message, **kwargs)\n",
    "\n",
    "    out_csv_file = \"./out/regression/sol-gpt4o-\"\n",
    "    merge_exps(out_csv_file)\n",
    "    save_backup(out_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./out/regression/ocm-regression-2.csv\", sep=\";\")\n",
    "df = df.query(\"model == 'gpt-3.5-turbo-0125' & N_train == 1000 & k_selected == 5 & Temperature == 1.5\")\n",
    "# df = df.query(\"model == 'gpt-4o' & N_train == 700 & k_selected == 5 & Temperature == 0.7  & x == '9,10-dimethylanthracene'\")\n",
    "# df = df.query(\"model == 'gpt-3.5-turbo-0125' & N_train == 700 & k_selected == 5 & Temperature == 0.7\")\n",
    "# df = df.query(\"model == 'gpt-4-0125-preview' & N_train == 700 & k_selected == 5 & Temperature == 0.7\")\n",
    "# df = df.query(\"model == 'gpr' & N_train == 700 & k_selected == 5 & Temperature == 0.7\")\n",
    "# print(df)\n",
    "\n",
    "# plot y by yhat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(data=df, x=\"y\", y=\"yhat\", hue=\"model\", style=\"model\")\n",
    "# plt.plot([-10, 2], [-10, 2], color=\"black\", linestyle=\"--\")\n",
    "plt.plot([0, 20], [0, 20], color=\"black\", linestyle=\"--\")\n",
    "plt.xlabel(\"True value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "# plt.ylim(-10, 10)\n",
    "# plt.xlim(-10, 5)\n",
    "plt.legend().remove()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import uncertainty_toolbox as uct\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as font_manager\n",
    "urllib.request.urlretrieve('https://github.com/google/fonts/raw/main/ofl/ibmplexmono/IBMPlexMono-Regular.ttf', 'IBMPlexMono-Regular.ttf')\n",
    "fe = font_manager.FontEntry(\n",
    "    fname='IBMPlexMono-Regular.ttf',\n",
    "    name='plexmono')\n",
    "font_manager.fontManager.ttflist.append(fe)\n",
    "plt.rcParams.update({'axes.facecolor':'#f5f4e9',\n",
    "            'grid.color' : '#AAAAAA',\n",
    "            'axes.edgecolor':'#333333',\n",
    "            'figure.facecolor':'#FFFFFF',\n",
    "            'axes.grid': False,\n",
    "            'axes.prop_cycle':   plt.cycler('color', plt.cm.Dark2.colors),\n",
    "            'font.family': fe.name,\n",
    "            'font.size': 16,\n",
    "            'figure.figsize': (3.5,3.5 / 1.2),\n",
    "            'ytick.left': True,\n",
    "            'xtick.bottom': True\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def mse(y, pred):\n",
    "  # return np.mean((y-pred)**2)\n",
    "  return mean_squared_error(y, pred)\n",
    "\n",
    "def mae(y, pred):\n",
    "  return mean_absolute_error(y, pred)\n",
    "\n",
    "def r2(y, pred):\n",
    "  return r2_score(y, pred)\n",
    "\n",
    "def corr(y, pred):\n",
    "  return np.corrcoef(y, pred)[0,1]\n",
    "\n",
    "def acc(y, pred, threshold):\n",
    "  acc = sum((abs(pred - y)<threshold))/len(pred)\n",
    "  return acc\n",
    "\n",
    "def log_likelihood(y, pred, ystd, eps=0):\n",
    "  y = np.array(y)\n",
    "  pred = np.array(pred)\n",
    "  ystd = np.array(ystd)\n",
    "  yvar = ystd**2 + eps\n",
    "  neg_ll = 0.5 * (np.log(yvar) + ((y - pred)**2 / yvar))\n",
    "  return np.sum(neg_ll)/len(y)\n",
    "\n",
    "def select_df(df, data, k, T, model, model_class, N):\n",
    "  config = {'k': k,\n",
    "            'T': T,\n",
    "            'data': data,\n",
    "            'model': model,\n",
    "            'model_class': model_class,\n",
    "            'N': N,\n",
    "            }\n",
    "\n",
    "  q = f\"\"\n",
    "  if T != 'any':\n",
    "    q += f\"Temperature=={T} and \"\n",
    "  if k != 'any':  \n",
    "    q+= f\"k_selected=={k} and \"\n",
    "  if model != 'any': \n",
    "    q+= f\"model=='{model}' and \"\n",
    "  q+= f\"model_class=='{model_class}' and \"\n",
    "  if N != 'any':\n",
    "    q+= f\"N_train=={N} and \"\n",
    "  q += f\"data=='{data}'\"\n",
    "  sel = df.query(q)\n",
    "  if sel.empty:\n",
    "    raise ValueError(f\"Dataframe is empty for the configuration {config}\")\n",
    "  return sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parities(df, data_property, data_range, nrows, ncols, data=None, k=None, T=None, model=None, model_class=None, N=None, axis_name=None, calibration=None, recal_ind=1, out_name=None, GPR=False):\n",
    "  config = {'k': k,\n",
    "            'T': T,\n",
    "            'data': data,\n",
    "            'model': model,\n",
    "            'model_class': model_class,\n",
    "            'N': N,\n",
    "            }\n",
    "\n",
    "  if sum([1 for i in config.values() if i is None]) > 1:\n",
    "    raise ValueError(\"Only the property being varied in data_range can be passed as None.\")\n",
    "\n",
    "  if nrows*ncols < len(data_range):\n",
    "    raise ValueError('''There's not enough space to plat all data in data_range.\n",
    "    Decrease the size of data_range or increase ncols/nrows.''')\n",
    "\n",
    "  fig, axs = plt.subplots(nrows=nrows, ncols=ncols, sharey=False, figsize=(6*ncols, 6*nrows), dpi=300)\n",
    "  for i, p in enumerate(data_range):\n",
    "    config[data_property] = p\n",
    "    y=[]\n",
    "    yhat=[]\n",
    "    yprob=[]\n",
    "    ax = axs if ncols*nrows == 1 else axs.flatten()[i]\n",
    "\n",
    "    df_sel = select_df(df, **config)\n",
    "\n",
    "    for prompt in df_sel['x'].unique():\n",
    "        y.append(df_sel[df_sel['x']==prompt]['y'].unique()[0])\n",
    "        # max_p = np.argmax(df_sel[df_sel['y']==d]['yprobs'].values)\n",
    "        yhat.append(df_sel[df_sel['x']==prompt]['yhat'].values)\n",
    "        yprob.append(df_sel[df_sel['x']==prompt]['yprobs'].values)\n",
    "    yprobs = [yhi.std() for yhi in yhat]\n",
    "    if config['model'] in [\"gpr\", 'knn', 'krr']:\n",
    "        ymeans = np.array([yhi.mean() for yhi in yhat])\n",
    "        ystds = np.array([ypi.mean() for ypi in yprob])\n",
    "    else:\n",
    "        ymeans = np.array([\n",
    "                  np.sum(yhi*ypi) if len(yhi)>1 else yhi.mean()\n",
    "                  for yhi,ypi in zip(yhat, yprob)\n",
    "                ])\n",
    "        ystds = np.array([\n",
    "                  np.sqrt(np.sum((yhi-ymi)**2*ypi)) if np.sum((yhi-ymi)**2*ypi)>1 else 0.1 #ypi.mean()\n",
    "                  for yhi,ypi,ymi in zip(yhat, yprob, ymeans)\n",
    "                ])\n",
    "\n",
    "    if calibration:\n",
    "        if calibration == \"scaling_factor\":\n",
    "          std_scaling = uct.recalibration.optimize_recalibration_ratio(ymeans[:recal_ind], ystds[:recal_ind], np.array(y[:recal_ind]), criterion=\"miscal\")\n",
    "          ystds = ystds * std_scaling\n",
    "        elif calibration == \"isotonic\":\n",
    "          exp_props, obs_props= uct.metrics_calibration.get_proportion_lists_vectorized(ymeans[:recal_ind], ystds[:recal_ind], np.array(y[:recal_ind]))\n",
    "          recal_model = uct.recalibration.iso_recal(exp_props, obs_props)\n",
    "          recal_bounds = uct.metrics_calibration.get_prediction_interval(ymeans, ystds, 0.95, recal_model)\n",
    "          ystds=np.array([ymeans - recal_bounds.lower,\n",
    "                 recal_bounds.upper - ymeans])\n",
    "\n",
    "    ax.plot(y,y)\n",
    "    if config['model'] not in ['knn', 'krr']:\n",
    "      ax.errorbar(y, \n",
    "                  ymeans, \n",
    "                  yerr=ystds,\n",
    "                  fmt='.', color='gray', alpha=0.3)\n",
    "    ax.scatter(\n",
    "        y, ymeans, s=6, alpha=1, color=f\"C{i}\"\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"{data_property}={p}\")\n",
    "\n",
    "    lim = (min(y)-1, max(y)+1)\n",
    "    \n",
    "    if config['model'] in [\"krr\", \"knn\"]:\n",
    "       metrics = {\n",
    "          \"accuracy\": uct.metrics.get_all_accuracy_metrics(ymeans, np.array(y), verbose=False)\n",
    "                  }\n",
    "    else:\n",
    "        metrics = uct.metrics.get_all_metrics(ymeans, ystds, np.array(y), verbose=False)\n",
    "    ax.text(lim[0] + 0.1*(max(y)-min(y)), lim[1] - 1*0.1*(max(y)-min(y)), f\"$(\\\\uparrow$)correlation = {metrics['accuracy']['corr']:.3f}\")\n",
    "    if config['model'] not in [\"krr\", \"knn\"]:\n",
    "      ax.text(lim[0] + 0.1*(max(y)-min(y)), lim[1] - 2*0.1*(max(y)-min(y)), f\"$(\\\\downarrow$)neg-ll = {metrics['scoring_rule']['nll']:.3f}\")\n",
    "    ax.text(lim[0] + 0.1*(max(y)-min(y)), lim[1] - 3*0.1*(max(y)-min(y)), f\"$(\\\\downarrow$)MAE = {metrics['accuracy']['mae']:.3f}\")\n",
    "\n",
    "    ax.set_ylim(lim[0],lim[1])\n",
    "    ax.set_xlim(lim[0],lim[1])\n",
    "\n",
    "    ax.set_xlabel(f\"measured {axis_name}\")\n",
    "    if (i%ncols==0):\n",
    "      ax.set_ylabel(f\"predicted {axis_name}\")\n",
    "\n",
    "  plt.tight_layout()\n",
    "  if (out_name):\n",
    "    plt.savefig(f\"./out/figs/{out_name}\", dpi=300)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ablation(df, data_property, data_range, nrows, ncols, data=None, k=None, T=None, model=None, model_class=None, N=None, out_name=None, GPR=False):\n",
    "  config = {'k': k,\n",
    "            'T': T,\n",
    "            'data': data,\n",
    "            'model': model,\n",
    "            'model_class': model_class,\n",
    "            'N': N,\n",
    "            }\n",
    "\n",
    "  MAE_list = []\n",
    "  RMSE_list = []\n",
    "  r_list = []\n",
    "  nll_list = []\n",
    "  prop_list = []\n",
    "  for i, p in enumerate(data_range):\n",
    "    config[data_property] = p\n",
    "    y=[]\n",
    "    yhat=[]\n",
    "    yprobs=[]\n",
    "    yprob=[]\t\n",
    "\n",
    "    df_sel = select_df(df, **config)\n",
    "\n",
    "    for prompt in df_sel['x'].unique():\n",
    "        y.append(df_sel[df_sel['x']==prompt]['y'].unique()[0])\n",
    "        # max_p = np.argmax(df_sel[df_sel['y']==d]['yprobs'].values)\n",
    "        yhat.append(df_sel[df_sel['x']==prompt]['yhat'].values)\n",
    "        yprob.append(df_sel[df_sel['x']==prompt]['yprobs'].values)\n",
    "    yprobs = [yhi.std() for yhi in yhat]\n",
    "    if GPR:\n",
    "        ymeans = np.array([yhi.mean() for yhi in yhat])\n",
    "        ystds = np.array([ypi.mean() for ypi in yprob])\n",
    "    else:\n",
    "        ymeans = np.array([\n",
    "                  np.sum(yhi*ypi) if len(yhi)>1 else yhi.mean()\n",
    "                  for yhi,ypi in zip(yhat, yprob)\n",
    "                ])\n",
    "        ystds = np.array([\n",
    "                  np.sqrt(np.sum((yhi-ymi)**2*ypi)) if yhi.std()>1 else ypi.mean()\n",
    "                  for yhi,ypi,ymi in zip(yhat, yprob, ymeans)\n",
    "                ])\n",
    "\n",
    "    metrics = uct.metrics.get_all_metrics(ymeans, ystds, np.array(y), verbose=False)\n",
    "    r_list.append(metrics['accuracy']['corr'])\n",
    "    RMSE_list.append(metrics['accuracy']['rmse'])\n",
    "    MAE_list.append(metrics['accuracy']['mae'])\n",
    "    nll_list.append(metrics['scoring_rule']['nll'])\n",
    "    prop_list.append(p)\n",
    "    print(f\"{model_class}(N:{config['N']}/k:{config['k']}/T:{config['T']}) => RMSE: | MAE: {MAE_list[-1]} | r: {r_list[-1]} | nll: {nll_list[-1]}\")\n",
    "\n",
    "  fig, axs = plt.subplots(nrows=nrows, ncols=ncols, sharey=False, figsize=(4*ncols, 4*nrows), dpi=300)\n",
    "  \n",
    "  axs[0].plot(prop_list, MAE_list)\n",
    "  axs[0].set_xlabel(data_property)\n",
    "  axs[0].set_ylabel(\"$\\\\rightarrow$MAE\")\n",
    "  \n",
    "  axs[1].plot(prop_list, r_list)\n",
    "  axs[1].set_xlabel(data_property)\n",
    "  axs[1].set_ylabel(\"$\\\\leftarrow$correlation\")\n",
    "\n",
    "  axs[2].plot(prop_list, nll_list)\n",
    "  axs[2].set_xlabel(data_property)\n",
    "  axs[2].set_yscale('log')\n",
    "  axs[2].set_ylabel(\"$\\\\rightarrow$negative log-likelihood\")\n",
    "\n",
    "  plt.tight_layout()\n",
    "  # plt.show()\n",
    "  if (out_name):\n",
    "    plt.savefig(f\"./out/figs/{out_name}\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_ablation_data(df, data_property, data_range, data=None, k=None, T=None, model=None, model_class=None, N=None, GPR=False):\n",
    "  config = {'k': k,\n",
    "          'T': T,\n",
    "          'data': data,\n",
    "          'model': model,\n",
    "          'model_class': model_class,\n",
    "          'N': N,\n",
    "          }\n",
    "   \n",
    "  MAE_list = []\n",
    "  RMSE_list = []\n",
    "  r_list = []\n",
    "  nll_list = []\n",
    "  prop_list = []\n",
    "  for i, p in enumerate(data_range):\n",
    "    config[data_property] = p\n",
    "    y=[]\n",
    "    yhat=[]\n",
    "    yprobs=[]\n",
    "    yprob=[]\n",
    "\n",
    "    df_sel = select_df(df, **config)\n",
    "\n",
    "    for prompt in df_sel['x'].unique():\n",
    "        y.append(df_sel[df_sel['x']==prompt]['y'].unique()[0])\n",
    "        # max_p = np.argmax(df_sel[df_sel['y']==d]['yprobs'].values)\n",
    "        yhat.append(df_sel[df_sel['x']==prompt]['yhat'].values)\n",
    "        yprob.append(df_sel[df_sel['x']==prompt]['yprobs'].values)\n",
    "    yprobs = [yhi.std() for yhi in yhat]\n",
    "    if GPR:\n",
    "        ymeans = np.array([yhi.mean() for yhi in yhat])\n",
    "        ystds = np.array([ypi.mean() for ypi in yprob])\n",
    "    else:\n",
    "        ymeans = np.array([\n",
    "                  np.sum(yhi*ypi) if len(yhi)>1 else yhi.mean()\n",
    "                  for yhi,ypi in zip(yhat, yprob)\n",
    "                ])\n",
    "        ystds = np.array([\n",
    "                  np.sqrt(np.sum((yhi-ymi)**2*ypi)) if np.sum((yhi-ymi)**2*ypi)>0 else 10\n",
    "                  for yhi,ypi,ymi in zip(yhat, yprob, ymeans)\n",
    "                ])\n",
    "\n",
    "    if model_class in [\"KRR\", \"KNN\"]:\n",
    "       metrics = {\n",
    "          \"accuracy\": uct.metrics.get_all_accuracy_metrics(ymeans, np.array(y), verbose=False)\n",
    "                  }\n",
    "    else:\n",
    "      metrics = uct.metrics.get_all_metrics(ymeans, ystds, np.array(y), verbose=False)\n",
    "      nll_list.append(metrics['scoring_rule']['nll'])\n",
    "    r_list.append(metrics['accuracy']['corr'])\n",
    "    RMSE_list.append(metrics['accuracy']['rmse'])\n",
    "    MAE_list.append(metrics['accuracy']['mae'])\n",
    "    prop_list.append(p)\n",
    "    with open(\"Table.tex\", \"a\") as t:\n",
    "      # t.write(f\"{config['data']}&{model_class}&{model}&{config['T']}&{config['k']}&{config['N']}&{RMSE_list[-1]}&{MAE_list[-1]}&{r_list[-1]}&{nll_list[-1] if nll_list else '-'}&\\\\\\\\\\n\")\n",
    "      t.write(f\"{model_class}(N:{config['N']}/k:{config['k']}/T:{config['T']}) => RMSE: | MAE: {MAE_list[-1]} | r: {r_list[-1]} | nll: {nll_list[-1]}\\n\")\n",
    "   \n",
    "  return prop_list, MAE_list, r_list, nll_list\n",
    "\n",
    "def create_sub_ablation(axs, df, lims, data_property, data_range, color='C0', data=None, k=None, T=None, model=None, model_class=None, N=None, label=False, GPR=False):\n",
    "  config = {'k': k,\n",
    "            'T': T,\n",
    "            'data': data,\n",
    "            'model': model,\n",
    "            'model_class': model_class,\n",
    "            'N': N,\n",
    "            }\n",
    "\n",
    "  prop_list, MAE_list, r_list, nll_list = get_sub_ablation_data(df, data_property, data_range, **config, GPR=GPR)\n",
    "\n",
    "  for ax in axs:\n",
    "    ax.label_outer()\n",
    "\n",
    "  if label:\n",
    "    if model_class==\"GPR-BOT\":\n",
    "      axs[0].plot(prop_list, MAE_list, label=\"GPR\", color=color)\n",
    "    else:\n",
    "      axs[0].plot(prop_list, MAE_list, label=model_class, color=color)\n",
    "  else:\n",
    "    axs[0].plot(prop_list, MAE_list, color=color)\n",
    "  axs[0].set_ylabel(\"MAE\\n$\\leftarrow$\")\n",
    "  axs[0].set_ylim(lims[0])\n",
    "  axs[0].set_label(model_class)\n",
    "  \n",
    "  axs[1].plot(prop_list, r_list, color=color)\n",
    "  axs[1].set_xlabel(data_property)\n",
    "  axs[1].set_ylabel(\"r\\n$\\\\rightarrow$\")\n",
    "  axs[1].set_ylim(lims[1])\n",
    "  axs[1].set_label(model_class)\n",
    "\n",
    "  if False: #model_class not in [\"KRR\", \"KNN\"]:\n",
    "    axs[2].plot(prop_list, nll_list, color=color)\n",
    "    axs[2].set_xlabel(data_property)\n",
    "    axs[2].set_yscale('log')\n",
    "    axs[2].set_ylabel(\"neg-ll\\n$\\leftarrow$\")\n",
    "    # axs[1].set_label(model_class)\n",
    "\n",
    "  for ax in axs:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub_shadow(axs, df, lims, data_property, data_range, color='C0', data=None, k=None, T=None, model=None, model_class=None, N=None, label=False, GPR=False):\n",
    "\n",
    "    all_MAE = []\n",
    "    all_r = []\n",
    "    config = {'k': k,\n",
    "            'T': T,\n",
    "            'data': data,\n",
    "            'model': model,\n",
    "            'model_class': model_class,\n",
    "            'N': N,\n",
    "            }\n",
    "\n",
    "    for i in range(1, 6):  # Loop through the 5 CSV files\n",
    "        df = pd.read_csv(f\"./out/regression/{data}-regression-{i}.csv\", sep=';')\n",
    "        df_sel = select_df(df, **config)\n",
    "\n",
    "        prop_list, MAE_list, r_list, nll_list = get_sub_ablation_data(df_sel, data_property, data_range, data=data, k=k, T=T, model=model, model_class=model_class, N=N)\n",
    "\n",
    "        all_MAE.append(MAE_list)\n",
    "        all_r.append(r_list)\n",
    "\n",
    "    all_MAE = np.array(all_MAE)\n",
    "    all_r = np.array(all_r)\n",
    "\n",
    "    # Compute average MAE and r\n",
    "    avg_MAE = np.mean(all_MAE, axis=0)\n",
    "    avg_r = np.mean(all_r, axis=0)\n",
    "\n",
    "    # Compute min and max for uncertainty region\n",
    "    min_MAE = np.min(all_MAE, axis=0)\n",
    "    max_MAE = np.max(all_MAE, axis=0)\n",
    "    min_r = np.min(all_r, axis=0)\n",
    "    max_r = np.max(all_r, axis=0)\n",
    "\n",
    "    # Plot average curve with uncertainty region\n",
    "    axs[0].fill_between(prop_list, min_MAE, max_MAE, color=color, alpha=0.1)\n",
    "    if label:\n",
    "        axs[0].plot(prop_list, avg_MAE, color=color, label=model)\n",
    "    else:\n",
    "        axs[0].plot(prop_list, avg_MAE, color=color)\n",
    "    axs[0].set_ylabel(\"MAE\\n$\\leftarrow$\")\n",
    "    axs[0].set_ylim(lims[0])\n",
    "\n",
    "    axs[1].fill_between(prop_list, min_r, max_r, color=color, alpha=0.1)\n",
    "    axs[1].plot(prop_list, avg_r, color=color)\n",
    "    axs[1].set_ylabel(\"r\\n$\\\\rightarrow$\")\n",
    "    axs[1].set_ylim(lims[1])\n",
    "\n",
    "    axs[1].set_xlabel(data_property)\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_parity_data():\n",
    "   pass\n",
    "   \n",
    "\n",
    "def create_sub_parity(ax, df_sel, axis_name, model_class, lim=[-1,1], color='gray', GPR=False, title=None, calibration=None, recal_ind=0):\n",
    "    y=[]\n",
    "    yhat=[]\n",
    "    yprob=[]\n",
    "    for prompt in df_sel['x'].unique():\n",
    "        y.append(df_sel[df_sel['x']==prompt]['y'].unique()[0])\n",
    "        # max_p = np.argmax(df_sel[df_sel['y']==d]['yprobs'].values)\n",
    "        yhat.append(df_sel[df_sel['x']==prompt]['yhat'].values)\n",
    "        yprob.append(df_sel[df_sel['x']==prompt]['yprobs'].values)\n",
    "    yprobs = [yhi.std() for yhi in yhat]\n",
    "    if GPR:\n",
    "        ymeans = np.array([yhi.mean() for yhi in yhat])\n",
    "        ystds = np.array([ypi.mean() for ypi in yprob])\n",
    "    else:\n",
    "        ymeans = np.array([\n",
    "                  np.sum(yhi*ypi) if len(yhi)>1 else yhi.mean()\n",
    "                  for yhi,ypi in zip(yhat, yprob)\n",
    "                ])\n",
    "        ystds = np.array([\n",
    "                  np.sqrt(np.sum((yhi-ymi)**2*ypi)) if len(yhi)>1 else ypi.mean()\n",
    "                  for yhi,ypi,ymi in zip(yhat, yprob, ymeans)\n",
    "                ])\n",
    "        # hack to fix uncertainties in finetuned model. 3.559 is the training set (N=1000) std\n",
    "        ystds = np.array([ysi if ysi!=10 else 3.559 for ysi in ystds]) \n",
    "  \n",
    "    if calibration:\n",
    "        if calibration == \"scaling_factor\":\n",
    "          std_scaling = uct.recalibration.optimize_recalibration_ratio(ymeans[:recal_ind], ystds[:recal_ind], np.array(y[:recal_ind]),\n",
    "                                                                        criterion=\"miscal\")\n",
    "          ystds = ystds * std_scaling\n",
    "        elif calibration == \"isotonic\":\n",
    "          exp_props, obs_props= uct.metrics_calibration.get_proportion_lists_vectorized(ymeans[:recal_ind], ystds[:recal_ind], np.array(y[:recal_ind]))\n",
    "          recal_model = uct.recalibration.iso_recal(exp_props, obs_props)\n",
    "          recal_bounds = uct.metrics_calibration.get_prediction_interval(ymeans, ystds, 0.95, recal_model)\n",
    "          ystds=np.array([ymeans - recal_bounds.lower,\n",
    "                 recal_bounds.upper - ymeans])\n",
    "\n",
    "    if model_class in [\"KRR\", \"KNN\"] or calibration==\"isotonic\":\n",
    "       metrics = {\n",
    "          \"accuracy\": uct.metrics.get_all_accuracy_metrics(ymeans, np.array(y), verbose=False)\n",
    "                  }\n",
    "    else:\n",
    "      metrics = uct.metrics.get_all_metrics(ymeans, ystds, np.array(y), verbose=False)\n",
    "      ax.text(lim[0] + 0.1*(max(y)-min(y)), lim[1] - 2*0.1*(max(y)-min(y)), f\"$(\\\\downarrow$)neg-ll = {metrics['scoring_rule']['nll']:.3f}\")\n",
    "    ax.text(lim[0] + 0.1*(max(y)-min(y)), lim[1] - 1*0.1*(max(y)-min(y)), f\"$(\\\\uparrow$)correlation = {metrics['accuracy']['corr']:.3f}\")\n",
    "    ax.text(lim[0] + 0.1*(max(y)-min(y)), lim[1] - 3*0.1*(max(y)-min(y)), f\"$(\\\\downarrow$)MAE = {metrics['accuracy']['mae']:.3f}\")\n",
    "\n",
    "    # with open(\"Table.tex\", \"a\") as t:\n",
    "    #   t.write(f\"{config['data']}&{model_class}&{model}&{config['T']}&{config['k']}&{config['N']}&{RMSE_list[-1]}&{MAE_list[-1]}&{r_list[-1]}&{nll_list[-1] if nll_list else '-'}&\\\\\\\\\\n\")\n",
    "    # print(f\"{model_class}(N:{config['N']}/k:{config['k']}/T:{config['T']}) => RMSE: | MAE: {MAE_list[-1]} | r: {r_list[-1]} | nll: {nll_list[-1]}\")\n",
    "    print(metrics['accuracy']['rmse'])\n",
    "\n",
    "    ax.set_xlabel(f\"measured {axis_name}\")\n",
    "    ax.set_ylabel(f\"predicted {axis_name}\")\n",
    "    ax.set_ylim(lim[0],lim[1])\n",
    "    ax.set_xlim(lim[0],lim[1])\n",
    "    ax.set_xticks(np.arange(lim[0],lim[1]+0.1,4.0))\n",
    "\n",
    "    if title:\n",
    "      ax.set_title(title)\n",
    "\n",
    "    ax.plot(y,y)\n",
    "    ax.plot(lim,lim)\n",
    "    if model_class not in [\"KRR\", \"KNN\"]:\n",
    "      ax.errorbar(y, \n",
    "                  ymeans, \n",
    "                  yerr=ystds,\n",
    "                  fmt='.', color='gray', alpha=0.2)\n",
    "    ax.scatter(\n",
    "        y, ymeans, s=6, alpha=1, color=color\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"ocm\"\n",
    "df = pd.read_csv(f\"./out/regression/{data}-regression-1.csv\", sep=';')\n",
    "\n",
    "df.groupby(['Temperature', 'data', 'k_selected', 'model_class', \"N_train\", \"model\"]).size().reset_index().sort_values(by=[\"model_class\", \"Temperature\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting values for tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for k in df.groupby(['Temperature', 'data', 'k_selected', 'model_class', \"N_train\", \"model\"]).size().items():\n",
    "  config = {\n",
    "    \"T\": k[0][0],\n",
    "    \"data\": k[0][1],\n",
    "    \"k\": k[0][2],\n",
    "    \"model_class\": k[0][3],\n",
    "    \"N\": k[0][4],\n",
    "    \"model\": k[0][5]\n",
    "    }\n",
    "  if (config['model'] != 'gpt-3.5-turbo-0125'):\n",
    "    continue\n",
    "  MAE_list = []\n",
    "  r_list = []\n",
    "  nll_list = []\n",
    "  for d in range(1,6):\n",
    "    df_data = pd.read_csv(f\"./out/regression/{data}-regression-{d}.csv\", sep=';')\n",
    "    _, MAE, r, nll = (get_sub_ablation_data(\n",
    "      df_data,\n",
    "      'T',\n",
    "      [config['T']],\n",
    "      **config\n",
    "    ))\n",
    "    MAE_list.append(MAE)\n",
    "    r_list.append(r)\n",
    "    nll_list.append(nll)\n",
    "  print(MAE_list)\n",
    "  MAE = f\"{np.mean(MAE_list):.3f} $\\pm$ {np.std(MAE_list):.3f}\"\n",
    "  r = f\"{np.mean(r_list):.3f} $\\pm$ {np.std(r_list):.3f}\"\n",
    "  nll = f\"{np.mean(nll_list):.3f} $\\pm$ {np.std(nll_list):.3f}\"\n",
    "  print(f\"{config['data']} & {config['model']} & {config['N']} & {config['k']} & {config['T']} & {MAE} & {r} & {nll} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# OCM\n",
    "k1  = [[3.08722], [3.0156400000000003], [2.66832], [2.5487100000000003], [2.85807]]\n",
    "k2  = [[2.27448], [2.64859], [2.57459], [2.1902000000000004], [2.12223]]\n",
    "k5  = [[2.4873600000000002], [2.4017700000000004], [2.4468], [2.8101100000000003], [2.5714699999999997]]\n",
    "k10 = [[2.6273400000000002], [2.36714], [2.64697], [2.5841], [2.50021]]\n",
    "\n",
    "T005= [[2.4873600000000002], [2.4017700000000004], [2.4468], [2.8101100000000003], [2.5714699999999997]]\n",
    "T01 = [[2.20297], [2.35719], [2.15844], [2.39386], [2.15257]]\n",
    "T05 = [[2.53152], [2.49365], [2.24776], [2.21711], [2.4835499999999997]]\n",
    "T07 = [[2.10888], [2.0664500000000006], [2.2181300000000004], [2.242], [2.4587]]\n",
    "T10 = [[2.19661], [2.20766], [2.2488], [2.33529], [2.2955099999999997]]\n",
    "T15 = [[3.2579315], [2.6343252], [3.058765529389575], [2.3256620000000003], [2.6990995]]\n",
    "\n",
    "N1 = [[2.84001], [3.4121799999999998], [3.1467888000000004], [6.42114], [3.5210100000000004]]\n",
    "N5 = [[2.6150300000000004], [3.2023600000000005], [3.14168], [3.05443], [2.5317000000000003]]\n",
    "N10 = [[5.16586], [4.16284], [2.90985], [4.08717], [3.6658600000000003]]\n",
    "N25 = [[2.81358], [2.9238299999999997], [2.8903700000000003], [3.2587200000000003], [3.05932]]\n",
    "N50 = [[3.0023400000000002], [2.9152], [3.1592600000000006], [3.0529], [2.7846699999999998]]\n",
    "N100 = [[3.0850400000000002], [3.01153], [3.054440000000001], [3.2039], [3.10955]]\n",
    "N250 = [[2.84294], [2.5029700000000004], [2.9302499999999996], [2.94748], [3.4290999999999996]]\n",
    "N500 = [[2.79453], [2.7104200000000005], [2.85055], [2.38013], [2.5443100000000003]]\n",
    "N1000 = [[2.10888], [2.0664500000000006], [2.2181300000000004], [2.242], [2.4587]]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Dependence on k. Tests for H0: Both samples have the same mean.\")\n",
    "print(f\"test      | d1  | d2   | p-value | Results\")\n",
    "print(f\"t-test    | k=1 | k=2  | {stats.ttest_ind(k1, k2).pvalue[0]:.5f} | {stats.ttest_ind(k1, k2)}\")\n",
    "print(f\"t-test    | k=2 | k=5 | {stats.ttest_ind(k1, k10).pvalue[0]:.5f} | {stats.ttest_ind(k1, k10)}\")\n",
    "print(f\"t-test    | k=2 | k=10 | {stats.ttest_ind(k2, k10).pvalue[0]:.5f} | {stats.ttest_ind(k2, k10)}\")\n",
    "print(f\"t-test    | k=5 | k=10 | {stats.ttest_ind(k5, k10).pvalue[0]:.5f} | {stats.ttest_ind(k5, k10)}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Dependence on T. Tests for H0: Both samples have the same mean.\")\n",
    "print(f\"test      | d1  | d2   | p-value | Results\")\n",
    "print(f\"t-test    | T=0.05 | T=0.1  | {stats.ttest_ind(T005, T01).pvalue[0]:.5f} | {stats.ttest_ind(T005, T01)}\")\n",
    "print(f\"t-test    | T=0.1  | T=0.5  | {stats.ttest_ind(T01, T05).pvalue[0]:.5f} | {stats.ttest_ind(T01, T05)}\")\n",
    "print(f\"t-test    | T=0.5  | T=0.7  | {stats.ttest_ind(T05, T07).pvalue[0]:.5f} | {stats.ttest_ind(T05, T07)}\")\n",
    "print(f\"t-test    | T=0.7  | T=1.0  | {stats.ttest_ind(T07, T10).pvalue[0]:.5f} | {stats.ttest_ind(T07, T10)}\")\n",
    "print(f\"t-test    | T=1.0  | T=1.5  | {stats.ttest_ind(T10, T15).pvalue[0]:.5f} | {stats.ttest_ind(T10, T15)}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Dependence on N. Tests for H0: Both samples have the same mean.\")\n",
    "print(f\"test      | d1  | d2   | p-value | Results\")\n",
    "print(f\"t-test    | N=1 | N=5  | {stats.ttest_ind(N1, N5).pvalue[0]:.5f} | {stats.ttest_ind(N1, N5)}\")\n",
    "print(f\"t-test    | N=5 | N=10 | {stats.ttest_ind(N5, N10).pvalue[0]:.5f} | {stats.ttest_ind(N5, N10)}\")\n",
    "print(f\"t-test    | N=10 | N=25 | {stats.ttest_ind(N10, N25).pvalue[0]:.5f} | {stats.ttest_ind(N10, N25)}\")\n",
    "print(f\"t-test    | N=25 | N=50 | {stats.ttest_ind(N25, N50).pvalue[0]:.5f} | {stats.ttest_ind(N25, N50)}\")\n",
    "print(f\"t-test    | N=50 | N=100 | {stats.ttest_ind(N50, N100).pvalue[0]:.5f} | {stats.ttest_ind(N50, N100)}\")\n",
    "print(f\"t-test    | N=100 | N=250 | {stats.ttest_ind(N100, N250).pvalue[0]:.5f} | {stats.ttest_ind(N100, N250)}\")\n",
    "print(f\"t-test    | N=250 | N=500 | {stats.ttest_ind(N250, N500).pvalue[0]:.5f} | {stats.ttest_ind(N250, N500)}\")\n",
    "print(f\"t-test    | N=500 | N=1k | {stats.ttest_ind(N500, N1000).pvalue[0]:.5f} | {stats.ttest_ind(N500, N1000)}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Dependence on k. Tests for H0: Both samples originates from the same distribution\")\n",
    "print(f\"test      | d1  | d2   | p-value | Results\")\n",
    "print(f\"KS-test   | k=1 | k=2  | {stats.ks_2samp(k1, k2).pvalue[0]:.5f} | {stats.ks_2samp(k1, k2)}\")\n",
    "print(f\"KS-test   | k=1 | k=10 | {stats.ks_2samp(k1, k10).pvalue[0]:.5f} | {stats.ks_2samp(k1, k10)}\")\n",
    "print(f\"KS-test   | k=2 | k=10 | {stats.ks_2samp(k2, k10).pvalue[0]:.5f} | {stats.ks_2samp(k2, k10)}\")\n",
    "print(f\"KS-test   | k=5 | k=10 | {stats.ks_2samp(k5, k10).pvalue[0]:.5f} | {stats.ks_2samp(k5, k10)}\")\n",
    "print(f\"M-W U-test| k=1 | k=2  | {stats.mannwhitneyu(k1, k2).pvalue[0]:.5f} | {stats.mannwhitneyu(k1, k2)}\")\n",
    "print(f\"M-W U-test| k=1 | k=10 | {stats.mannwhitneyu(k1, k10).pvalue[0]:.5f} | {stats.mannwhitneyu(k1, k10)}\")\n",
    "print(f\"M-W U-test| k=2 | k=10 | {stats.mannwhitneyu(k2, k10).pvalue[0]:.5f} | {stats.mannwhitneyu(k2, k10)}\")\n",
    "print(f\"M-W U-test| k=5 | k=10 | {stats.mannwhitneyu(k5, k10).pvalue[0]:.5f} | {stats.mannwhitneyu(k5, k10)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = [15.88, 15.77, 16.07, 18.9, 19.34]\n",
    "gpt4 = [12.67, 17.1, 20.88, 20.15, 19.43]\n",
    "gpr =  [18.191, 18.192, 18.193, 18.194, 18.195]\n",
    "\n",
    "print(stats.ttest_ind(gpr, gpt4))\n",
    "print(stats.ks_2samp(gpr, gpt4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = \"ocm\"\n",
    "\n",
    "files_path = \"./out/regression\"\n",
    "max_N = 1000 if data==\"ocm\" else 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_ablation_fig():\n",
    "  fig = plt.figure(figsize=(16,6), constrained_layout=True)\n",
    "  subfigs = fig.subfigures(1,3, wspace=0.1, hspace=0.1)\n",
    "\n",
    "  lims = [(0,5),(0,1),(-1,1)]\n",
    "\n",
    "  sub00 = subfigs[0].subplots(2,1, sharex=True, sharey=False)\n",
    "  df1 = pd.read_csv(f\"{files_path}/{data}-regression-1.csv\", sep=';')\n",
    "  d01 = select_df(df1, data=data, k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N='any')\n",
    "  create_sub_ablation(sub00, d01, lims, 'N', sorted(d01['N_train'].unique()), data=data, color='C1', k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N=None, label=False)\n",
    "  df2 = pd.read_csv(f\"{files_path}/{data}-regression-2.csv\", sep=';')\n",
    "  d02 = select_df(df2, data=data, k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N='any')\n",
    "  create_sub_ablation(sub00, d02, lims, 'N', sorted(d02['N_train'].unique()), data=data, color='C2', k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N=None, label=False)\n",
    "  df3 = pd.read_csv(f\"{files_path}/{data}-regression-3.csv\", sep=';')\n",
    "  d03 = select_df(df3, data=data, k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N='any')\n",
    "  create_sub_ablation(sub00, d03, lims, 'N', sorted(d03['N_train'].unique()), data=data, color='C3', k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N=None, label=False)\n",
    "  df4 = pd.read_csv(f\"{files_path}/{data}-regression-4.csv\", sep=';')\n",
    "  d04 = select_df(df4, data=data, k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N='any')\n",
    "  create_sub_ablation(sub00, d04, lims, 'N', sorted(d04['N_train'].unique()), data=data, color='C4', k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N=None, label=False)\n",
    "  df5 = pd.read_csv(f\"{files_path}/{data}-regression-5.csv\", sep=';')\n",
    "  d05 = select_df(df5, data=data, k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N='any')\n",
    "  create_sub_ablation(sub00, d05, lims, 'N', sorted(d05['N_train'].unique()), data=data, color='C5', k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N=None, label=False)\n",
    "\n",
    "  sub02 = subfigs[1].subplots(2,1, sharex=True, sharey=False)\n",
    "  df1 = pd.read_csv(f\"{files_path}/{data}-regression-1.csv\", sep=';')\n",
    "  d01 = select_df(df1, data=data, k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_ablation(sub02, d01, lims, 'k', sorted(d01['k_selected'].unique()), data=data, color='C1', k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "  df2 = pd.read_csv(f\"{files_path}/{data}-regression-2.csv\", sep=';')\n",
    "  d02 = select_df(df2, data=data, k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_ablation(sub02, d02, lims, 'k', sorted(d02['k_selected'].unique()), data=data, color='C2', k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "  df3 = pd.read_csv(f\"{files_path}/{data}-regression-3.csv\", sep=';')\n",
    "  d03 = select_df(df3, data=data, k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_ablation(sub02, d03, lims, 'k', sorted(d03['k_selected'].unique()), data=data, color='C3', k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "  df4 = pd.read_csv(f\"{files_path}/{data}-regression-4.csv\", sep=';')\n",
    "  d04 = select_df(df4, data=data, k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_ablation(sub02, d04, lims, 'k', sorted(d04['k_selected'].unique()), data=data, color='C4', k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "  df5 = pd.read_csv(f\"{files_path}/{data}-regression-5.csv\", sep=';')\n",
    "  d05 = select_df(df5, data=data, k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_ablation(sub02, d05, lims, 'k', sorted(d05['k_selected'].unique()), data=data, color='C5', k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "\n",
    "  sub03 = subfigs[2].subplots(2,1, sharex=True, sharey=False)\n",
    "  df1 = pd.read_csv(f\"{files_path}/{data}-regression-1.csv\", sep=';')\n",
    "  d01 = select_df(df1, data=data, k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_ablation(sub03, d01, lims, 'T', sorted(d01['Temperature'].unique()), data=data, color='C1', k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "  df2 = pd.read_csv(f\"{files_path}/{data}-regression-2.csv\", sep=';')\n",
    "  d02 = select_df(df2, data=data, k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_ablation(sub03, d02, lims, 'T', sorted(d02['Temperature'].unique()), data=data, color='C2', k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "  df3 = pd.read_csv(f\"{files_path}/{data}-regression-3.csv\", sep=';')\n",
    "  d03 = select_df(df3, data=data, k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_ablation(sub03, d03, lims, 'T', sorted(d03['Temperature'].unique()), data=data, color='C3', k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "  df4 = pd.read_csv(f\"{files_path}/{data}-regression-4.csv\", sep=';')\n",
    "  d04 = select_df(df4, data=data, k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_ablation(sub03, d04, lims, 'T', sorted(d04['Temperature'].unique()), data=data, color='C4', k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "  df5 = pd.read_csv(f\"{files_path}/{data}-regression-5.csv\", sep=';')\n",
    "  d05 = select_df(df5, data=data, k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_ablation(sub03, d05, lims, 'T', sorted(d05['Temperature'].unique()), data=data, color='C5', k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "\n",
    "\n",
    "  # fig.legend(loc='upper center', bbox_to_anchor=(0.5 ,0),\n",
    "  #           fancybox=True, shadow=True, ncol=6)\n",
    "\n",
    "  # plt.tight_layout()\n",
    "  plt.savefig(f\"./out/figs/{data}_metrics\", dpi=300, bbox_inches='tight')\n",
    "  plt.show()\n",
    "\n",
    "make_ablation_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def make_sub_shadow_fig():\n",
    "  fig = plt.figure(figsize=(16,6), constrained_layout=True)\n",
    "  subfigs = fig.subfigures(1,3, wspace=0.1, hspace=0.1) \n",
    "\n",
    "  lims = [(0,5),(0,1),(-1,1)]\n",
    "\n",
    "  sub00 = subfigs[0].subplots(2,1, sharex=True, sharey=False)\n",
    "  df0 = pd.read_csv(f\"{files_path}/{data}-regression-1.csv\", sep=';')\n",
    "  d00 = select_df(df0, data=data, k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N='any')\n",
    "  create_sub_shadow(sub00, d00, lims, 'N', sorted(d00['N_train'].unique()), data=data, color='C1', k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N='any', label=False)\n",
    "\n",
    "  sub01 = subfigs[1].subplots(2,1, sharex=True, sharey=False)\n",
    "  d01 = select_df(df0, data=data, k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_shadow(sub01, d01, lims, 'k', sorted(d01['k_selected'].unique()), data=data, color='C1', k='any', T=0.05, model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "\n",
    "  sub02 = subfigs[2].subplots(2,1, sharex=True, sharey=False)\n",
    "  d02 = select_df(df0, data=data, k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N)\n",
    "  create_sub_shadow(sub02, d02, lims, 'T', sorted(d02['Temperature'].unique()), data=data, color='C1', k=5, T='any', model='gpt-3.5-turbo-0125', model_class='topk', N=max_N, label=False)\n",
    "\n",
    "  # fig.legend(loc='upper center', bbox_to_anchor=(0.5 ,0),\n",
    "  #           fancybox=True, shadow=True, ncol=6)\n",
    "\n",
    "  # plt.tight_layout()\n",
    "  plt.savefig(f\"./out/figs/{data}_metrics_shaded\", dpi=300, bbox_inches='tight')\n",
    "  plt.show()\n",
    "\n",
    "make_sub_shadow_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def make_shadow_models_fig():\n",
    "\n",
    "  fig = plt.figure(figsize=(4,6), constrained_layout=True)\n",
    "  subfigs = fig.subfigures(1,1, wspace=0.1, hspace=0.1) \n",
    "\n",
    "  lims = [(0,5),(0,1),(-1,1)]\n",
    "\n",
    "  sub00 = subfigs.subplots(2,1, sharex=True, sharey=False)\n",
    "  df0 = pd.read_csv(f\"{files_path}/{data}-regression-1.csv\", sep=';')\n",
    "  d00 = select_df(df0, data=data, k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N='any')\n",
    "  create_sub_shadow(sub00, d00, lims, 'N', sorted(d00['N_train'].unique()), data=data, color='C1', k=5, T=0.7, model='gpt-3.5-turbo-0125', model_class='topk', N='any', label=True)\n",
    "  d01 = select_df(df0, data=data, k=5, T=0.7, model='gpt-4o', model_class='topk', N='any')\n",
    "  create_sub_shadow(sub00, d01, lims, 'N', sorted(d01['N_train'].unique()), data=data, color='C2', k=5, T=0.7, model='gpt-4o', model_class='topk', N='any', label=True)\n",
    "  d02 = select_df(df0, data=data, k=5, T=0.7, model='knn', model_class='topk', N='any')\n",
    "  create_sub_shadow(sub00, d02, lims, 'N', sorted(d02['N_train'].unique()), data=data, color='C3', k=5, T=0.7, model='knn', model_class='topk', N='any', label=True)\n",
    "  d03 = select_df(df0, data=data, k=5, T=0.7, model='krr', model_class='topk', N='any')\n",
    "  d03 = d03[d03['N_train']!=1]\n",
    "  create_sub_shadow(sub00, d03, lims, 'N', sorted(d03['N_train'].unique()), data=data, color='C4', k=5, T=0.7, model='krr', model_class='topk', N='any', label=True)\n",
    "  d04 = select_df(df0, data=data, k=5, T=0.7, model='gpr', model_class='topk', N='any')\n",
    "  create_sub_shadow(sub00, d04, lims, 'N', sorted(d04['N_train'].unique()), data=data, color='C5', k=5, T=0.7, model='gpr', model_class='topk', N='any', label=True)\n",
    "\n",
    "  fig.legend(loc='upper center', bbox_to_anchor=(0.5 ,0),\n",
    "            fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "  # plt.tight_layout()\n",
    "  plt.savefig(f\"./out/figs/{data}_metrics_models\", dpi=300, bbox_inches='tight')\n",
    "  plt.show()\n",
    "\n",
    "make_shadow_models_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{files_path}/{data}-regression-2.csv\", sep=';')\n",
    "df.columns\n",
    "\n",
    "plot_data = df[(df['data'] == data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(df, \n",
    "              'model', \n",
    "              [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\", \"gpt-4o\", \"krr\", \"finetune\", \"gpr\"],\n",
    "              nrows=2, ncols=3,\n",
    "              data=data, \n",
    "              k=5,\n",
    "              T=0.7,\n",
    "              model=None,\n",
    "              model_class='topk',\n",
    "              N=max_N,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\" if data==\"ocm\" else \"LogS solubility\",\n",
    "              out_name=f\"{data}_parities_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(df, \n",
    "              'N', \n",
    "              [1,50,250,max_N],\n",
    "              nrows=1, ncols=4,\n",
    "              data=data, \n",
    "              k=5, \n",
    "              T=0.7, \n",
    "              model='gpt-3.5-turbo-0125', \n",
    "              model_class='topk', \n",
    "              N=None,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\" if data==\"ocm\" else \"LogS solubility\",\n",
    "              out_name=f\"{data}_par_gpt3.5_N.png\")\n",
    "\n",
    "plot_parities(df, \n",
    "              'k', \n",
    "              [1,2,5,10],\n",
    "              nrows=1, ncols=4,\n",
    "              data=data, \n",
    "              k=None, \n",
    "              T=0.05, \n",
    "              model='gpt-3.5-turbo-0125', \n",
    "              model_class='topk', \n",
    "              N=max_N,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\" if data==\"ocm\" else \"LogS solubility\",\n",
    "              out_name=f\"{data}_par_gpt3.5_k.png\")\n",
    "\n",
    "plot_parities(df, \n",
    "              'T', \n",
    "              [0.01,0.1,0.5,0.7,1.0,1.5],\n",
    "              nrows=2, ncols=3,\n",
    "              data=data, \n",
    "              k=5, \n",
    "              T=None, \n",
    "              model='gpt-3.5-turbo-0125', \n",
    "              model_class='topk', \n",
    "              N=max_N,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\" if data==\"ocm\" else \"LogS solubility\",\n",
    "              out_name=f\"{data}_par_gpt3.5_T.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{files_path}/{data}-regression-5.csv\", sep=';')\n",
    "df.columns\n",
    "\n",
    "c2_data = df[(df['data'] == 'ocm')]\n",
    "# c2_data.groupby(['Temperature', 'data', 'k_selected', 'model_class', \"N_train\", \"model\"]).size().reset_index().sort_values(by=[\"model_class\", \"Temperature\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(df, \n",
    "              'model', \n",
    "              [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\", \"gpt-4o\", \"knn\", \"finetune\", \"gpr\"],\n",
    "              nrows=2, ncols=3,\n",
    "              data='ocm', \n",
    "              k=5,\n",
    "              T=0.7,\n",
    "              model=None,\n",
    "              model_class='topk',\n",
    "              N=1000,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\",\n",
    "              out_name=\"ocm_parities_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(df, \n",
    "              'N', \n",
    "              [1,50,250,1000], #sorted(c2_data[(c2_data['model_class']==\"topk\") & (c2_data['model']==\"text-curie-001\")]['N_train'].unique()), \n",
    "              nrows=1, ncols=4,\n",
    "              data='ocm', \n",
    "              k=5, \n",
    "              T=0.7, \n",
    "              model='gpt-3.5-turbo-0125', \n",
    "              model_class='topk', \n",
    "              N=None,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\",\n",
    "              out_name=\"ocm_par_gpt3.5_N.png\")\n",
    "\n",
    "plot_parities(df, \n",
    "              'k', \n",
    "              [1,2,5,10], #sorted(c2_data[(c2_data['model_class']==\"topk\") & (c2_data['model']==\"text-curie-001\")]['k_selected'].unique()), \n",
    "              nrows=1, ncols=4,\n",
    "              data='ocm', \n",
    "              k=None, \n",
    "              T=0.05, \n",
    "              model='gpt-3.5-turbo-0125', \n",
    "              model_class='topk', \n",
    "              N=1000,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\",\n",
    "              out_name=\"ocm_par_gpt3.5_k.png\")\n",
    "\n",
    "plot_parities(df, \n",
    "              'T', \n",
    "              [0.01,0.1,0.5,0.7,1.0,1.5], #sorted(c2_data[(c2_data['model_class']==\"topk\") & (c2_data['model']==\"text-curie-001\")]['k_selected'].unique()), \n",
    "              nrows=2, ncols=3,\n",
    "              data='ocm', \n",
    "              k=5, \n",
    "              T=None, \n",
    "              model='gpt-3.5-turbo-0125', \n",
    "              model_class='topk', \n",
    "              N=1000,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\",\n",
    "              out_name=\"ocm_par_gpt3.5_T.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(c2_data, \n",
    "              'N', \n",
    "              [1000], #sorted(c2_data[(c2_data['model_class']==\"GPR-BOT\") & (c2_data['model']==\"text-ada-001\")]['N_train'].unique()), \n",
    "              nrows=1, ncols=4,\n",
    "              data='C2', \n",
    "              k=5, \n",
    "              T=0.7, \n",
    "              model='gpr', \n",
    "              model_class='topk', \n",
    "              N=None,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\",\n",
    "              out_name=\"par_C2_GPR_N.png\",\n",
    "              GPR=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(c2_data, \n",
    "              'N', \n",
    "              [5, 50, 500, 1000], #sorted(c2_data[(c2_data['model_class']==\"GPR-BOT\") & (c2_data['model']==\"text-ada-001\")]['N_train'].unique()), \n",
    "              nrows=1, ncols=4,\n",
    "              data='ocm', \n",
    "              k=5, \n",
    "              T=0.7, \n",
    "              model='krr', \n",
    "              model_class='topk', \n",
    "              N=None,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\",\n",
    "              out_name=\"par_ocm_KNN_N.png\",\n",
    "              GPR=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(c2_data, \n",
    "              'N', \n",
    "              [50,100,250,1000], #sorted(c2_data[(c2_data['model_class']==\"finetune\")]['N_train'].unique()), \n",
    "              nrows=1, ncols=4,\n",
    "              data='C2', \n",
    "              k=0, \n",
    "              T=0.05, \n",
    "              model='any', \n",
    "              model_class='finetune', \n",
    "              N=None,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\",\n",
    "              out_name=\"par_C2_FT_N.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_ablation(df, \n",
    "#               'N', \n",
    "#               sorted(c2_data[(c2_data['model_class']==\"finetune\")]['N_train'].unique()), \n",
    "#               nrows=1, ncols=3,\n",
    "#               data='C2',\n",
    "#               k=0,\n",
    "#               T=0.05,\n",
    "#               model='any',\n",
    "#               model_class='finetune',\n",
    "#               N=None,\n",
    "#               out_name=\"ablation_C2_FT_N_ada.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solubility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./out/regression/sol-regression-1.csv\", sep=';')\n",
    "df.columns\n",
    "\n",
    "iupac_sol_data = df[(df['data'] == 'sol')]\n",
    "iupac_sol_data.groupby(['Temperature', 'data','k_selected', 'model_class', \"N_train\", \"model\"]).size().reset_index().sort_values(by=[\"model_class\", \"Temperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(df, \n",
    "              'model', \n",
    "              [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\", \"gpt-4o\", \"knn\", \"krr\", \"gpr\"],\n",
    "              nrows=2, ncols=3,\n",
    "              data='sol', \n",
    "              k=5,\n",
    "              T=0.7,\n",
    "              model=None,\n",
    "              model_class='topk',\n",
    "              N=700,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"LogS solubility\",\n",
    "              out_name=\"parities-sol-models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(iupac_sol_data, \n",
    "              'N', \n",
    "              [1,10,250,700], #sorted(iupac_sol_data[iupac_sol_data['model_class']==\"topk\"]['N_train'].unique()), \n",
    "              nrows=1, ncols=4,\n",
    "              data='sol', \n",
    "              k=5, \n",
    "              T=0.7, \n",
    "              model='gpt-3.5-turbo-0125', \n",
    "              model_class='topk',\n",
    "              N=None,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"LogS solubility\",\n",
    "              out_name=\"par_sol_gpt-3.5_N.png\")\n",
    "\n",
    "plot_parities(iupac_sol_data, \n",
    "              'k', \n",
    "              [1,2,5,10], #sorted(iupac_sol_data[iupac_sol_data['model_class']==\"multi\"]['N_train'].unique()), \n",
    "              nrows=1, ncols=4,\n",
    "              data='sol', \n",
    "              k=None,\n",
    "              T=0.05, \n",
    "              model='gpt-3.5-turbo-0125', \n",
    "              model_class='topk', \n",
    "              N=700,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"LogS solubility\",\n",
    "              out_name=\"par_sol_gpt-3.5_k.png\")\n",
    "\n",
    "plot_parities(iupac_sol_data, \n",
    "              'T', \n",
    "              [0.05, 0.5, 0.7, 1.0], #sorted(iupac_sol_data[iupac_sol_data['model_class']==\"multi\"]['N_train'].unique()), \n",
    "              nrows=1, ncols=4,\n",
    "              data='sol', \n",
    "              k=5,\n",
    "              T=None, \n",
    "              model='gpt-3.5-turbo-0125', \n",
    "              model_class='topk', \n",
    "              N=700,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"LogS solubility\",\n",
    "              out_name=\"par_sol_gpt-3.5_T.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(iupac_sol_data, \n",
    "              'N', \n",
    "              [1,10,250,700], #sorted(iupac_sol_data[(iupac_sol_data['model_class']==\"GPR-BOT\") & (iupac_sol_data['model']==\"text-ada-001\")]['N_train'].unique()), \n",
    "              nrows=1, ncols=4,\n",
    "              data='iupac-sol', \n",
    "              k=32, \n",
    "              T=0.05, \n",
    "              model='text-ada-001', \n",
    "              model_class='GPR-BOT', \n",
    "              N=None,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"LoS solubility\",\n",
    "              out_name=\"par_sol_GPR_N.png\",\n",
    "              GPR=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ablation(df, \n",
    "              'N', \n",
    "              sorted(iupac_sol_data[(iupac_sol_data['model_class']==\"GPR-BOT\") & (iupac_sol_data['model']==\"text-ada-001\")]['N_train'].unique()), \n",
    "              nrows=1, ncols=3,\n",
    "              data='iupac-sol',\n",
    "              k=32,\n",
    "              T=0.05,\n",
    "              model='text-ada-001',\n",
    "              model_class='GPR-BOT',\n",
    "              N=None,\n",
    "              out_name=\"ablation_sol_GPR_N_ada.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(iupac_sol_data, \n",
    "              'N', \n",
    "              [5,10,25,50,100,250,500,700], #sorted(c2_data[(c2_data['model_class']==\"GPR-BOT\") & (c2_data['model']==\"text-ada-001\")]['N_train'].unique()), \n",
    "              nrows=2, ncols=4,\n",
    "              data='iupac-sol', \n",
    "              k=1, \n",
    "              T=0.05, \n",
    "              model='text-ada-001', \n",
    "              model_class='KNN', \n",
    "              N=None,\n",
    "              calibration=None,\n",
    "              recal_ind=300,\n",
    "              axis_name=\"C2 yield\",\n",
    "              out_name=\"par_sol_KNN_N.png\",\n",
    "              GPR=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parities(iupac_sol_data, \n",
    "              'N', \n",
    "              [50, 250, 700],#sorted(iupac_sol_data[iupac_sol_data['model_class']==\"finetune\"]['N_train'].unique()), \n",
    "              nrows=1, ncols=3,\n",
    "              data='iupac-sol', \n",
    "              k=0, \n",
    "              T=0.05, \n",
    "              model='any', \n",
    "              N=None,\n",
    "              axis_name=\"LogS solubility\",\n",
    "              out_name=\"par_sol_FT_N.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ablation(iupac_sol_data, \n",
    "              'N', \n",
    "              sorted(iupac_sol_data[iupac_sol_data['model_class']==\"finetune\"]['N_train'].unique()), \n",
    "              nrows=1, ncols=3,\n",
    "              data='iupac-sol',\n",
    "              k=0,\n",
    "              T=0.05,\n",
    "              model='any',\n",
    "              model_class='finetune',\n",
    "              N=None,\n",
    "              out_name=\"ablation_sol_FT_N_ada.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrated MMR vs Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14,12), constrained_layout=True)\n",
    "for ax in axs.flat:\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "d00 = select_df(df, data=\"C2\", k=5, T=0.7, model='text-curie-001', model_class='topk_NN', N=1000)\n",
    "lim_c2 = (min(d00['y']), max(d00['y']))\n",
    "lim_c2 = (-2, 25)\n",
    "text_anchor = sum(lim_c2)/len(lim_c2)\n",
    "create_sub_parity(axs[0], d00, 'C2 yield', lim=lim_c2, model_class=\"topk_NN\", color=f'C4', title=\"topk_cos_sim|N=1000|T=.7|k=5|curie\",calibration='scaling_factor',recal_ind=300)\n",
    "\n",
    "d00 = select_df(df, data=\"C2\", k=5, T=0.7, model='text-curie-001', model_class='multi_NN', N=1000)\n",
    "lim_c2 = (min(d00['y']), max(d00['y']))\n",
    "lim_c2 = (-2, 25)\n",
    "text_anchor = sum(lim_c2)/len(lim_c2)\n",
    "create_sub_parity(axs[1], d00, 'C2 yield', lim=lim_c2, model_class=\"multi_NN\", color=f'C4', title=\"multi_cos_sim|N=1000|T=.7|k=5|curie\",calibration='scaling_factor',recal_ind=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_sub_parity(ax, df_sel, axis_name=\"topk\",model_class=\"topk\", lim=[-1,1], color='gray', GPR=False, Type_cali =False,rec_split=100,model='curie-001'):\n",
    "   \n",
    "    def process_data(df, unique_vals):\n",
    "        y, yhat, yprob = [], [], []\n",
    "        for prompt in unique_vals:\n",
    "            y.append(df[df['x'] == prompt]['y'].unique()[0])\n",
    "            yhat.append(df[df['x'] == prompt]['yhat'].values)\n",
    "            yprob.append(df[df['x'] == prompt]['yprobs'].values)\n",
    "        return y, yhat, yprob\n",
    "\n",
    "    def calculate_means_and_stds(yhat, yprob):\n",
    "        means = np.array([np.sum(yhi * ypi) if len(yhi) > 1 else ypi.mean() for yhi, ypi in zip(yhat, yprob)])\n",
    "        means = np.where(means == 0, 0.0001, means)\n",
    "        stds = np.array([np.sqrt(np.sum((yhi - ymi)**2 * ypi)) if len(yhi) > 1 else ypi.mean() for yhi, ypi, ymi in zip(yhat, yprob, means)])\n",
    "        return means, stds\n",
    "\n",
    "    unique_vals = df_sel['x'].unique()\n",
    "    y, yhat, yprob = process_data(df_sel, unique_vals[:rec_split])\n",
    "    y_rec, yhat_rec, yprob_rec = process_data(df_sel, unique_vals[rec_split:])\n",
    "\n",
    "    ymeans, ystds = calculate_means_and_stds(yhat, yprob)\n",
    "    ymeans_rec, ystds_rec = calculate_means_and_stds(yhat_rec, yprob_rec)\n",
    "    \n",
    "    if Type_cali == \"cali\":\n",
    "\n",
    "        if GPR:\n",
    "            yprobs = np.array([ypi.mean() for ypi in yprob])\n",
    "\n",
    "            yhats=np.array(np.concatenate(yhat))\n",
    "\n",
    "            ma = uct.miscalibration_area(yhats,yprobs, np.array(y), recal_model=None)\n",
    "            \n",
    "            x, y1 = uct.get_proportion_lists_vectorized(yhats,yprobs, np.array(y))\n",
    "            \n",
    "            ax.plot(x,x, linestyle= '--')\n",
    "            ax.plot(x, y1)\n",
    "            ax.fill_between(x, x, y1, color='teal', alpha=0.3)\n",
    "            \n",
    "            ax.set_title(' {} | {} | {}'.format(model,axis_name, model_class))\n",
    "           \n",
    "            ax.set_ylim(lim[0],lim[1])\n",
    "            ax.set_xlim(lim[0],lim[1])\n",
    "            ax.set_ylabel(\"Observed Proportion in Interval\")\n",
    "            \n",
    "            # ax.(lim[0] + 0.1*(max(yhat)-min(yhat)), lim[1] - 1*0.1*(max(yhat)-min(y)), f\"(\\u2193)Miscalibration area = {ma:.3f}\")\n",
    "\n",
    "            ax.annotate(f\"(\\u2193)Miscalibration area = {ma:.3f}\",\n",
    "                xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                horizontalalignment='left', verticalalignment='top')\n",
    "\n",
    "        else:\n",
    "            ma = uct.miscalibration_area(ymeans,ystds, np.array(y), recal_model=None)\n",
    "            \n",
    "            x, y1 = uct.get_proportion_lists_vectorized(ymeans,ystds, np.array(y))\n",
    "            \n",
    "            ax.plot(x,x, linestyle= '--')\n",
    "            ax.plot(x, y1)\n",
    "            ax.fill_between(x, x, y1, color='teal', alpha=0.3)\n",
    "            \n",
    "            ax.set_title(' {} | {} | {}'.format(model,axis_name, model_class))\n",
    "            # ax.set_xlabel(f\"measured {axis_name}\")\n",
    "            # ax.set_ylabel(\"Obsererved Proportion in Interval\")\n",
    "            ax.set_ylabel(\"Observed Proportion in Interval\")\n",
    "            ax.set_ylim(lim[0],lim[1])\n",
    "            ax.set_xlim(lim[0],lim[1])\n",
    "            \n",
    "            # ax.(lim[0] + 0.1*(max(yhat)-min(yhat)), lim[1] - 1*0.1*(max(yhat)-min(y)), f\"(\\u2193)Miscalibration area = {ma:.3f}\")\n",
    "\n",
    "            ax.annotate(f\"(\\u2193)Miscalibration area = {ma:.3f}\",\n",
    "                xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                horizontalalignment='left', verticalalignment='top')\n",
    "            \n",
    "\n",
    "    elif Type_cali == \"recali\":\n",
    "\n",
    "        if GPR:\n",
    "\n",
    "            yprobs = np.array([ypi.mean() for ypi in yprob])\n",
    "            yyprobs = np.array([ypi.mean() for ypi in yprob_rec])\n",
    "\n",
    "            yhats=np.array(np.concatenate(yhat))\n",
    "            yyhats=np.array(np.concatenate(yhat_rec))\n",
    "\n",
    "\n",
    "            exp_props,obs_props= uct.metrics_calibration.get_proportion_lists_vectorized(yyhats[:100], yyprobs[:100], np.array(y_rec[:100]))\n",
    "\n",
    "            \n",
    "            recal_model = uct.recalibration.iso_recal(exp_props, obs_props)\n",
    "\n",
    "            ma = uct.miscalibration_area(yhats, yprobs, np.array(y), recal_model=recal_model)\n",
    "            \n",
    "            x, y1 = uct.metrics_calibration.get_proportion_lists_vectorized(yhats, yprobs, np.array(y), recal_model=recal_model)\n",
    "            \n",
    "            ax.plot(x,x, linestyle= '--')\n",
    "            ax.plot(x, y1)\n",
    "            ax.fill_between(x, x, y1, color='teal', alpha=0.3)\n",
    "            \n",
    "            ax.set_title('Isotonic')\n",
    "            ax.set_ylim(lim[0],lim[1])\n",
    "            ax.set_xlim(lim[0],lim[1])\n",
    "            \n",
    "            # ax.(lim[0] + 0.1*(max(yhat)-min(yhat)), lim[1] - 1*0.1*(max(yhat)-min(y)), f\"(\\u2193)Miscalibration area = {ma:.3f}\")\n",
    "\n",
    "            ax.annotate(f\"(\\u2193)Miscalibration area = {ma:.3f}\",\n",
    "                xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                horizontalalignment='left', verticalalignment='top')\n",
    "\n",
    "        else:\n",
    "        \n",
    "            exp_props,obs_props= uct.metrics_calibration.get_proportion_lists_vectorized(ymeans_rec[:100], ystds_rec[:100], np.array(y_rec[:100]))\n",
    "\n",
    "\n",
    "            recal_model = uct.recalibration.iso_recal(exp_props, obs_props)\n",
    "\n",
    "            ma = uct.miscalibration_area(ymeans, ystds, np.array(y), recal_model=recal_model)\n",
    "            \n",
    "            x, y1 = uct.metrics_calibration.get_proportion_lists_vectorized(ymeans,ystds, np.array(y), recal_model=recal_model)\n",
    "            \n",
    "            ax.plot(x,x, linestyle= '--')\n",
    "            ax.plot(x, y1)\n",
    "            ax.fill_between(x, x, y1, color='teal', alpha=0.3)\n",
    "            \n",
    "            ax.set_title('Isotonic')\n",
    "            \n",
    "            ax.set_ylabel(\"Observed Proportion in Interval\")\n",
    "            ax.set_ylim(lim[0],lim[1])\n",
    "            ax.set_xlim(lim[0],lim[1])\n",
    "            \n",
    "            # ax.(lim[0] + 0.1*(max(yhat)-min(yhat)), lim[1] - 1*0.1*(max(yhat)-min(y)), f\"(\\u2193)Miscalibration area = {ma:.3f}\")\n",
    "\n",
    "            ax.annotate(f\"(\\u2193)Miscalibration area = {ma:.3f}\",\n",
    "                xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                horizontalalignment='left', verticalalignment='top')\n",
    "\n",
    "    elif Type_cali == \"recali_scale\":\n",
    "\n",
    "\n",
    "        if GPR:\n",
    "            \n",
    "            yhats=np.concatenate(yhat)\n",
    "            yyhats=np.concatenate(yhat_rec)\n",
    "\n",
    "            yprobs = np.array([ypi.mean() for ypi in yprob])\n",
    "            yyprobs = np.array([ypi.mean() for ypi in yprob_rec])\n",
    "\n",
    "            std_scaling = uct.recalibration.optimize_recalibration_ratio(yyhats[:100], yyprobs[:100], np.array(y_rec[:100]), criterion=\"miscal\")\n",
    "\n",
    "\n",
    "            ystds = yprobs * std_scaling\n",
    "            print(std_scaling,model)\n",
    "\n",
    "            ma = uct.miscalibration_area(yhats, ystds, np.array(y), recal_model=None)\n",
    "    \n",
    "            \n",
    "            x, y1 = uct.metrics_calibration.get_proportion_lists_vectorized(yhats, ystds, np.array(y), recal_model=None)\n",
    "            \n",
    "            ax.plot(x,x, linestyle= '--')\n",
    "            ax.plot(x, y1)\n",
    "            ax.fill_between(x, x, y1, color='teal', alpha=0.3)\n",
    "            \n",
    "            ax.set_title('Scaling')\n",
    "            ax.set_xlabel(\"Predicted Proportion in Interval\")\n",
    "            ax.set_ylabel(\"Observed Proportion in Interval\")\n",
    "            ax.set_ylim(lim[0],lim[1])\n",
    "            ax.set_xlim(lim[0],lim[1])\n",
    "            \n",
    "\n",
    "            ax.annotate(f\"(\\u2193)Miscalibration area = {ma:.3f}\",\n",
    "                xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                horizontalalignment='left', verticalalignment='top')\n",
    "\n",
    "        else:\n",
    "\n",
    "            std_scaling = uct.recalibration.optimize_recalibration_ratio(ymeans_rec[:100], ystds_rec[:100], np.array(y_rec[:100]),\n",
    "                                                                        criterion=\"miscal\")\n",
    "            print(std_scaling)\n",
    "            ystds = ystds * std_scaling\n",
    "\n",
    "\n",
    "            ma = uct.miscalibration_area(ymeans, ystds, np.array(y), recal_model=None)\n",
    "            \n",
    "            x, y1 = uct.metrics_calibration.get_proportion_lists_vectorized(ymeans, np.array(ystds), np.array(y), recal_model=None)\n",
    "            \n",
    "            ax.plot(x,x, linestyle= '--')\n",
    "            ax.plot(x, y1)\n",
    "            ax.fill_between(x, x, y1, color='teal', alpha=0.3)\n",
    "            \n",
    "            ax.set_title('Scaling')\n",
    "            ax.set_xlabel(\"Predicted Proportion in Interval\")\n",
    "            ax.set_ylabel(\"Observed Proportion in Interval\")\n",
    "            ax.set_ylim(lim[0],lim[1])\n",
    "            ax.set_xlim(lim[0],lim[1])\n",
    "            \n",
    "            # ax.(lim[0] + 0.1*(max(yhat)-min(yhat)), lim[1] - 1*0.1*(max(yhat)-min(y)), f\"(\\u2193)Miscalibration area = {ma:.3f}\")\n",
    "\n",
    "            ax.annotate(f\"(\\u2193)Miscalibration area = {ma:.3f}\",\n",
    "                xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                horizontalalignment='left', verticalalignment='top')\n",
    "\n",
    "    else:\n",
    "\n",
    "        ax.set_xlabel(f\"measured {axis_name}\")\n",
    "        ax.set_ylabel(f\"predicted {axis_name}\")\n",
    "        ax.set_ylim(lim[0], lim[1])\n",
    "        ax.set_xlim(lim[0], lim[1])\n",
    "\n",
    "        corr_val = corr(y, [yhi.mean() for yhi in yhat])\n",
    "        ax.text(lim[0] + 0.1 * (max(y) - min(y)), lim[1] - 1 * 0.1 * (max(y) - min(y)), f\"(\\u2191)correlation = {corr_val:.3f}\")\n",
    "        \n",
    "        if GPR:\n",
    "            ax.text(lim[0] + 0.1*(max(y)-min(y)), lim[1] - 2*0.1*(max(y)-min(y)), f\"(\\u2193)neg-ll = {log_likelihood(y, [yhi.mean() for yhi in yhat], yprobs, eps=1e-6):.3f}\")\n",
    "        else:\n",
    "            ax.text(lim[0] + 0.1*(max(y)-min(y)), lim[1] - 2*0.1*(max(y)-min(y)), f\"(\\u2193)neg-ll = {log_likelihood(y, [yhi.mean() for yhi in yhat], [yhi.std() if len(yhi)>1 else max(yprobs) for yhi in yhat], eps=1e-6):.3f}\")\n",
    "        ax.text(lim[0] + 0.1*(max(y)-min(y)), lim[1] - 3*0.1*(max(y)-min(y)), f\"(\\u2193)MAE = {mae(y, [yhi.mean() for yhi in yhat]):.3f}\")\n",
    "        \n",
    "        ax.plot(y,y)\n",
    "        ax.plot(lim,lim)\n",
    "        \n",
    "        if GPR:\n",
    "            ax.errorbar(y, \n",
    "                        [yhi.mean() for yhi in yhat], \n",
    "                        yerr=[abs(recal_bounds.lower),abs(recal_bounds.upper)],\n",
    "                        fmt='.', color='gray', alpha=0.3)\n",
    "        else:  \n",
    "            ax.errorbar(y, \n",
    "                        [yhi.mean() for yhi in yhat], \n",
    "                        yerr=[yhi.std() if len(yhi)>1 else max(yprobs) for yhi in yhat],\n",
    "                        fmt='.', color='gray', alpha=0.3)\n",
    "        ax.scatter(\n",
    "            y, [yhi.mean() for yhi in yhat], s=6, alpha=1, color=color\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import uncertainty_toolbox as uct\n",
    "\n",
    "#figsize=(6.4,4.8)\n",
    "fig, axs = plt.subplots(nrows=3, ncols=7, figsize=(30,12), constrained_layout=True)\n",
    "for ax in axs.flat:\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "\n",
    "# plot axs[0,0]\n",
    "d00 = select_df(df, data=\"C2\", k=5, T=.7, model='text-curie-001', model_class='topk', N=1000)\n",
    "lim_c2 = (min(d00['y']), max(d00['y']))\n",
    "lim_c2 = (0, 1)\n",
    "text_anchor = sum(lim_c2)/len(lim_c2)\n",
    "create_sub_parity(axs[0,0], d00, axis_name='C2',model_class='topk', lim=lim_c2, color=f'C0',Type_cali=\"cali\",rec_split=100)\n",
    "# # plot axs[0,1]\n",
    "d01 = select_df(df, data=\"C2\", k=5, T=.7, model='text-curie-001', model_class='multi', N=1000)\n",
    "create_sub_parity(axs[0,1], d01, axis_name='C2',model_class='multi', lim=lim_c2, color=f'C1', Type_cali=\"cali\",rec_split=100)\n",
    "# # plot axs[0,2]\n",
    "d02 = select_df(df, data=\"C2\", k=5, T=0.7, model='text-davinci-003', model_class='topk', N=1000)\n",
    "create_sub_parity(axs[0,2], d02, axis_name='C2',model_class='topk', lim=lim_c2, color=f'C2', Type_cali=\"cali\", rec_split=100,model='text-davinci-003')\n",
    "\n",
    "\n",
    "\n",
    "# # plot axs[1,0]\n",
    "d10 = select_df(df, data=\"C2\", k=5, T=0.7, model='text-curie-001', model_class='topk', N=1000)\n",
    "lim_c2 = (min(d10['y']), max(d10['y']))\n",
    "lim_c2 = (0, 1)\n",
    "create_sub_parity(axs[1,0], d10, axis_name='C2 yield',model_class='topk', lim=lim_c2, color=f'C4', Type_cali=\"recali\",rec_split=100) # calibration plot\n",
    "# # # plot axs[1,1]\n",
    "d11 = select_df(df, data=\"C2\", k=5, T=.7, model='text-curie-001', model_class='multi', N=1000)\n",
    "lim = (min(d11['y']), max(d11['y']))\n",
    "create_sub_parity(axs[1,1], d11, axis_name='C2 yield',model_class='multi', lim=lim_c2, color=f'C5', Type_cali=\"recali\",rec_split=100)\n",
    "# # plot axs[1,2]\n",
    "d12 = select_df(df, data=\"C2\", k=5, T=0.7, model='text-davinci-003', model_class='topk', N=1000)\n",
    "lim = (min(d12['y']), max(d12['y']))\n",
    "create_sub_parity(axs[1,2], d12, axis_name='C2 yield',model_class='topk', lim=lim_c2, color=f'C6',Type_cali=\"recali\",rec_split=100)\n",
    "\n",
    "\n",
    "\n",
    "# # plot axs[2,0]\n",
    "d20 = select_df(df, data=\"C2\", k=5, T=0.7, model='text-curie-001', model_class='topk', N=1000)\n",
    "lim = (min(d20['y']), max(d20['y']))\n",
    "create_sub_parity(axs[2,0], d20, 'C2 yield',model_class='topk', lim=lim_c2, color=f'C6',Type_cali=\"recali_scale\",rec_split=100)\n",
    "\n",
    "# # plot axs[2,1]\n",
    "d21 = select_df(df, data=\"C2\", k=5, T=0.7, model='text-curie-001', model_class='multi', N=1000)\n",
    "lim = (min(d21['y']), max(d21['y']))\n",
    "create_sub_parity(axs[2,1], d21, 'C2 yield',model_class='multi', lim=lim_c2, color=f'C6',Type_cali=\"recali_scale\",GPR=False,rec_split=100)\n",
    "\n",
    "# # # plot axs[2,2]\n",
    "d22 = select_df(df, data=\"C2\", k=5, T=0.7, model='text-davinci-003', model_class='topk', N=1000)\n",
    "lim = (min(d22['y']), max(d22['y']))\n",
    "create_sub_parity(axs[2,2], d22, 'C2 yield', lim=lim_c2, color=f'C6',Type_cali=\"recali_scale\",rec_split=100)\n",
    "\n",
    "\n",
    "# # plot axs[0,3]\n",
    "d03 = select_df(df, data=\"C2\", k=5, T=0.7, model='gpt-4', model_class='topk', N=1000)\n",
    "lim_c2 = (min(d03['y']), max(d03['y']))\n",
    "lim_c2 = (0, 1)\n",
    "text_anchor = sum(lim_c2)/len(lim_c2)\n",
    "lim = (min(d03['y']), max(d03['y']))\n",
    "create_sub_parity(axs[0,3], d03, 'C2 yield',model='gpt-4', model_class='topk', lim=lim_c2, color=f'C6',Type_cali=\"cali\",rec_split=100)\n",
    "\n",
    "# # plot axs[1,3]\n",
    "d33 = select_df(df, data=\"C2\", k=5, T=0.7, model='gpt-4', model_class='topk', N=1000)\n",
    "lim = (min(d33['y']), max(d33['y']))\n",
    "create_sub_parity(axs[1,3], d33, 'C2 yield', lim=lim_c2, color=f'C6',Type_cali=\"recali\",rec_split=100)\n",
    "\n",
    "# # # plot axs[2,3]\n",
    "d33 = select_df(df, data=\"C2\", k=5, T=0.7, model='gpt-4', model_class='topk', N=1000)\n",
    "lim = (min(d33['y']), max(d33['y']))\n",
    "create_sub_parity(axs[2,3], d33, 'C2 yield', lim=lim_c2, color=f'C6',Type_cali=\"recali_scale\",rec_split=100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # plot axs[0,4]\n",
    "d04 = select_df(df, data=\"C2\", k=0, T=0, model='GPR', model_class='GPR_mat', N=1000)\n",
    "lim_c2 = (min(d04['y']), max(d04['y']))\n",
    "lim_c2 = (0, 1)\n",
    "text_anchor = sum(lim_c2)/len(lim_c2)\n",
    "lim = (min(d04['y']), max(d04['y']))\n",
    "create_sub_parity(axs[0,4], d04, 'C2 yield',model='GPR',GPR=True,model_class='GPR_mat', lim=lim_c2, color=f'C6',Type_cali=\"cali\",rec_split=100)\n",
    "\n",
    "# # plot axs[1,4]\n",
    "d14 = select_df(df, data=\"C2\", k=0, T=0, model='GPR', model_class='GPR_mat', N=1000)\n",
    "lim = (min(d14['y']), max(d14['y']))\n",
    "create_sub_parity(axs[1,4], d14, 'C2 yield', lim=lim_c2, color=f'C6',Type_cali=\"recali\",GPR=True,rec_split=100)\n",
    "\n",
    "# # # plot axs[2,4]\n",
    "d24 = select_df(df, data=\"C2\", k=0, T=0, model='GPR', model_class='GPR_mat', N=1000)\n",
    "lim = (min(d24['y']), max(d24['y']))\n",
    "create_sub_parity(axs[2,4], d24, 'C2 yield', lim=lim_c2, color=f'C6',Type_cali=\"recali_scale\",GPR=True,rec_split=100)\n",
    "\n",
    "\n",
    "\n",
    "# # plot axs[0,5]\n",
    "d05 = select_df(df, data=\"C2\", k=0, T=0, model='GPR', model_class='GPR_ada', N=1000)\n",
    "lim_c2 = (min(d05['y']), max(d05['y']))\n",
    "lim_c2 = (0, 1)\n",
    "text_anchor = sum(lim_c2)/len(lim_c2)\n",
    "lim = (min(d05['y']), max(d05['y']))\n",
    "create_sub_parity(axs[0,5], d05, 'C2 yield',model='GPR',GPR=True, model_class='GPR-ada', lim=lim_c2, color=f'C6',Type_cali=\"cali\",rec_split=100)\n",
    "\n",
    "# # plot axs[1,5]\n",
    "d35 = select_df(df, data=\"C2\", k=0, T=0, model='GPR', model_class='GPR_ada', N=1000)\n",
    "lim = (min(d35['y']), max(d35['y']))\n",
    "create_sub_parity(axs[1,5], d35, 'C2 yield', lim=lim_c2, color=f'C6',Type_cali=\"recali\",GPR=True,rec_split=100)\n",
    "\n",
    "# # # plot axs[2,5]\n",
    "d35 = select_df(df, data=\"C2\", k=0, T=0, model='GPR', model_class='GPR_ada', N=1000)\n",
    "lim = (min(d35['y']), max(d35['y']))\n",
    "create_sub_parity(axs[2,5], d35, 'C2 yield', lim=lim_c2, color=f'C6',GPR=True,Type_cali=\"recali_scale\",rec_split=100)\n",
    "\n",
    "\n",
    "\n",
    "# # plot axs[0,6]\n",
    "d06 = select_df(df, data=\"C2\", k=0, T=0, model='GPR', model_class='GPR_num', N=1000)\n",
    "lim_c2 = (min(d06['y']), max(d06['y']))\n",
    "lim_c2 = (0, 1)\n",
    "text_anchor = sum(lim_c2)/len(lim_c2)\n",
    "lim = (min(d06['y']), max(d06['y']))\n",
    "create_sub_parity(axs[0,6], d06, 'C2 yield',model='GPR',GPR=True,model_class='GPR_num', lim=lim_c2, color=f'C6',Type_cali=\"cali\",rec_split=100)\n",
    "\n",
    "# # plot axs[1,6]\n",
    "d16 = select_df(df, data=\"C2\", k=0, T=0, model='GPR', model_class='GPR_num', N=1000)\n",
    "lim = (min(d16['y']), max(d16['y']))\n",
    "create_sub_parity(axs[1,6], d16, 'C2 yield', lim=lim_c2, color=f'C6',Type_cali=\"recali\",GPR=True,rec_split=100)\n",
    "\n",
    "# # # plot axs[2,6]\n",
    "d26 = select_df(df, data=\"C2\", k=0, T=0, model='GPR', model_class='GPR_num', N=1000)\n",
    "lim = (min(d26['y']), max(d26['y']))\n",
    "create_sub_parity(axs[2,6], d26, 'C2 yield', lim=lim_c2, color=f'C6',Type_cali=\"recali_scale\",GPR=True,rec_split=100)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bolift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
