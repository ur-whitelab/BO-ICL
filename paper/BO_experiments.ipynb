{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "initial_train = 1\n",
    "initial_transfer_train=0\n",
    "ask_K = 1\n",
    "N=30\n",
    "M=5\n",
    "lambda_multi = 0.1\n",
    "# model=\"gpt-3.5-turbo-instruct\"\n",
    "model=\"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Focused Pool For Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "model = \"text-embedding-3-large\"\n",
    "\n",
    "def get_embeddings(texts, model=model, batch_size=100):\n",
    "    all_embeddings = []\n",
    "    cleaned_texts = [text.replace(\"\\n\", \" \") if isinstance(text, str) else \"\" for text in texts]\n",
    "    for i in range(0, len(cleaned_texts), batch_size):\n",
    "        batch = cleaned_texts[i: i + batch_size]\n",
    "        embeddings_data = client.embeddings.create(input=batch, model=model).data\n",
    "        all_embeddings.extend([embedding.embedding for embedding in embeddings_data])\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "small_data_path = \"/Users/shane/repos/BO-LIFT/paper/dataset/data/bias_free_ocmdataset_p_comp.csv\" \n",
    "large_data_path = \"/Users/shane/repos/BO-LIFT/paper/dataset/data/C2_yield_meth_oxy_short_corrected.csv\"\n",
    "\n",
    "small_data = pd.read_csv(small_data_path)\n",
    "large_data = pd.read_csv(large_data_path)\n",
    "\n",
    "prompt_col = \"prompt\"\n",
    "completion_col = \"completion\"\n",
    "\n",
    "small_prompts = small_data[prompt_col].fillna(\"\").tolist()\n",
    "large_prompts = large_data[prompt_col].fillna(\"\").tolist()\n",
    "\n",
    "small_embeddings = get_embeddings(small_prompts)\n",
    "large_embeddings = get_embeddings(large_prompts)\n",
    "\n",
    "similarities = cosine_similarity(small_embeddings, large_embeddings)\n",
    "\n",
    "selected_indices = set()\n",
    "new_data_list = []\n",
    "\n",
    "for i, small_prompt in tqdm(enumerate(small_prompts), total=len(small_prompts)):\n",
    "\n",
    "    sorted_indices = np.argsort(similarities[i])[::-1]\n",
    "\n",
    "    count = 0\n",
    "    for index in sorted_indices:\n",
    "        if index not in selected_indices:\n",
    "            large_prompt = large_data.iloc[index][prompt_col]\n",
    "            completion_text = large_data.iloc[index][completion_col]\n",
    "            new_data_list.append({\"prompt\": large_prompt, \"completion\": completion_text})\n",
    "            selected_indices.add(index)\n",
    "            count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "\n",
    "new_data = pd.DataFrame(new_data_list)\n",
    "t_data_path = \"top_10_similar_subset.csv\"\n",
    "new_data.to_csv(t_data_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Tranfer Learning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 20\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "t_data_path=\"top_10_similar_subset.csv\"\n",
    "\n",
    "t_data_path = \"/Users/shane/repos/BO-LIFT/\" + t_data_path\n",
    "# transfer_data = pd.read_csv(t_data_path)\n",
    "transfer_data = pd.read_csv(\"dataset/data/C2_yield_meth_oxy_short_corrected.csv\")\n",
    "\n",
    "t_N = transfer_data.shape[0]\n",
    "t_indexes = np.random.choice(transfer_data.shape[0], int(t_N), replace=False)\n",
    "t_x_name = \"prompt\"\n",
    "t_y_name = \"completion\"\n",
    "len(t_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in-house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"in-house\"\n",
    "kwargs = dict(\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@\\nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@\\nA:\",\n",
    "    x_formatter=lambda x: f\"experimental procedure: {x}\",\n",
    "    y_name=\"CO STY\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    "    selector_k=5,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "path = f\"./out/bias_free_ocmdataset_p_comp.pkl\"\n",
    "pool_path = \"./dataset/data/42000_in-house_pool.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"ocm\"\n",
    "kwargs = dict(\n",
    "    # prefix=\"You are a bot who knows chemistry and catalysts. \" \\\n",
    "    #         \"Below, you'll see examples of experimental procedures to synthesize catalysts and the measured C2 yield in a oxidative methane coupling reaction. \" \\\n",
    "    #         \"The following question should be answered with a number and finished with ###\\n\",\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@\\nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@\\nA:\",\n",
    "    x_formatter=lambda x: f\"experimental procedure: {x}\",\n",
    "    y_name=\"C2 yield\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    "    selector_k=5,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "inv_system_message_path = \"./prompts/inv_prompt_1.txt\"\n",
    "system_message_path = \"./prompts/prompt_1.txt\"\n",
    "\n",
    "# path = f\"./out/{dataset}_{model}_300_{initial_train}_{ask_K}_lambda_mult{lambda_multi}_corrected_tableprompt_transfer_data_{initial_transfer_train}.pkl\"\n",
    "\n",
    "path = f\"./out/{dataset}_{model}_{initial_train}_{ask_K}_lambda_mult{lambda_multi}_corrected_tableprompt_1random_initialpoint.pkl\"\n",
    "# pool_path = \"./dataset/data/bias_free_ocmdataset_p_comp.pkl\"\n",
    "\n",
    "# path = f\"./out/inv_system_message_test_{lambda_multi}.pkl\"\n",
    "pool_path = \"./dataset/data/C2_yield_meth_oxy_short_corrected_allacqs1.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solubility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"sol\"\n",
    "kwargs = dict(\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@\\nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@\\nA:\",\n",
    "    x_formatter=lambda x: f\"iupac name {x}\",\n",
    "    y_name=\"measured log solubility in mols per litre\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    "    selector_k=5,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "path = f\"./out/{dataset}_{model}_882_{initial_train}_{ask_K}_16nr.pkl\"\n",
    "pool_path = \"./out/sol_pool.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import copy, cloudpickle\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.join(current_dir,'..')\n",
    "sys.path.insert(0,parent_dir)\n",
    "\n",
    "import bolift\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "transfer_data = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/google/fonts/raw/main/ofl/ibmplexmono/IBMPlexMono-Regular.ttf\",\n",
    "    \"IBMPlexMono-Regular.ttf\",\n",
    ")\n",
    "fe = font_manager.FontEntry(fname=\"IBMPlexMono-Regular.ttf\", name=\"plexmono\")\n",
    "font_manager.fontManager.ttflist.append(fe)\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.facecolor\": \"#f5f4e9\",\n",
    "        \"grid.color\": \"#AAAAAA\",\n",
    "        \"axes.edgecolor\": \"#333333\",\n",
    "        \"figure.facecolor\": \"#FFFFFF\",\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.prop_cycle\": plt.cycler(\"color\", plt.cm.Dark2.colors),\n",
    "        \"font.family\": fe.name,\n",
    "        \"figure.figsize\": (3.5, 3.5 / 1.2),\n",
    "        \"ytick.left\": True,\n",
    "        \"xtick.bottom\": True,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# import uncertainty_toolbox as uct\n",
    "\n",
    "def combine(s, l):\n",
    "  '''Number of combinations of l elements with max = s'''\n",
    "  return (s**l - (s-1)**(l))\n",
    "\n",
    "def prob(s, l, n):\n",
    "  '''Probability of getting a sample with max([x0,x1,...,xl]) = s where xi={0,n}'''\n",
    "  return combine(s,l) * ((1/n)**l)\n",
    "\n",
    "def expected_value_p(l, n):\n",
    "  '''Expected value of max([x0,x1,...,xl]) where xi={0,n}'''\n",
    "  E = [s * prob(s, l, n) for s in range(1,100+1)]\n",
    "  return sum(E)\n",
    "\n",
    "def expected_value_q(l, n, data):\n",
    "  '''Expected value of max([x0,x1,...,xl]) where xi={0,n}'''\n",
    "  quants = [data.quantile(i/100) for i in range(100+1)]\n",
    "  # E = [(quants[s-1]) * prob(s, l, n) for s in range(1,100+1)]\n",
    "  E = [((quants[s-1]+quants[s])/2) * prob(s, l, n) for s in range(1,100+1)]\n",
    "  return sum(E)\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "# @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def run_experiment(\n",
    "    asktell, pool, raw_data, indexes, x_name, y_name, N=1, initial_train=1, ask_K=1, aq=\"random\", start_index=0, calibrate=False,\n",
    "    lambda_multi=0.1, system_message=\"\", inv_system_message=\"\",transfer_train=1, transfer=False, trans_data=transfer_data, t_indexes=t_indexes\n",
    "):\n",
    "    if aq=='random_mean':\n",
    "       return [ (i, expected_value_q(i, 100, raw_data[y_name])) for i in range(1,N+initial_train) ]\n",
    "    \n",
    "    point=[]\n",
    "    counter = 1\n",
    "    for i in indexes[:initial_train]:\n",
    "        asktell.tell(raw_data[x_name].iloc[i], float(raw_data[y_name].iloc[i]))\n",
    "        if counter == 1:\n",
    "           i_best = float(raw_data[y_name].iloc[i])\n",
    "        else:\n",
    "           i_best = sorted(point, key=lambda i_points: i_points[-1])[-1][-1]\n",
    "           \n",
    "        print(raw_data[x_name].iloc[i], float(raw_data[y_name].iloc[i]))\n",
    "        \n",
    "        # point.append((raw_data[x_name].iloc[i],counter,i_best,float(raw_data[y_name].iloc[i])))\n",
    "        counter+=1\n",
    "\n",
    "    if transfer:\n",
    "      for j in t_indexes[:transfer_train]:\n",
    "          asktell.tell(trans_data[x_name].iloc[j], float(trans_data[y_name].iloc[j]))\n",
    "        \n",
    "    if calibrate:\n",
    "        # y = [float(raw_data[y_name].iloc[i]) for i in indexes[:initial_train]]\n",
    "        # pred = asktell.predict(y)\n",
    "        # ymeans = np.array([yhi.mean() for yhi in pred])\n",
    "        # ystds = np.array([yhi.std() for yhi in pred])\n",
    "        # calibration_factor = uct.recalibration.optimize_recalibration_ratio(ymeans, ystds, np.array(y), criterion=\"miscal\")\n",
    "        calibration_factor = 5.0\n",
    "        asktell.set_calibration_factor(calibration_factor)\n",
    "\n",
    "    x = raw_data[x_name].tolist()\n",
    "\n",
    "    pool.reset()\n",
    "    xi = x[start_index]\n",
    "    x.remove(xi)\n",
    "    pool.choose(xi)\n",
    "    yi = float(raw_data[raw_data[x_name] == xi][y_name].iloc[0])\n",
    "    asktell.tell(xi, yi)\n",
    "    point.append((xi, 1+initial_train,i_best, yi))\n",
    "\n",
    "    best = sorted(point, key=lambda points: points[-1])[-1][-1]\n",
    "\n",
    "    for i in range(1, N):\n",
    "        if i == N - 1 and aq != \"random\":\n",
    "            aq = \"greedy\"\n",
    "        px, _, py = asktell.ask(pool,\n",
    "                                k=ask_K,\n",
    "                                aq_fxn=aq,\n",
    "                                _lambda=1.0,\n",
    "                                inv_filter=16,\n",
    "                                aug_random_filter=0,\n",
    "                                lambda_mult=lambda_multi,\n",
    "                                system_message=system_message,\n",
    "                                inv_system_message=inv_system_message,\n",
    "                                )\n",
    "        for j in range(ask_K):\n",
    "          xc = px[j]\n",
    "          x.remove(xc)\n",
    "          pool.choose(xc)\n",
    "          y = float(raw_data[raw_data[x_name] == xc][y_name].iloc[0])\n",
    "          asktell.tell(xc, y)\n",
    "          best = max(y, best)\n",
    "        point.append((xc, 1+initial_train+i*ask_K, best, y))\n",
    "        \n",
    "    return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(data: str, M=M):\n",
    "    match data:\n",
    "        case \"in-house\":\n",
    "            data_path = \"./dataset/data/71023_BO_ready_pool.csv\"\n",
    "            raw_data = pd.read_csv(data_path)\n",
    "\n",
    "            raw_data['Catalyst'] = raw_data['Prompt'].str.extract(r'(\\b[A-Z][a-z]?:[A-Z][a-z]?:[A-Z][a-z]?\\b)')\n",
    "            unique_cat = raw_data['Catalyst'].unique()\n",
    "            c = {c: 0.2+m*(5/len(unique_cat)) for m, c in enumerate(unique_cat)}\n",
    "            raw_data['dummy_Completion'] = raw_data['Catalyst'].apply(lambda x: np.random.normal(c[x], 0.05))\n",
    "\n",
    "            x_name = \"Prompt\"\n",
    "            y_name = \"dummy_Completion\"\n",
    "        case \"ocm\":\n",
    "            data_path = \"./dataset/data/C2_yield_meth_oxy_short_corrected.csv\"\n",
    "            raw_data = pd.read_csv(data_path, sep=\",\")\n",
    "            raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "            x_name = \"prompt\"\n",
    "            y_name = \"completion\"\n",
    "        case \"biasfree_ocm\":\n",
    "            data_path = \"./dataset/data/bias_free_ocmdataset_p_comp.csv\"\n",
    "            raw_data = pd.read_csv(data_path, sep=\",\")\n",
    "            raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "            x_name = \"prompt\"\n",
    "            y_name = \"completion\"\n",
    "        case \"sol\":\n",
    "            data_path = \"./dataset/data/esol_iupac.csv\"\n",
    "            raw_data = pd.read_csv(data_path)\n",
    "            raw_data = raw_data.dropna()\n",
    "            raw_data = raw_data[[\"IUPAC\", \"measured log(solubility:mol/L)\"]]\n",
    "            x_name = \"IUPAC\"\n",
    "            y_name = \"measured log(solubility:mol/L)\"\n",
    "        case _:\n",
    "            raise ValueError(\"Unknown data\")\n",
    "        \n",
    "    n_data = raw_data.shape[0]\n",
    "    indexes = np.random.choice(raw_data.shape[0], int(n_data), replace=False)\n",
    "\n",
    "    print(f\"Dataset size: \\n\\t{n_data}\")\n",
    "    starts = raw_data.sort_values(by=y_name, ascending=True).head(10).sample(M+initial_train)# np.random.randint(0, len(indexes), M)\n",
    "    print(f\"Start xs: \\n\\t{starts[x_name].to_list()}\")\n",
    "    print(f\"Start ys: \\n\\t{starts[y_name].to_list()}\")\n",
    "    starts = starts.index\n",
    "    print(f\"Start indexes: \\n\\t{starts}\\n\")\n",
    "\n",
    "    return raw_data, starts, indexes, x_name, y_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_asktell(model: str, kwargs: dict = {}, pool: bolift.Pool = None, knn: int = 1):\n",
    "    match model:\n",
    "        case \"instruct\":\n",
    "            kwargs['model']=\"gpt-3.5-turbo-instruct\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-turbo\":\n",
    "            kwargs['model']=\"gpt-3.5-turbo-0125\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4\":\n",
    "            kwargs['model']=\"gpt-4\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4-turbo\":\n",
    "            kwargs['model']=\"gpt-4-0125-preview\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4o\":\n",
    "            kwargs['model']=\"gpt-4o\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"davinci\":\n",
    "            kwargs['model']=\"davinci-002\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"curie\":\n",
    "            kwargs['model']=\"text-curie-001\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpr\":\n",
    "            kwargs['selector_k'] = 0\n",
    "            kwargs['pool'] = pool if pool else None\n",
    "            kwargs['n_components'] = 32\n",
    "            return bolift.AskTellGPR(**kwargs)\n",
    "        # knn and krr don't output uncertainties\n",
    "        # case \"knn\":\n",
    "        #     del kwargs['selector_k']\n",
    "        #     kwargs['knn'] = knn\n",
    "        #     return bolift.AskTellNearestNeighbor(**kwargs)\n",
    "        # case \"krr\":\n",
    "        #     kwargs['alpha'] = 0.5\n",
    "        #     return bolift.AskTellRidgeKernelRegression(**kwargs)\n",
    "        case _:\n",
    "            raise ValueError(\"Unknown model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_bkp(path, pool_path, indexes, kwargs):\n",
    "    if os.path.exists(pool_path):\n",
    "        with open(pool_path, \"rb\") as f:\n",
    "            pool = cloudpickle.load(f)\n",
    "        pool.reset()\n",
    "    else:\n",
    "        x = [raw_data[x_name].iloc[i] for i in indexes]\n",
    "        pool = bolift.Pool(list(x), formatter=kwargs['x_formatter'])\n",
    "\n",
    "        # cloudpickle.dump(pool, open(pool_path, \"wb\"))\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        bayesOpts = cloudpickle.load(open(path, \"rb\"))\n",
    "    else:\n",
    "        bayesOpts = {}\n",
    "    return bayesOpts, pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BayesOpt experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Changing the sparsity structure of a csr_matrix is expensive.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not contained to the unit cube.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not standardized.*')\n",
    "warnings.filterwarnings('ignore', message=\"Keyword arguments .* will be ignored because they are not allowed parameters for function .*\", category=UserWarning)\n",
    "\n",
    "raw_data, starts, indexes, x_name, y_name = get_dataset(dataset, M=M)\n",
    "bayesOpts, pool = read_bkp(path, pool_path, indexes, kwargs)\n",
    "\n",
    "if os.path.exists(system_message_path):\n",
    "    with open(system_message_path, \"r\") as f:\n",
    "        system_message = f.read()\n",
    "\n",
    "if os.path.exists(inv_system_message_path):\n",
    "    with open(inv_system_message_path, \"r\") as f:\n",
    "        inv_system_message = f.read()\n",
    "\n",
    "asktell = get_asktell(model, kwargs=kwargs)#, pool=bolift.Pool(list(pool.sample(5000))), knn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Changing the sparsity structure of a csr_matrix is expensive.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not contained to the unit cube.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not standardized.*')\n",
    "warnings.filterwarnings('ignore', message=\"Keyword arguments .* will be ignored because they are not allowed parameters for function .*\", category=UserWarning)\n",
    "\n",
    "for aq in [\"random_mean\", \"upper_confidence_bound\", \"random\"]: #['expected_improvement','log_expected_improvement','probability_of_improvement', 'upper_confidence_bound', 'greedy',\"random\",\"random_mean\"]:\n",
    "    print(aq, \"start:\", end=\" \")\n",
    "    points = []\n",
    "    for i in range(M):\n",
    "        print(i, end=\",  \")\n",
    "        \n",
    "        point = run_experiment(\n",
    "            copy.deepcopy(asktell),\n",
    "            pool, # copy.deepcopy(pool)\n",
    "            raw_data,\n",
    "            indexes=indexes,\n",
    "            x_name=x_name,\n",
    "            y_name=y_name,\n",
    "            N=N,\n",
    "            aq=aq,\n",
    "            start_index=starts[i+initial_train],\n",
    "            calibrate=True,\n",
    "            initial_train=initial_train,\n",
    "            ask_K=ask_K,\n",
    "            lambda_multi=lambda_multi,\n",
    "            system_message=system_message,\n",
    "            inv_system_message=inv_system_message,\n",
    "            transfer_train=initial_transfer_train,\n",
    "            transfer=False,\n",
    "            trans_data=transfer_data,\n",
    "            t_indexes=t_indexes)\n",
    "        \n",
    "        points.append(point)\n",
    "    points = np.array(points)\n",
    "    bayesOpts[aq] = points\n",
    "    print(aq, \"done\")\n",
    "    # if isinstance(asktell, bolift.AskTellGPR):\n",
    "    #     asktell.save_cache(\"GPR_ada_embed_cache.csv\")\n",
    "    cloudpickle.dump(bayesOpts, open(path, \"wb\"))\n",
    "cloudpickle.dump(bayesOpts, open(path, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(point, key=lambda i_points: i_points[-1])[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = cloudpickle.load(open(\"./out/inv_system_message_test_0.1.pkl\", \"rb\"))\n",
    "d = bayesOpts\n",
    "exps = d['upper_confidence_bound']\n",
    "for i, step in enumerate(exps[0]):\n",
    "    print(step)\n",
    "    # print(f\"Sample {i:>2d}: {' | '.join(exp[i][0][10:60] for exp in exps)}\")\n",
    "\n",
    "# 1.7 ,2 , 2, 15.6, 7.6, 16.4, 17\n",
    "# 1.7 ,4.8 , .5, 7.6, 10.3, 1.4, 17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "# path = \"./out/biasfree_ocm_gpt-turbo_300_1_1_lambda_mult0.1_corrected_tablepromptgpt44.pkll\"\n",
    "title = \"\" #f\"{path[6:-4]}\" \n",
    "d = cloudpickle.load(open(path, \"rb\"))\n",
    "# d = bayesOpts\n",
    "data=raw_data[y_name]\n",
    "lim=(data.min()-1, data.max()+1)\n",
    "\n",
    "# name = \"LogS\"\n",
    "name = \"C$_2$ yield\"\n",
    "# name = \"CO STY\"\n",
    "\n",
    "def plot_config():\n",
    "    plt.title(title)\n",
    "    plt.axhline(y=data.max(), color=\"C15\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.max(), \"max\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.quantile(0.99), color=\"C14\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.quantile(0.99), \"99%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.quantile(0.95), color=\"C13\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.quantile(0.95), \"95%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.mean(), color=\"C12\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.mean(), \"mean\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.quantile(0.05), color=\"C11\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.quantile(0.05), \"5%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.min(), color=\"C10\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.min(), \"Min\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.ylim(lim)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5,-0.1),\n",
    "          fancybox=True, shadow=True, ncol=3)\n",
    "\n",
    "#Debugging plots\n",
    "# Plot best value on the entire run\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xlabel(\"Number of samples\")\n",
    "# plt.ylabel(\"Max C$_2$ yield\")\n",
    "plt.ylabel(f\"Max {name}\")\n",
    "\n",
    "for i, acq in enumerate(d.keys()):\n",
    "    if acq == \"log_expected_improvement\":\n",
    "        continue\n",
    "    if acq == \"random_mean\":\n",
    "        plt.plot(d[acq][0,:,0].astype(int), d[acq][:, :, 1].astype(float).mean(axis=0), \n",
    "                 label=f\"random\", color=\"gray\", linestyle=\"dashed\")\n",
    "    else:\n",
    "        for j in range(M):\n",
    "            try:\n",
    "                plt.plot(d[acq][j,:,1].astype(int), d[acq][j, :, 2].astype(float), alpha=0.2, color=f\"C{i}\")\n",
    "            except:\n",
    "                continue\n",
    "        plt.plot(d[acq][0,:,1].astype(int), d[acq][:, :, 2].astype(float).mean(axis=0), label=acq, color=f\"C{i}\")\n",
    "plot_config()\n",
    "plt.show()\n",
    "\n",
    "# Plot current values on each iteration\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(f\"{name}\")\n",
    "for acq in d.keys():\n",
    "    if acq == \"random_mean\" or acq == \"log_expected_improvement\" or acq == \"random\":\n",
    "        continue\n",
    "    else:\n",
    "        for i in range(M):\n",
    "            plt.plot(d[acq][i,:,1], d[acq][i, :, 3].astype(float), label=f\"{acq}:{i}\", alpha=0.2)\n",
    "        plt.plot(d[acq][0,:,1], d[acq][:, :, 3].astype(float).mean(axis=0), label=f\"{acq}\")\n",
    "plot_config()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in d.items():\n",
    "    print(key, value.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesOpt Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_BO(ax, data_file, title, data, axis_name, lim=None, label=False, M=1):\n",
    "    d = cloudpickle.load(open(data_file, \"rb\"))\n",
    "\n",
    "    for i in range(M):\n",
    "        if \"expected_improvement\" in d.keys():\n",
    "          ax.plot(\n",
    "            [int(s) for s in d['expected_improvement'][i, :, 1]],\n",
    "            [float(y) for y in d['expected_improvement'][i, :, 2]], \n",
    "            color=\"C1\", alpha=0.2\n",
    "          )\n",
    "        if \"greedy\" in d.keys():\n",
    "           ax.plot(\n",
    "            [int(s) for s in d['greedy'][i, :, 1]],\n",
    "            [float(y) for y in d['greedy'][i, :, 2]], \n",
    "            color=\"C2\", alpha=0.2\n",
    "          )\n",
    "        if \"upper_confidence_bound\" in d.keys():\n",
    "          ax.plot(\n",
    "            [int(s) for s in d['upper_confidence_bound'][i, :, 1]],\n",
    "            [float(y) for y in d['upper_confidence_bound'][i, :, 2]], \n",
    "            color=\"C3\", alpha=0.2\n",
    "          )\n",
    "        if \"probability_of_improvement\" in d.keys():\n",
    "          ax.plot(\n",
    "            [int(s) for s in d['probability_of_improvement'][i, :, 1]],\n",
    "            [float(y) for y in d['probability_of_improvement'][i, :, 2]], \n",
    "            color=\"C4\", alpha=0.2\n",
    "          )\n",
    "        if \"random\" in d.keys():\n",
    "          ax.plot(\n",
    "            [int(s) for s in d['random'][i, :, 1]],\n",
    "            [float(y) for y in d['random'][i, :, 2]], \n",
    "            color=\"C8\", alpha=0.2\n",
    "          )\n",
    "    if \"expected_improvement\" in d.keys():\n",
    "      label = \"EI\" if label else None\n",
    "      ax.plot(\n",
    "            d['expected_improvement'][:, :, 1].astype('int').mean(axis=0),\n",
    "            d['expected_improvement'][:, :, 2].astype('float').mean(axis=0), \n",
    "            color=\"C1\", label=label\n",
    "          )\n",
    "    if \"greedy\" in d.keys():\n",
    "      label = \"Greedy\" if label else None\n",
    "      ax.plot(\n",
    "            d['greedy'][:, :, 1].astype('int').mean(axis=0),\n",
    "            d['greedy'][:, :, 2].astype('float').mean(axis=0), \n",
    "            color=\"C2\", label=label\n",
    "          )\n",
    "    if \"upper_confidence_bound\" in d.keys():\n",
    "      label = \"UCB\" if label else None\n",
    "      ax.plot(\n",
    "            d['upper_confidence_bound'][:, :, 1].astype('int').mean(axis=0),\n",
    "            d['upper_confidence_bound'][:, :, 2].astype('float').mean(axis=0), \n",
    "            color=\"C3\", label=label\n",
    "          )\n",
    "    if \"probability_of_improvement\" in d.keys():\n",
    "      label = \"POI\" if label else None\n",
    "      ax.plot(\n",
    "            d['probability_of_improvement'][:, :, 1].astype('int').mean(axis=0),\n",
    "            d['probability_of_improvement'][:, :, 2].astype('float').mean(axis=0), \n",
    "            color=\"C4\", label=label\n",
    "          )\n",
    "    if \"random\" in d.keys():\n",
    "      label = \"random\" if label else None\n",
    "      ax.plot(\n",
    "            d['random'][:,:, 1].astype('int').mean(axis=0),\n",
    "            d['random'][:,:, 2].astype('float').mean(axis=0), \n",
    "            color=\"C8\", label=label\n",
    "          )\n",
    "    if \"random_mean\" in d.keys():\n",
    "      label = \"Random\" if label else None\n",
    "      ax.plot(\n",
    "            d['random_mean'][:, :, 0].astype('int').mean(axis=0),\n",
    "            d['random_mean'][:, :, 1].astype('float').mean(axis=0), \n",
    "            color=\"gray\", label=label, linestyle=\"dashed\"\n",
    "      )\n",
    "    ax.axhline(y=data.max(), color=\"C15\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.max(), \"max\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.axhline(y=data.quantile(0.99), color=\"C14\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.quantile(0.99), \"99%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.axhline(y=data.quantile(0.95), color=\"C13\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.quantile(0.95), \"95%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.axhline(y=data.mean(), color=\"C12\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.mean(), \"mean\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    if not data_file.startswith(\"./out/sol\"):\n",
    "      ax.axhline(y=data.quantile(0.05), color=\"C11\", linestyle=\"--\")\n",
    "      ax.text(ax.get_xlim()[1]+1, data.quantile(0.05)+0.3, \"5%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "      ax.axhline(y=data.min(), color=\"C10\", linestyle=\"--\")\n",
    "      ax.text(ax.get_xlim()[1]+1, data.min()-0.3, \"min\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.set_xlabel(\"Number of samples\")\n",
    "    ax.set_ylabel(f\"Measured {axis_name}\")\n",
    "    # ax.set_xticks([i for i in range(0,N+1,5)], [str(x * 1) for x in [i for i in range(0,N+1,5)]])\n",
    "    if lim:\n",
    "      ax.set_ylim(lim)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"ocm\", M=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(4,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(raw_data[y_name].min()-1, raw_data[y_name].max()+1)\n",
    "# plot_BO(axs[0], \"./out/ocm_curie_12744_1_1_16nr.pkl\",\"GPT3\", \n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=True, M=5)\n",
    "\n",
    "# plot_BO(axs[0], \"./out/biasfree_ocm_gpt-turbo_300_1_1_lambda_mult0.1_corrected.pkl\",\"Bias Free: lambda .1_corrected\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=True, M=5)\n",
    "\n",
    "# plot_BO(axs[1], \"./out/biasfree_ocm_gpt-turbo_300_1_1_lambda_mult0.5_corrected.pkl\",\"Bias Free: lambda .5_corrected\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "# plot_BO(axs[2], \"/Users/shane/repos/BO-LIFT/paper/out/biasfree_ocm_gpt-turbo_300_1_1_lambda_mult0.9_corrected.pkl\",\"Bias Free: lambda .9_corrected\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "\n",
    "# plot_BO(axs, \"./out/biasfree_ocm_gpt-turbo_300_1_1_lambda_mult0.1_corrected_tableprompt_transfer_data_10.pkl\",\"GPT 4 Bias Free Table Format t_train=10: lambda .1\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=True, M=5)\n",
    "\n",
    "\n",
    "plot_BO(axs, path,\"Bias Free Table Format_large: 100_t_train_non_random lambda .1 \",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, M=5)\n",
    "\n",
    "# plot_BO(axs[1], './out/biasfree_ocm_gpt-turbo_300_1_1_lambda_mult0.1_corrected_tableprompt_transfer_data_100.pkl',\" GPT4 Table Format: lambda .1\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "# plot_BO(axs[4], \"/Users/shane/repos/BO-LIFT/paper/out/biasfree_ocm_gpt-turbo_300_1_1_lambda_mult1.pkl\",\"Bias Free: lambda 1\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "# plot_BO(axs[3], \"/Users/shane/repos/BO-LIFT/paper/out/biasfree_ocm_gpt-turbo_300_1_1_lambda_mult1_corrected_1.pkl\",\"Bias Free: lambda 1_corrected\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "\n",
    "# path = f\"./out/{dataset}_{model}_300_{initial_train}_{ask_K}_lambda_mult{lambda_multi}.pkl\"\n",
    "\n",
    "# plot_BO(axs[1], \"./out/ocm_GPT-4_12744_0_s.pkl\", \"GPT4\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "# d\n",
    "# plot_BO(axs[2], \"./out/biasfree_ocm_gpt-turbo_300_1_1_lambda_mult0.1_corrected_tablepromptgpt4_1.5*normal.pkl\", \"GPT4 Table Format: lambda .1 - 1.5*\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "# plot_BO(axs[3], \"/Users/shane/repos/BO-LIFT/paper/out/biasfree_ocm_gpt-turbo_300_1_1_lambda_mult0.1_corrected_tablepromptgpt4_1.5*normal_meanstart.pkl\", \"GPT4 Table Format (mean): lambda .1 - 1.5*\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "fig.suptitle(\"Bayesian Optimization on OCM dataset\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "# plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,4), constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(raw_data[y_name].min()-1, raw_data[y_name].max()+1)\n",
    "\n",
    "plot_BO(axs[0], \"./out/ocm_gpt35turbo_12744_1_1_test0.pkl\",\"GPT3.5-turbo: 1.2\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, M=5)\n",
    "\n",
    "plot_BO(axs[1], \"./out/ocm_gpt35instruct_12744_5_1_test8.pkl \", \"GPT3.5-turbo: 1.2\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "fig.suptitle(\"Bayesian Optimization on OCM dataset\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_curie = cloudpickle.load(open(\"./out/ocm_curie_12744_1_1_16nr.pkl\", \"rb\"))\n",
    "d_davinci = cloudpickle.load(open(\"./out/C2_davinci2_12744_1_16nr.pkl\", \"rb\"))\n",
    "d_gpt4 = cloudpickle.load(open(\"./out/ocm_GPT-4_12744_0_s.pkl\", \"rb\"))\n",
    "d_gpr = cloudpickle.load(open(\"./out/C2_gpr_12744_1_16nr.pkl\", \"rb\"))\n",
    "\n",
    "print(d_curie['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_curie = d_curie['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Curie is top{np.sum(raw_data[y_name] > best_curie)}: {best_curie}\")\n",
    "\n",
    "print(d_davinci['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "print(d_gpt4['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpt4 = d_gpt4['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "print(d_gpr['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpr = d_gpr['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "# print(np.sum(raw_data[y_name] > best))\n",
    "plt.xlabel(\"measured C$_2$ yield\")\n",
    "plt.axvline(best_curie, color='C4', linestyle='--', label=\"Curie\")\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"sol\", M=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(1.8)\n",
    "\n",
    "lim=(raw_data[y_name].mean()-1, raw_data[y_name].max()+1)\n",
    "\n",
    "plot_BO(axs[0], \"./out/sol_davinci_100.pkl\", \"GPT3.5\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=False)\n",
    "plot_BO(axs[1], \"./out/sol_gpt4_100.pkl\", \"GPT4\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=False)\n",
    "plot_BO(axs[2], \"./out/sol_GPR_100.pkl\", \"GPR\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=True)\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_davinci = cloudpickle.load(open(\"./out/sol_davinci_100.pkl\", \"rb\"))\n",
    "d_gpt4 = cloudpickle.load(open(\"./out/sol_gpt4_100.pkl\", \"rb\"))\n",
    "d_gpr = cloudpickle.load(open(\"./out/sol_GPR_100.pkl\", \"rb\"))\n",
    "\n",
    "print(d_davinci['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "print(d_gpt4['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpt4 = d_gpt4['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "print(d_gpr['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpr = d_gpr['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci-0.08])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(88)\n",
    "\n",
    "# data_path = \"./dataset/data/12744_ocm_dataset.csv\"\n",
    "# raw_data = pd.read_csv(data_path, sep=\";\")\n",
    "# # raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "# # raw_data['completion'] = - raw_data['completion']\n",
    "\n",
    "# x_name = \"prompt\"\n",
    "# y_name = \"completion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"ocm\", M=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(0,8)\n",
    "raw_data = pd.read_csv(\"dataset/data/1180_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[0], \"./out/C2_davinci_1180_1_tree.pkl\",\"20\", \n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, data_file_random=\"./out/C2 - random - 1180.pkl\")\n",
    "\n",
    "lim=(0,10)\n",
    "raw_data = pd.read_csv(\"dataset/data/2950_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[1], \"./out/C2_davinci_2950_1_tree.pkl\",\"50\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 2950.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/5900_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[2], \"./out/C2_davinci_5900_1_tree.pkl\", \"100\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 5900.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/12744_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[3], \"./out/C2_davinci_12744_1_tree_2.pkl\", \"216\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "fig.suptitle(\"TreePool with davinci\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = [\n",
    "    './out/C2_davinci_1180_1_tree.pkl',\n",
    "    './out/C2_davinci_2950_1_tree.pkl',\n",
    "    './out/C2_davinci_5900_1_tree.pkl',\n",
    "    './out/C2_davinci_12744_1_tree.pkl',\n",
    "]\n",
    "for p in pools:\n",
    "    print(p)\n",
    "    d = cloudpickle.load(open(p, \"rb\"))\n",
    "    for run in d['upper_confidence_bound'][:, :, 0]:\n",
    "        print([r[14:r.find(\",\")] for r in run])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(0,8)\n",
    "raw_data = pd.read_csv(\"dataset/data/1180_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[0], \"./out/C2_davinci_1180_1_16hh.pkl\",\"20\", \n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, data_file_random=\"./out/C2 - random - 1180.pkl\")\n",
    "\n",
    "lim=(0,10)\n",
    "raw_data = pd.read_csv(\"dataset/data/2950_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[1], \"./out/C2_davinci_2950_1_16hh.pkl\",\"50\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 2950.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/5900_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[2], \"./out/C2_davinci_5900_1_16hh.pkl\", \"100\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 5900.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/12744_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[3], \"./out/C2_davinci_12744_1_16hh.pkl\", \"216\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "         \n",
    "\n",
    "fig.suptitle(\"Subpool of 16 with half random samples with davinci\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = [\n",
    "    './out/C2_davinci_1180_1_16hh.pkl',\n",
    "    './out/C2_davinci_2950_1_16hh.pkl',\n",
    "    './out/C2_davinci_5900_1_16hh.pkl',\n",
    "    './out/C2_davinci_12744_1_16hh.pkl',\n",
    "]\n",
    "for p in pools:\n",
    "    print(p)\n",
    "    d = cloudpickle.load(open(p, \"rb\"))\n",
    "    for run in d['upper_confidence_bound'][:, :, 0]:\n",
    "        print([r[14:r.find(\",\")] for r in run])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/12744_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[0], \"./out/C2_davinci_fulldataset_new_subpool_16_allrandom_1init.pkl\",\"all_random\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "plot_BO(axs[1], \"./out/C2_davinci_12744_1_16hh.pkl\",\"half_random\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "plot_BO(axs[2], \"./out/C2_davinci_fulldataset_subpool_16_no_random_1_init.pkl\", \"no_random\",\n",
    "# plot_BO(axs[2], \"./out/C2_davinci_fulldataset_subpool_16_no_random_newest_seed0_2_init.pkl\", \"no_random\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "plot_BO(axs[3], \"./out/C2_davinci_12744_1_tree_2.pkl\", \"TreePool\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "         \n",
    "\n",
    "fig.suptitle(\"216 samples with each subpool\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = [\n",
    "    # './out/C2_davinci_fulldataset_new_subpool_16_allrandom_1init.pkl',\n",
    "    # './out/C2_davinci_12744_1_16hh.pkl',\n",
    "    # './out/C2_davinci_fulldataset_subpool_16_no_random_1_init.pkl',\n",
    "    './out/C2_davinci_fulldataset_subpool_16_no_random_newest_seed0_2_init.pkl',\n",
    "    # './out/C2_davinci_12744_1_tree.pkl',\n",
    "]\n",
    "for p in pools:\n",
    "    print(p)\n",
    "    d = cloudpickle.load(open(p, \"rb\"))\n",
    "    for k in d.keys():\n",
    "        for run in d[k][:, :, 0]:\n",
    "            print(k, [r[14:r.find(\",\")] for r in run])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_davinci = cloudpickle.load(open(\"paper/out/C2_davinci_100.pkl\", \"rb\"))\n",
    "print(d_davinci['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "d_gpt4 = cloudpickle.load(open(\"paper/out/C2_GPT4_100.pkl\", \"rb\"))\n",
    "print(d_gpt4['upper_confidence_bound'][:, -1, 1].astype(float))\n",
    "best_gpt4 = d_gpt4['upper_confidence_bound'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "d_gpr = cloudpickle.load(open(\"paper/out/C2_GPR_100.pkl\", \"rb\"))\n",
    "print(d_gpr['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_gpr = d_gpr['upper_confidence_bound'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "# print(np.sum(raw_data[y_name] > best))\n",
    "plt.xlabel(\"measured C$_2$ yield\")\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iupac-sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data_path = \"paper/data/esol_iupac.csv\"\n",
    "raw_data = pd.read_csv(data_path)\n",
    "raw_data = raw_data.dropna()\n",
    "raw_data = raw_data[[\"IUPAC\", \"measured log(solubility:mol/L)\"]]\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# raw_data['measured log(solubility:mol/L)'] = -raw_data['measured log(solubility:mol/L)']\n",
    "x_name = \"IUPAC\"\n",
    "y_name = \"measured log(solubility:mol/L)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"sol\", M=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12,4), constrained_layout=True)\n",
    "for ax in axs.flat:\n",
    "    ax.set_aspect(1.8)\n",
    "\n",
    "lim=(-5.5,2)\n",
    "\n",
    "plot_BO(axs[0], \"paper/out/sol_davinci_100.pkl\", \"Topk - davinci\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=False, data_file_random=\"paper/out/sol - random.pkl\")\n",
    "plot_BO(axs[1], \"paper/out/sol_gpt4_100.pkl\", \"Topk - GPT-4\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=False, data_file_random=\"paper/out/sol - random.pkl\")\n",
    "plot_BO(axs[2], \"paper/out/sol_GPR_100.pkl\", \"GPR\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=True, data_file_random=\"paper/out/sol - random.pkl\")\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_davinci = cloudpickle.load(open(\"paper/out/sol_davinci_100.pkl\", \"rb\"))\n",
    "d_gpt4 = cloudpickle.load(open(\"paper/out/sol_gpt4_100.pkl\", \"rb\"))\n",
    "d_gpr = cloudpickle.load(open(\"paper/out/sol_GPR_100.pkl\", \"rb\"))\n",
    "\n",
    "print(d_davinci['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "print(d_gpt4['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_gpt4 = d_gpt4['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "print(d_gpr['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_gpr = d_gpr['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci-0.08])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
