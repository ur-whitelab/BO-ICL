{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "initial_train = 3\n",
    "ask_K = 1\n",
    "N=20\n",
    "M=2\n",
    "lambda_multi = 0.1\n",
    "# model=\"gpt-3.5-turbo-instruct\"\n",
    "model=\"gpt-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I saved the number of samples in the training set in the pickle file. With that, old pickle weren't compatible with the new printing format.\n",
    "# This script adds an array [0 .. len(samples)] to the old pickle files, so that they are compatible with the new printing format.\n",
    "\n",
    "# import numpy as np\n",
    "# import cloudpickle\n",
    "# d = cloudpickle.load(open(\"./out/<OLD_FORMAT_BO>.pkl\", \"rb\"))\n",
    "\n",
    "# data = d\n",
    "# for key, values in data.items():\n",
    "#     num_aqs = values.shape[0]\n",
    "#     num_entries = values.shape[1]\n",
    "#     new_values = np.empty((num_aqs, num_entries, 3), dtype='<U349')\n",
    "    \n",
    "#     for j in range(num_aqs):\n",
    "#         for i in range(num_entries):\n",
    "#             description, value = values[j, i]\n",
    "#             new_values[j, i] = [description, str(i+1), value]\n",
    "#     data[key] = new_values\n",
    "\n",
    "# cloudpickle.dump(data, open(\"./out/<NEW_FORMAT_BO>.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in-house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"in-house\"\n",
    "kwargs = dict(\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@\\nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@\\nA:\",\n",
    "    x_formatter=lambda x: f\"experimental procedure: {x}\",\n",
    "    y_name=\"CO STY\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    "    selector_k=5,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "path = f\"./out/{dataset}_{model}_42000_{initial_train}_{ask_K}_16nr.pkl\"\n",
    "pool_path = \"./dataset/data/42000_in-house_pool.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"ocm\"\n",
    "kwargs = dict(\n",
    "    # prefix=\"You are a bot who knows chemistry and catalysts. \" \\\n",
    "    #         \"Below, you'll see examples of experimental procedures to synthesize catalysts and the measured C2 yield in a oxidative methane coupling reaction. \" \\\n",
    "    #         \"The following question should be answered with a number and finished with ###\\n\",\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@\\nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@\\nA:\",\n",
    "    x_formatter=lambda x: f\"experimental procedure: {x}\",\n",
    "    y_name=\"C2 yield\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    "    selector_k=5,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "inv_system_message_path = \"./prompts/inv_prompt_1.txt\"\n",
    "system_message_path = \"./prompts/prompt_1.txt\"\n",
    "\n",
    "path = f\"./out/{dataset}_{model}_12744_{initial_train}_{ask_K}_16nr.pkl\"\n",
    "pool_path = \"./dataset/data/12744_ocm_pool.pkl\"\n",
    "\n",
    "path = f\"./out/inv_system_message_test_{lambda_multi}.pkl\"\n",
    "pool_path = \"./dataset/data/C2_yield_meth_oxy_short_corrected.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solubility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"sol\"\n",
    "kwargs = dict(\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@\\nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@\\nA:\",\n",
    "    x_formatter=lambda x: f\"iupac name {x}\",\n",
    "    y_name=\"measured log solubility in mols per litre\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    "    selector_k=5,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "path = f\"./out/{dataset}_{model}_882_{initial_train}_{ask_K}_16nr.pkl\"\n",
    "pool_path = \"./out/sol_pool.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import bolift\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import copy, cloudpickle\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/google/fonts/raw/main/ofl/ibmplexmono/IBMPlexMono-Regular.ttf\",\n",
    "    \"IBMPlexMono-Regular.ttf\",\n",
    ")\n",
    "fe = font_manager.FontEntry(fname=\"IBMPlexMono-Regular.ttf\", name=\"plexmono\")\n",
    "font_manager.fontManager.ttflist.append(fe)\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.facecolor\": \"#f5f4e9\",\n",
    "        \"grid.color\": \"#AAAAAA\",\n",
    "        \"axes.edgecolor\": \"#333333\",\n",
    "        \"figure.facecolor\": \"#FFFFFF\",\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.prop_cycle\": plt.cycler(\"color\", plt.cm.Dark2.colors),\n",
    "        \"font.family\": fe.name,\n",
    "        \"figure.figsize\": (3.5, 3.5 / 1.2),\n",
    "        \"ytick.left\": True,\n",
    "        \"xtick.bottom\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "import random\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# import uncertainty_toolbox as uct\n",
    "\n",
    "def combine(s, l):\n",
    "  '''Number of combinations of l elements with max = s'''\n",
    "  return (s**l - (s-1)**(l))\n",
    "\n",
    "def prob(s, l, n):\n",
    "  '''Probability of getting a sample with max([x0,x1,...,xl]) = s where xi={0,n}'''\n",
    "  return combine(s,l) * ((1/n)**l)\n",
    "\n",
    "def expected_value_p(l, n):\n",
    "  '''Expected value of max([x0,x1,...,xl]) where xi={0,n}'''\n",
    "  E = [s * prob(s, l, n) for s in range(1,100+1)]\n",
    "  return sum(E)\n",
    "\n",
    "def expected_value_q(l, n, data):\n",
    "  '''Expected value of max([x0,x1,...,xl]) where xi={0,n}'''\n",
    "  quants = [data.quantile(i/100) for i in range(100+1)]\n",
    "  # E = [(quants[s-1]) * prob(s, l, n) for s in range(1,100+1)]\n",
    "  E = [((quants[s-1]+quants[s])/2) * prob(s, l, n) for s in range(1,100+1)]\n",
    "  return sum(E)\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "# @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def run_experiment(\n",
    "    asktell, pool, raw_data, indexes, x_name, y_name, N=10, initial_train=1, ask_K=1, aq=\"random\", start_index=0, calibrate=False,\n",
    "    lambda_multi=0.1, system_message=\"\", inv_system_message=\"\"\n",
    "):\n",
    "    if aq=='random_mean':\n",
    "       return [ (i, expected_value_q(i, 100, raw_data[y_name])) for i in range(1,N+1) ]\n",
    "    for i in indexes[:initial_train]:\n",
    "        asktell.tell(raw_data[x_name].iloc[i], float(raw_data[y_name].iloc[i]))\n",
    "    if calibrate:\n",
    "        # y = [float(raw_data[y_name].iloc[i]) for i in indexes[:initial_train]]\n",
    "        # pred = asktell.predict(y)\n",
    "        # ymeans = np.array([yhi.mean() for yhi in pred])\n",
    "        # ystds = np.array([yhi.std() for yhi in pred])\n",
    "        # calibration_factor = uct.recalibration.optimize_recalibration_ratio(ymeans, ystds, np.array(y), criterion=\"miscal\")\n",
    "        calibration_factor = 5.0\n",
    "        asktell.set_calibration_factor(calibration_factor)\n",
    "\n",
    "    x = raw_data[x_name].tolist()\n",
    "\n",
    "    pool.reset()\n",
    "    xi = x[start_index]\n",
    "    x.remove(xi)\n",
    "    pool.choose(xi)\n",
    "    yi = float(raw_data[raw_data[x_name] == xi][y_name].iloc[0])\n",
    "    asktell.tell(xi, yi)\n",
    "    point = [(xi, 1+initial_train, yi, yi)]\n",
    "    best = point[0][-1]\n",
    "    for i in range(1, N):\n",
    "        if i == N - 1 and aq != \"random\":\n",
    "            aq = \"greedy\"\n",
    "        px, _, py = asktell.ask(pool,\n",
    "                                k=ask_K,\n",
    "                                aq_fxn=aq,\n",
    "                                _lambda=1.0,\n",
    "                                inv_filter=16,\n",
    "                                aug_random_filter=0,\n",
    "                                lambda_mult=lambda_multi,\n",
    "                                system_message=system_message,\n",
    "                                inv_system_message=inv_system_message,\n",
    "                                )\n",
    "        for j in range(ask_K):\n",
    "          xc = px[j]\n",
    "          x.remove(xc)\n",
    "          pool.choose(xc)\n",
    "          y = float(raw_data[raw_data[x_name] == xc][y_name].iloc[0])\n",
    "          asktell.tell(xc, y)\n",
    "          best = max(y, best)\n",
    "        point.append((xc, 1+initial_train+i*ask_K, best, y))\n",
    "    return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(data: str, M=5):\n",
    "    match data:\n",
    "        case \"in-house\":\n",
    "            data_path = \"./dataset/data/71023_BO_ready_pool.csv\"\n",
    "            raw_data = pd.read_csv(data_path)\n",
    "\n",
    "            raw_data['Catalyst'] = raw_data['Prompt'].str.extract(r'(\\b[A-Z][a-z]?:[A-Z][a-z]?:[A-Z][a-z]?\\b)')\n",
    "            unique_cat = raw_data['Catalyst'].unique()\n",
    "            c = {c: 0.2+m*(5/len(unique_cat)) for m, c in enumerate(unique_cat)}\n",
    "            raw_data['dummy_Completion'] = raw_data['Catalyst'].apply(lambda x: np.random.normal(c[x], 0.05))\n",
    "\n",
    "            x_name = \"Prompt\"\n",
    "            y_name = \"dummy_Completion\"\n",
    "        case \"ocm\":\n",
    "            data_path = \"./dataset/data/C2_yield_meth_oxy_short_corrected.csv\"\n",
    "            raw_data = pd.read_csv(data_path, sep=\",\")\n",
    "            raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "            x_name = \"prompt\"\n",
    "            y_name = \"completion\"\n",
    "        case \"sol\":\n",
    "            data_path = \"./dataset/data/esol_iupac.csv\"\n",
    "            raw_data = pd.read_csv(data_path)\n",
    "            raw_data = raw_data.dropna()\n",
    "            raw_data = raw_data[[\"IUPAC\", \"measured log(solubility:mol/L)\"]]\n",
    "            x_name = \"IUPAC\"\n",
    "            y_name = \"measured log(solubility:mol/L)\"\n",
    "        case _:\n",
    "            raise ValueError(\"Unknown data\")\n",
    "        \n",
    "    N = raw_data.shape[0]\n",
    "    indexes = np.random.choice(raw_data.shape[0], int(N), replace=False)\n",
    "\n",
    "    print(f\"Dataset size: \\n\\t{N}\")\n",
    "    starts = raw_data.sort_values(by=y_name, ascending=True).head(100).sample(M)# np.random.randint(0, len(indexes), M)\n",
    "    print(f\"Start xs: \\n\\t{starts[x_name].to_list()}\")\n",
    "    print(f\"Start ys: \\n\\t{starts[y_name].to_list()}\")\n",
    "    starts = starts.index\n",
    "    print(f\"Start indexes: \\n\\t{starts}\\n\")\n",
    "\n",
    "    return raw_data, starts, indexes, x_name, y_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_asktell(model: str, kwargs: dict = {}, pool: bolift.Pool = None, knn: int = 1):\n",
    "    match model:\n",
    "        case \"instruct\":\n",
    "            kwargs['model']=\"gpt-3.5-turbo-instruct\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-turbo\":\n",
    "            kwargs['model']=\"gpt-3.5-turbo-0125\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4\":\n",
    "            kwargs['model']=\"gpt-4\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"davinci\":\n",
    "            kwargs['model']=\"davinci-002\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"curie\":\n",
    "            kwargs['model']=\"text-curie-001\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpr\":\n",
    "            kwargs['selector_k'] = 0\n",
    "            kwargs['pool'] = pool if pool else None\n",
    "            kwargs['n_components'] = 32\n",
    "            return bolift.AskTellGPR(**kwargs)\n",
    "        # knn and krr don't output uncertainties\n",
    "        # case \"knn\":\n",
    "        #     del kwargs['selector_k']\n",
    "        #     kwargs['knn'] = knn\n",
    "        #     return bolift.AskTellNearestNeighbor(**kwargs)\n",
    "        # case \"krr\":\n",
    "        #     kwargs['alpha'] = 0.5\n",
    "        #     return bolift.AskTellRidgeKernelRegression(**kwargs)\n",
    "        case _:\n",
    "            raise ValueError(\"Unknown model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_bkp(path, pool_path, indexes, kwargs):\n",
    "    if os.path.exists(pool_path):\n",
    "        with open(pool_path, \"rb\") as f:\n",
    "            pool = cloudpickle.load(f)\n",
    "        pool.reset()\n",
    "    else:\n",
    "        x = [raw_data[x_name].iloc[i] for i in indexes]\n",
    "        pool = bolift.Pool(list(x), formatter=kwargs['x_formatter'])\n",
    "        # cloudpickle.dump(pool, open(pool_path, \"wb\"))\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        bayesOpts = cloudpickle.load(open(path, \"rb\"))\n",
    "    else:\n",
    "        bayesOpts = {}\n",
    "    return bayesOpts, pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BayesOpt experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Changing the sparsity structure of a csr_matrix is expensive.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not contained to the unit cube.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not standardized.*')\n",
    "warnings.filterwarnings('ignore', message=\"Keyword arguments .* will be ignored because they are not allowed parameters for function .*\", category=UserWarning)\n",
    "\n",
    "raw_data, starts, indexes, x_name, y_name = get_dataset(dataset, M=M)\n",
    "bayesOpts, pool = read_bkp(path, pool_path, indexes, kwargs)\n",
    "\n",
    "if os.path.exists(system_message_path):\n",
    "    with open(system_message_path, \"r\") as f:\n",
    "        system_message = f.read()\n",
    "\n",
    "if os.path.exists(inv_system_message_path):\n",
    "    with open(inv_system_message_path, \"r\") as f:\n",
    "        inv_system_message = f.read()\n",
    "\n",
    "asktell = get_asktell(model, kwargs=kwargs)#, pool=bolift.Pool(list(pool.sample(5000))), knn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Changing the sparsity structure of a csr_matrix is expensive.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not contained to the unit cube.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not standardized.*')\n",
    "warnings.filterwarnings('ignore', message=\"Keyword arguments .* will be ignored because they are not allowed parameters for function .*\", category=UserWarning)\n",
    "\n",
    "for aq in [ 'expected_improvement', 'random_mean', 'random']: #[\"random_mean\", \"random\", \"expected_improvement\", \"greedy\", 'upper_confidence_bound', 'probability_of_improvement']:\n",
    "    print(aq, \"start:\", end=\" \")\n",
    "    points = []\n",
    "    for i in range(M):\n",
    "        print(i, end=\",  \")\n",
    "        point = run_experiment(\n",
    "            copy.deepcopy(asktell),\n",
    "            pool, # copy.deepcopy(pool),\n",
    "            raw_data,\n",
    "            indexes,\n",
    "            x_name,\n",
    "            y_name,\n",
    "            N=N,\n",
    "            aq=aq,\n",
    "            start_index=starts[i],\n",
    "            calibrate=True,\n",
    "            initial_train=initial_train,\n",
    "            ask_K=ask_K,\n",
    "            lambda_multi=lambda_multi,\n",
    "            system_message=system_message,\n",
    "            inv_system_message=inv_system_message\n",
    "        )\n",
    "        points.append(point)\n",
    "    points = np.array(points)\n",
    "    bayesOpts[aq] = points\n",
    "    print(aq, \"done\")\n",
    "    # if isinstance(asktell, bolift.AskTellGPR):\n",
    "    #     asktell.save_cache(\"GPR_ada_embed_cache.csv\")\n",
    "    cloudpickle.dump(bayesOpts, open(path, \"wb\"))\n",
    "cloudpickle.dump(bayesOpts, open(path, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = cloudpickle.load(open(\"./out/inv_system_message_test_0.1.pkl\", \"rb\"))\n",
    "d = bayesOpts\n",
    "exps = d['expected_improvement']\n",
    "for i, step in enumerate(exps[0]):\n",
    "    print(f\"Sample {i:>2d}: {' | '.join(exp[i][0][15:30] for exp in exps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "path = \"./out/ocm_gpt-4_12744_1_1_test.pkl\"\n",
    "title = \"\" #f\"{path[6:-4]}\" \n",
    "# d = cloudpickle.load(open(path, \"rb\"))\n",
    "d = bayesOpts\n",
    "data=raw_data[y_name]\n",
    "lim=(data.min()-1, data.max()+1)\n",
    "\n",
    "# name = \"LogS\"\n",
    "name = \"C$_2$ yield\"\n",
    "# name = \"CO STY\"\n",
    "\n",
    "def plot_config():\n",
    "    plt.title(title)\n",
    "    plt.axhline(y=data.max(), color=\"C15\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.max(), \"max\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.quantile(0.99), color=\"C14\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.quantile(0.99), \"99%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.quantile(0.95), color=\"C13\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.quantile(0.95), \"95%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.mean(), color=\"C12\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.mean(), \"mean\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.quantile(0.05), color=\"C11\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.quantile(0.05), \"5%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.min(), color=\"C10\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.min(), \"Min\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.ylim(lim)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5,-0.1),\n",
    "          fancybox=True, shadow=True, ncol=3)\n",
    "\n",
    "#Debugging plots\n",
    "# Plot best value on the entire run\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xlabel(\"Number of samples\")\n",
    "# plt.ylabel(\"Max C$_2$ yield\")\n",
    "plt.ylabel(f\"Max {name}\")\n",
    "for i, acq in enumerate(d.keys()):\n",
    "    if acq == \"log_expected_improvement\":\n",
    "        continue\n",
    "    if acq == \"random_mean\":\n",
    "        plt.plot(d[acq][0,:,0].astype(int), d[acq][:, :, 1].astype(float).mean(axis=0), \n",
    "                 label=f\"random\", color=\"gray\", linestyle=\"dashed\")\n",
    "    else:\n",
    "        for j in range(M):\n",
    "            plt.plot(d[acq][j,:,1].astype(int), d[acq][j, :, 2].astype(float), alpha=0.2, color=f\"C{i}\")\n",
    "        plt.plot(d[acq][0,:,1].astype(int), d[acq][:, :, 2].astype(float).mean(axis=0), label=acq, color=f\"C{i}\")\n",
    "plot_config()\n",
    "plt.show()\n",
    "\n",
    "# Plot current values on each iteration\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(f\"{name}\")\n",
    "for acq in d.keys():\n",
    "    if acq == \"random_mean\" or acq == \"log_expected_improvement\" or acq == \"random\":\n",
    "        continue\n",
    "    else:\n",
    "        for i in range(M):\n",
    "            plt.plot(d[acq][i,:,1], d[acq][i, :, 3].astype(float), label=f\"{acq}:{i}\", alpha=0.2)\n",
    "        plt.plot(d[acq][0,:,1], d[acq][:, :, 3].astype(float).mean(axis=0), label=f\"{acq}\")\n",
    "plot_config()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesOpt Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_BO(ax, data_file, title, data, axis_name, lim=None, label=False, M=1):\n",
    "    d = cloudpickle.load(open(data_file, \"rb\"))\n",
    "\n",
    "    for i in range(M):\n",
    "        if \"expected_improvement\" in d.keys():\n",
    "          ax.plot(\n",
    "            [int(s) for s in d['expected_improvement'][i, :, 1]],\n",
    "            [float(y) for y in d['expected_improvement'][i, :, 2]], \n",
    "            color=\"C1\", alpha=0.2\n",
    "          )\n",
    "        if \"greedy\" in d.keys():\n",
    "           ax.plot(\n",
    "            [int(s) for s in d['greedy'][i, :, 1]],\n",
    "            [float(y) for y in d['greedy'][i, :, 2]], \n",
    "            color=\"C2\", alpha=0.2\n",
    "          )\n",
    "        if \"upper_confidence_bound\" in d.keys():\n",
    "          ax.plot(\n",
    "            [int(s) for s in d['upper_confidence_bound'][i, :, 1]],\n",
    "            [float(y) for y in d['upper_confidence_bound'][i, :, 2]], \n",
    "            color=\"C3\", alpha=0.2\n",
    "          )\n",
    "        if \"probability_of_improvement\" in d.keys():\n",
    "          ax.plot(\n",
    "            [int(s) for s in d['probability_of_improvement'][i, :, 1]],\n",
    "            [float(y) for y in d['probability_of_improvement'][i, :, 2]], \n",
    "            color=\"C4\", alpha=0.2\n",
    "          )\n",
    "        if \"random\" in d.keys():\n",
    "          ax.plot(\n",
    "            [int(s) for s in d['random'][i, :, 1]],\n",
    "            [float(y) for y in d['random'][i, :, 2]], \n",
    "            color=\"C5\", alpha=0.2\n",
    "          )\n",
    "    if \"expected_improvement\" in d.keys():\n",
    "      label = \"EI\" if label else None\n",
    "      ax.plot(\n",
    "            d['expected_improvement'][:, :, 1].astype('int').mean(axis=0),\n",
    "            d['expected_improvement'][:, :, 2].astype('float').mean(axis=0), \n",
    "            color=\"C1\", label=label\n",
    "          )\n",
    "    if \"greedy\" in d.keys():\n",
    "      label = \"Greedy\" if label else None\n",
    "      ax.plot(\n",
    "            d['greedy'][:, :, 1].astype('int').mean(axis=0),\n",
    "            d['greedy'][:, :, 2].astype('float').mean(axis=0), \n",
    "            color=\"C2\", label=label\n",
    "          )\n",
    "    if \"upper_confidence_bound\" in d.keys():\n",
    "      label = \"UCB\" if label else None\n",
    "      ax.plot(\n",
    "            d['upper_confidence_bound'][:, :, 1].astype('int').mean(axis=0),\n",
    "            d['upper_confidence_bound'][:, :, 2].astype('float').mean(axis=0), \n",
    "            color=\"C3\", label=label\n",
    "          )\n",
    "    if \"probability_of_improvement\" in d.keys():\n",
    "      label = \"POI\" if label else None\n",
    "      ax.plot(\n",
    "            d['probability_of_improvement'][:, :, 1].astype('int').mean(axis=0),\n",
    "            d['probability_of_improvement'][:, :, 2].astype('float').mean(axis=0), \n",
    "            color=\"C4\", label=label\n",
    "          )\n",
    "    if \"random\" in d.keys():\n",
    "      label = \"random\" if label else None\n",
    "      ax.plot(\n",
    "            d['random'][:,:, 1].astype('int').mean(axis=0),\n",
    "            d['random'][:,:, 2].astype('float').mean(axis=0), \n",
    "            color=\"C5\", label=label\n",
    "          )\n",
    "    if \"random_mean\" in d.keys():\n",
    "      label = \"Random\" if label else None\n",
    "      ax.plot(\n",
    "            d['random_mean'][:, :, 0].astype('int').mean(axis=0),\n",
    "            d['random_mean'][:, :, 1].astype('float').mean(axis=0), \n",
    "            color=\"gray\", label=label, linestyle=\"dashed\"\n",
    "      )\n",
    "    ax.axhline(y=data.max(), color=\"C15\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.max(), \"max\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.axhline(y=data.quantile(0.99), color=\"C14\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.quantile(0.99), \"99%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.axhline(y=data.quantile(0.95), color=\"C13\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.quantile(0.95), \"95%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.axhline(y=data.mean(), color=\"C12\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.mean(), \"mean\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    if not data_file.startswith(\"./out/sol\"):\n",
    "      ax.axhline(y=data.quantile(0.05), color=\"C11\", linestyle=\"--\")\n",
    "      ax.text(ax.get_xlim()[1]+1, data.quantile(0.05)+0.3, \"5%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "      ax.axhline(y=data.min(), color=\"C10\", linestyle=\"--\")\n",
    "      ax.text(ax.get_xlim()[1]+1, data.min()-0.3, \"min\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.set_xlabel(\"Number of samples\")\n",
    "    ax.set_ylabel(f\"Measured {axis_name}\")\n",
    "    # ax.set_xticks([i for i in range(0,N+1,5)], [str(x * 1) for x in [i for i in range(0,N+1,5)]])\n",
    "    if lim:\n",
    "      ax.set_ylim(lim)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"ocm\", M=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(8,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(raw_data[y_name].min()-1, raw_data[y_name].max()+1)\n",
    "# plot_BO(axs[0], \"./out/ocm_curie_12744_1_1_16nr.pkl\",\"GPT3\", \n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=True, M=5)\n",
    "\n",
    "plot_BO(axs[0], \"./out/C2_davinci2_12744_1_16nr.pkl\",\"GPT3.5\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, M=5)\n",
    "\n",
    "plot_BO(axs[1], \"./out/ocm_GPT-4_12744_0_fix.pkl\", \"GPT4\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "plot_BO(axs[2], \"./out/C2_gpr_12744_1_16nr.pkl\", \"GPR\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "fig.suptitle(\"Bayesian Optimization on OCM dataset\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,4), constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(raw_data[y_name].min()-1, raw_data[y_name].max()+1)\n",
    "\n",
    "plot_BO(axs[0], \"./out/ocm_gpt35turbo_12744_1_1_test0.pkl\",\"GPT3.5-turbo: 1.2\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, M=5)\n",
    "\n",
    "plot_BO(axs[1], \"./out/ocm_gpt35instruct_12744_5_1_test8.pkl \", \"GPT3.5-turbo: 1.2\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "fig.suptitle(\"Bayesian Optimization on OCM dataset\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_curie = cloudpickle.load(open(\"./out/ocm_curie_12744_1_1_16nr.pkl\", \"rb\"))\n",
    "d_davinci = cloudpickle.load(open(\"./out/C2_davinci2_12744_1_16nr.pkl\", \"rb\"))\n",
    "d_gpt4 = cloudpickle.load(open(\"./out/ocm_GPT-4_12744_0_s.pkl\", \"rb\"))\n",
    "d_gpr = cloudpickle.load(open(\"./out/C2_gpr_12744_1_16nr.pkl\", \"rb\"))\n",
    "\n",
    "print(d_curie['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_curie = d_curie['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Curie is top{np.sum(raw_data[y_name] > best_curie)}: {best_curie}\")\n",
    "\n",
    "print(d_davinci['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "print(d_gpt4['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpt4 = d_gpt4['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "print(d_gpr['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpr = d_gpr['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "# print(np.sum(raw_data[y_name] > best))\n",
    "plt.xlabel(\"measured C$_2$ yield\")\n",
    "plt.axvline(best_curie, color='C4', linestyle='--', label=\"Curie\")\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"sol\", M=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(1.8)\n",
    "\n",
    "lim=(raw_data[y_name].mean()-1, raw_data[y_name].max()+1)\n",
    "\n",
    "plot_BO(axs[0], \"./out/sol_davinci_100.pkl\", \"GPT3.5\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=False)\n",
    "plot_BO(axs[1], \"./out/sol_gpt4_100.pkl\", \"GPT4\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=False)\n",
    "plot_BO(axs[2], \"./out/sol_GPR_100.pkl\", \"GPR\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=True)\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_davinci = cloudpickle.load(open(\"./out/sol_davinci_100.pkl\", \"rb\"))\n",
    "d_gpt4 = cloudpickle.load(open(\"./out/sol_gpt4_100.pkl\", \"rb\"))\n",
    "d_gpr = cloudpickle.load(open(\"./out/sol_GPR_100.pkl\", \"rb\"))\n",
    "\n",
    "print(d_davinci['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "print(d_gpt4['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpt4 = d_gpt4['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "print(d_gpr['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpr = d_gpr['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci-0.08])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(88)\n",
    "\n",
    "data_path = \"./dataset/data/12744_ocm_dataset.csv\"\n",
    "raw_data = pd.read_csv(data_path, sep=\";\")\n",
    "# raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "# raw_data['completion'] = - raw_data['completion']\n",
    "\n",
    "x_name = \"prompt\"\n",
    "y_name = \"completion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"ocm\", M=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(0,8)\n",
    "raw_data = pd.read_csv(\"dataset/data/1180_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[0], \"./out/C2_davinci_1180_1_tree.pkl\",\"20\", \n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, data_file_random=\"./out/C2 - random - 1180.pkl\")\n",
    "\n",
    "lim=(0,10)\n",
    "raw_data = pd.read_csv(\"dataset/data/2950_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[1], \"./out/C2_davinci_2950_1_tree.pkl\",\"50\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 2950.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/5900_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[2], \"./out/C2_davinci_5900_1_tree.pkl\", \"100\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 5900.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/12744_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[3], \"./out/C2_davinci_12744_1_tree_2.pkl\", \"216\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "fig.suptitle(\"TreePool with davinci\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = [\n",
    "    './out/C2_davinci_1180_1_tree.pkl',\n",
    "    './out/C2_davinci_2950_1_tree.pkl',\n",
    "    './out/C2_davinci_5900_1_tree.pkl',\n",
    "    './out/C2_davinci_12744_1_tree.pkl',\n",
    "]\n",
    "for p in pools:\n",
    "    print(p)\n",
    "    d = cloudpickle.load(open(p, \"rb\"))\n",
    "    for run in d['upper_confidence_bound'][:, :, 0]:\n",
    "        print([r[14:r.find(\",\")] for r in run])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(0,8)\n",
    "raw_data = pd.read_csv(\"dataset/data/1180_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[0], \"./out/C2_davinci_1180_1_16hh.pkl\",\"20\", \n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, data_file_random=\"./out/C2 - random - 1180.pkl\")\n",
    "\n",
    "lim=(0,10)\n",
    "raw_data = pd.read_csv(\"dataset/data/2950_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[1], \"./out/C2_davinci_2950_1_16hh.pkl\",\"50\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 2950.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/5900_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[2], \"./out/C2_davinci_5900_1_16hh.pkl\", \"100\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 5900.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/12744_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[3], \"./out/C2_davinci_12744_1_16hh.pkl\", \"216\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "         \n",
    "\n",
    "fig.suptitle(\"Subpool of 16 with half random samples with davinci\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = [\n",
    "    './out/C2_davinci_1180_1_16hh.pkl',\n",
    "    './out/C2_davinci_2950_1_16hh.pkl',\n",
    "    './out/C2_davinci_5900_1_16hh.pkl',\n",
    "    './out/C2_davinci_12744_1_16hh.pkl',\n",
    "]\n",
    "for p in pools:\n",
    "    print(p)\n",
    "    d = cloudpickle.load(open(p, \"rb\"))\n",
    "    for run in d['upper_confidence_bound'][:, :, 0]:\n",
    "        print([r[14:r.find(\",\")] for r in run])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/12744_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[0], \"./out/C2_davinci_fulldataset_new_subpool_16_allrandom_1init.pkl\",\"all_random\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "plot_BO(axs[1], \"./out/C2_davinci_12744_1_16hh.pkl\",\"half_random\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "plot_BO(axs[2], \"./out/C2_davinci_fulldataset_subpool_16_no_random_1_init.pkl\", \"no_random\",\n",
    "# plot_BO(axs[2], \"./out/C2_davinci_fulldataset_subpool_16_no_random_newest_seed0_2_init.pkl\", \"no_random\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "plot_BO(axs[3], \"./out/C2_davinci_12744_1_tree_2.pkl\", \"TreePool\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "         \n",
    "\n",
    "fig.suptitle(\"216 samples with each subpool\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = [\n",
    "    # './out/C2_davinci_fulldataset_new_subpool_16_allrandom_1init.pkl',\n",
    "    # './out/C2_davinci_12744_1_16hh.pkl',\n",
    "    # './out/C2_davinci_fulldataset_subpool_16_no_random_1_init.pkl',\n",
    "    './out/C2_davinci_fulldataset_subpool_16_no_random_newest_seed0_2_init.pkl',\n",
    "    # './out/C2_davinci_12744_1_tree.pkl',\n",
    "]\n",
    "for p in pools:\n",
    "    print(p)\n",
    "    d = cloudpickle.load(open(p, \"rb\"))\n",
    "    for k in d.keys():\n",
    "        for run in d[k][:, :, 0]:\n",
    "            print(k, [r[14:r.find(\",\")] for r in run])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_davinci = cloudpickle.load(open(\"paper/out/C2_davinci_100.pkl\", \"rb\"))\n",
    "print(d_davinci['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "d_gpt4 = cloudpickle.load(open(\"paper/out/C2_GPT4_100.pkl\", \"rb\"))\n",
    "print(d_gpt4['upper_confidence_bound'][:, -1, 1].astype(float))\n",
    "best_gpt4 = d_gpt4['upper_confidence_bound'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "d_gpr = cloudpickle.load(open(\"paper/out/C2_GPR_100.pkl\", \"rb\"))\n",
    "print(d_gpr['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_gpr = d_gpr['upper_confidence_bound'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "# print(np.sum(raw_data[y_name] > best))\n",
    "plt.xlabel(\"measured C$_2$ yield\")\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iupac-sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data_path = \"paper/data/esol_iupac.csv\"\n",
    "raw_data = pd.read_csv(data_path)\n",
    "raw_data = raw_data.dropna()\n",
    "raw_data = raw_data[[\"IUPAC\", \"measured log(solubility:mol/L)\"]]\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# raw_data['measured log(solubility:mol/L)'] = -raw_data['measured log(solubility:mol/L)']\n",
    "x_name = \"IUPAC\"\n",
    "y_name = \"measured log(solubility:mol/L)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"sol\", M=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12,4), constrained_layout=True)\n",
    "for ax in axs.flat:\n",
    "    ax.set_aspect(1.8)\n",
    "\n",
    "lim=(-5.5,2)\n",
    "\n",
    "plot_BO(axs[0], \"paper/out/sol_davinci_100.pkl\", \"Topk - davinci\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=False, data_file_random=\"paper/out/sol - random.pkl\")\n",
    "plot_BO(axs[1], \"paper/out/sol_gpt4_100.pkl\", \"Topk - GPT-4\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=False, data_file_random=\"paper/out/sol - random.pkl\")\n",
    "plot_BO(axs[2], \"paper/out/sol_GPR_100.pkl\", \"GPR\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=True, data_file_random=\"paper/out/sol - random.pkl\")\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_davinci = cloudpickle.load(open(\"paper/out/sol_davinci_100.pkl\", \"rb\"))\n",
    "d_gpt4 = cloudpickle.load(open(\"paper/out/sol_gpt4_100.pkl\", \"rb\"))\n",
    "d_gpr = cloudpickle.load(open(\"paper/out/sol_GPR_100.pkl\", \"rb\"))\n",
    "\n",
    "print(d_davinci['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "print(d_gpt4['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_gpt4 = d_gpt4['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "print(d_gpr['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_gpr = d_gpr['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci-0.08])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
