{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "initial_train = 1\n",
    "initial_transfer_train=0\n",
    "ask_K = 1\n",
    "N=30\n",
    "M=5\n",
    "lambda_multi = 0.1\n",
    "# model=\"gpt-3.5-instruct\"\n",
    "# model=\"gpt-4o\"\n",
    "# model=\"sonnet\"\n",
    "model = \"gemini-2.5-flash-preview\"\n",
    "# model=\"gpt-3.5-turbo\"\n",
    "# model=\"gpt-4-turbo\"\n",
    "# model=\"gpr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in-house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"in-house\"\n",
    "kwargs = dict(\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@nA:\",\n",
    "    x_formatter=lambda x: f\"experimental procedure: {x}\",\n",
    "    y_name=\"CO STY\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    "    selector_k=5,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "path = f\"./out/bias_free_ocmdataset_p_comp.pkl\"\n",
    "pool_path = \"./dataset/data/42000_in-house_pool.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"ocm\"\n",
    "kwargs = dict(\n",
    "    # prefix=\"You are a bot who knows chemistry and catalysts. \" \\\n",
    "    #         \"Below, you'll see examples of experimental procedures to synthesize catalysts and the measured C2 yield in a oxidative methane coupling reaction. \" \\\n",
    "    #         \"The following question should be answered with a number and finished with ###n\",\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@nA:\",\n",
    "    x_formatter=lambda x: f\"experimental procedure: {x}\",\n",
    "    y_name=\"C2 yield\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    "    selector_k=5,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "inv_system_message_path = \"./prompts/inv_prompt_1.txt\"\n",
    "system_message_path = \"./prompts/prompt_1.txt\"\n",
    "\n",
    "path = f\"./out/{dataset}_{model}_{initial_train}_{ask_K}_{lambda_multi}.pkl\"\n",
    "pool_path = \"./dataset/data/ocm_dataset.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transfer learning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 20\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "t_data_path=\"top_10_similar_subset.csv\"\n",
    "\n",
    "t_data_path = \"/Users/shane/repos/BO-LIFT/\" + t_data_path\n",
    "# transfer_data = pd.read_csv(t_data_path)\n",
    "transfer_data = pd.read_csv(\"dataset/data/C2_yield_meth_oxy_short_corrected.csv\")\n",
    "\n",
    "t_N = transfer_data.shape[0]\n",
    "t_indexes = np.random.choice(transfer_data.shape[0], int(t_N), replace=False)\n",
    "t_x_name = \"prompt\"\n",
    "t_y_name = \"completion\"\n",
    "len(t_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "client = OpenAI()\n",
    "\n",
    "model = \"text-embedding-3-large\"\n",
    "\n",
    "def get_embeddings(texts, model=model, batch_size=100):\n",
    "    all_embeddings = []\n",
    "    cleaned_texts = [text.replace(\"n\", \" \") if isinstance(text, str) else \"\" for text in texts]\n",
    "    for i in range(0, len(cleaned_texts), batch_size):\n",
    "        batch = cleaned_texts[i: i + batch_size]\n",
    "        embeddings_data = client.embeddings.create(input=batch, model=model).data\n",
    "        all_embeddings.extend([embedding.embedding for embedding in embeddings_data])\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "small_data_path = \"./dataset/data/bias_free_ocmdataset_p_comp.csv\" \n",
    "large_data_path = \"./dataset/data/C2_yield_meth_oxy_short_corrected.csv\"\n",
    "\n",
    "small_data = pd.read_csv(small_data_path)\n",
    "large_data = pd.read_csv(large_data_path)\n",
    "\n",
    "prompt_col = \"prompt\"\n",
    "completion_col = \"completion\"\n",
    "\n",
    "small_prompts = small_data[prompt_col].fillna(\"\").tolist()\n",
    "large_prompts = large_data[prompt_col].fillna(\"\").tolist()\n",
    "\n",
    "small_embeddings = get_embeddings(small_prompts)\n",
    "large_embeddings = get_embeddings(large_prompts)\n",
    "\n",
    "similarities = cosine_similarity(small_embeddings, large_embeddings)\n",
    "\n",
    "selected_indices = set()\n",
    "new_data_list = []\n",
    "\n",
    "for i, small_prompt in tqdm(enumerate(small_prompts), total=len(small_prompts)):\n",
    "\n",
    "    sorted_indices = np.argsort(similarities[i])[::-1]\n",
    "\n",
    "    count = 0\n",
    "    for index in sorted_indices:\n",
    "        if index not in selected_indices:\n",
    "            large_prompt = large_data.iloc[index][prompt_col]\n",
    "            completion_text = large_data.iloc[index][completion_col]\n",
    "            new_data_list.append({\"prompt\": large_prompt, \"completion\": completion_text})\n",
    "            selected_indices.add(index)\n",
    "            count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "\n",
    "new_data = pd.DataFrame(new_data_list)\n",
    "t_data_path = \"top_10_similar_subset.csv\"\n",
    "new_data.to_csv(t_data_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solubility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"sol\"\n",
    "kwargs = dict(\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@nA:\",\n",
    "    x_formatter=lambda x: f\"iupac name {x}\",\n",
    "    y_name=\"measured log solubility in mols per litre\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    "    selector_k=5,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "inv_system_message_path = \"./prompts/inv_prompt_sol.txt\"\n",
    "system_message_path = \"./prompts/prompt_sol.txt\"\n",
    "\n",
    "path = f\"./out/{dataset}_{model}_{initial_train}_{ask_K}_{lambda_multi}.pkl\"\n",
    "pool_path = \"./out/sol_pool.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alloy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"alloy\"\n",
    "kwargs = dict(\n",
    "    prefix=\"\",\n",
    "    prompt_template=PromptTemplate(\n",
    "        input_variables=[\"x\", \"y\", \"y_name\"],\n",
    "        template=\"Q: What is the {y_name} of {x}?@@@nA: {y}###\",\n",
    "    ),\n",
    "    suffix=\"What is the {y_name} of {x}?@@@nA:\",\n",
    "    x_formatter=lambda x: f\"the corresponding experimental procedure: {x}\",\n",
    "    y_name=\"the log(charge transfer) [coulombs/cmÂ²]\", # inverse prompt : If {y_name} is {y}, then {x_name} is @@@n{x}###\",\n",
    "    y_formatter=lambda y: f\"{y:.2f}\",\n",
    "    selector_k=5,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "inv_system_message_path = \"./prompts/alloy_inv_prompt_1.txt\"\n",
    "system_message_path = \"./prompts/ocm_prompt_1.txt\"\n",
    "\n",
    "path = f\"./out/{dataset}_{model}_{initial_train}_{ask_K}_{lambda_multi}.pkl\"\n",
    "pool_path = \"./dataset/data/alloy_pool.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maykcaldas/Documents/WhiteLab/BO-LIFT/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import copy, cloudpickle\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.join(current_dir,'..')\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "import bolift\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "transfer_data = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/google/fonts/raw/main/ofl/ibmplexmono/IBMPlexMono-Regular.ttf\",\n",
    "    \"IBMPlexMono-Regular.ttf\",\n",
    ")\n",
    "fe = font_manager.FontEntry(fname=\"IBMPlexMono-Regular.ttf\", name=\"plexmono\")\n",
    "font_manager.fontManager.ttflist.append(fe)\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.facecolor\": \"#f5f4e9\",\n",
    "        \"grid.color\": \"#AAAAAA\",\n",
    "        \"axes.edgecolor\": \"#333333\",\n",
    "        \"figure.facecolor\": \"#FFFFFF\",\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.prop_cycle\": plt.cycler(\"color\", plt.cm.Dark2.colors),\n",
    "        \"font.family\": fe.name,\n",
    "        \"figure.figsize\": (3.5, 3.5 / 1.2),\n",
    "        \"ytick.left\": True,\n",
    "        \"xtick.bottom\": True,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# import uncertainty_toolbox as uct\n",
    "\n",
    "def combine(s, l):\n",
    "  '''Number of combinations of l elements with max = s'''\n",
    "  return (s**l - (s-1)**(l))\n",
    "\n",
    "def prob(s, l, n):\n",
    "  '''Probability of getting a sample with max([x0,x1,...,xl]) = s where xi={0,n}'''\n",
    "  return combine(s,l) * ((1/n)**l)\n",
    "\n",
    "def expected_value_p(l, n):\n",
    "  '''Expected value of max([x0,x1,...,xl]) where xi={0,n}'''\n",
    "  E = [s * prob(s, l, n) for s in range(1,100+1)]\n",
    "  return sum(E)\n",
    "\n",
    "def expected_value_q(l, n, data):\n",
    "  '''Expected value of max([x0,x1,...,xl]) where xi={0,n}'''\n",
    "  quants = [data.quantile(i/100) for i in range(100+1)]\n",
    "  # E = [(quants[s-1]) * prob(s, l, n) for s in range(1,100+1)]\n",
    "  E = [((quants[s-1]+quants[s])/2) * prob(s, l, n) for s in range(1,100+1)]\n",
    "  return sum(E)\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "# @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def run_experiment(\n",
    "    asktell, pool, raw_data, indexes, x_name, y_name, N=1, initial_train=1, ask_K=1, aq=\"random\", start_index=0, calibrate=False,\n",
    "    lambda_multi=0.1, system_message=\"\", inv_system_message=\"\",transfer_train=1, transfer=False, trans_data=None, t_indexes=None\n",
    "):\n",
    "    if aq=='random_mean':\n",
    "       return [ (i, expected_value_q(i, 100, raw_data[y_name])) for i in range(1,N+initial_train) ]\n",
    "    \n",
    "    point=[]\n",
    "    py = 0 \n",
    "    mod_std = 0\n",
    "    counter = 0\n",
    "    for i in indexes[:initial_train]:\n",
    "        mean_value= np.mean(raw_data[y_name])\n",
    "        y_list = raw_data[y_name]\n",
    "        differences = np.abs(y_list - mean_value)\n",
    "        mean_index = np.argmin(differences)\n",
    "        \n",
    "        asktell.tell(raw_data[x_name].iloc[mean_index], float(raw_data[y_name].iloc[mean_index]))\n",
    "        \n",
    "        if counter == 0:\n",
    "           i_best = float(raw_data[y_name].iloc[mean_index])\n",
    "        else:\n",
    "           i_best = sorted(point, key=lambda i_points: i_points[-1])[-1][-3]\n",
    "\n",
    "        point.append((raw_data[x_name].iloc[mean_index],counter,i_best,float(raw_data[y_name].iloc[mean_index]),float(raw_data[y_name].iloc[mean_index]),mod_std))\n",
    "        counter+=1\n",
    "\n",
    "    if all([transfer, trans_data, t_indexes]):\n",
    "      for j in t_indexes[:transfer_train]:\n",
    "          asktell.tell(trans_data[x_name].iloc[j], float(trans_data[y_name].iloc[j]))\n",
    "\n",
    "    if calibrate: \n",
    "        # y = [float(raw_data[y_name].iloc[i]) for i in indexes[:initial_train]]\n",
    "        # pred = asktell.predict(y)\n",
    "        # ymeans = np.array([yhi.mean() for yhi in pred])\n",
    "        # ystds = np.array([yhi.std() for yhi in pred])\n",
    "        # calibration_factor = uct.recalibration.optimize_recalibration_ratio(ymeans, ystds, np.array(y), criterion=\"miscal\")\n",
    "        calibration_factor = 5.0\n",
    "        asktell.set_calibration_factor(calibration_factor)\n",
    "\n",
    "    x = raw_data[x_name].tolist()\n",
    "\n",
    "    pool.reset()\n",
    "    xi = x[start_index]\n",
    "    x.remove(xi)\n",
    "    pool.choose(xi)\n",
    "    yi = float(raw_data[raw_data[x_name] == xi][y_name].iloc[0])\n",
    "    asktell.tell(xi, yi)\n",
    "    best = yi\n",
    "    point.append((xi, 1+initial_train,best, yi))\n",
    "\n",
    "    for i in range(1, N):\n",
    "        if i == N - 1 and aq != \"random\":\n",
    "            aq = \"greedy\"\n",
    "        px, _, py = asktell.ask(pool,\n",
    "                                k=ask_K,\n",
    "                                aq_fxn=aq,\n",
    "                                _lambda=1.0,\n",
    "                                inv_filter=16,\n",
    "                                aug_random_filter=0,\n",
    "                                lambda_mult=lambda_multi,\n",
    "                                system_message=system_message,\n",
    "                                inv_system_message=inv_system_message,\n",
    "                                )\n",
    "        for j in range(ask_K):\n",
    "          xc = px[j]\n",
    "          x.remove(xc)\n",
    "          pool.choose(xc)\n",
    "          y = float(raw_data[raw_data[x_name] == xc][y_name].iloc[0])\n",
    "          asktell.tell(xc, y)\n",
    "          best = max(y, best)\n",
    "        point.append((xc, 1+initial_train+i*ask_K, best, y))\n",
    "        \n",
    "    return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(data: str, M=M):\n",
    "    match data:\n",
    "        case \"in-house\":\n",
    "            data_path = \"./dataset/data/71023_BO_ready_pool.csv\"\n",
    "            raw_data = pd.read_csv(data_path)\n",
    "\n",
    "            raw_data['Catalyst'] = raw_data['Prompt'].str.extract(r'(\\b[A-Z][a-z]?:[A-Z][a-z]?:[A-Z][a-z]?\\b)')\n",
    "            unique_cat = raw_data['Catalyst'].unique()\n",
    "            c = {c: 0.2+m*(5/len(unique_cat)) for m, c in enumerate(unique_cat)}\n",
    "            raw_data['dummy_Completion'] = raw_data['Catalyst'].apply(lambda x: np.random.normal(c[x], 0.05))\n",
    "\n",
    "            x_name = \"Prompt\"\n",
    "            y_name = \"dummy_Completion\"\n",
    "        case \"ocm\":\n",
    "            data_path = \"./dataset/data/12708_ocm_dataset.csv\" #C2_yield_meth_oxy_short_corrected.csv\n",
    "            raw_data = pd.read_csv(data_path, sep=\";\")\n",
    "            raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "            x_name = \"prompt\"\n",
    "            y_name = \"completion\"\n",
    "        case \"biasfree_ocm\":\n",
    "            data_path = \"./dataset/data/bias_free_ocmdataset_p_comp.csv\"\n",
    "            raw_data = pd.read_csv(data_path, sep=\",\")\n",
    "            raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "            x_name = \"prompt\"\n",
    "            y_name = \"completion\"\n",
    "        case \"sol\":\n",
    "            data_path = \"./dataset/data/esol_iupac.csv\"\n",
    "            raw_data = pd.read_csv(data_path)\n",
    "            raw_data = raw_data.dropna()\n",
    "            raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "            raw_data = raw_data[[\"IUPAC\", \"measured log(solubility:mol/L)\"]]\n",
    "            x_name = \"IUPAC\"\n",
    "            y_name = \"measured log(solubility:mol/L)\"\n",
    "        case \"alloy\":\n",
    "            data_path = \"./dataset/data/charge_transfer_dataset.csv\"\n",
    "            raw_data = pd.read_csv(data_path, sep=\",\")\n",
    "            raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "            x_name = \"prompt\"\n",
    "            y_name = \"completion\"\n",
    "        case _:\n",
    "            raise ValueError(\"Unknown data\")\n",
    "        \n",
    "    n_data = raw_data.shape[0]\n",
    "    indexes = np.random.choice(raw_data.shape[0], int(n_data), replace=False)\n",
    "\n",
    "    print(f\"Dataset size: \\n\\t{n_data}\")\n",
    "    # starts = raw_data.sort_values(by=y_name, ascending=True).head(520).sample(M+initial_train)\n",
    "    starts = raw_data.sort_values(by=y_name, ascending=True).sample(M+initial_train)# np.random.randint(0, len(indexes), M)\n",
    "    print(f\"Start xs: \\n\\t{starts[x_name].to_list()}\")\n",
    "    print(f\"Start ys: \\n\\t{starts[y_name].to_list()}\")\n",
    "    starts = starts.index\n",
    "    print(f\"Start indexes: \\n\\t{starts}\\n\")\n",
    "\n",
    "    return raw_data, starts, indexes, x_name, y_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_asktell(model: str, kwargs: dict = {}, pool: bolift.Pool = None, knn: int = 1):\n",
    "    match model:\n",
    "        case \"mistral-7b-instruct:free\":\n",
    "            kwargs['model']=\"openrouter/mistralai/mistral-7b-instruct:free\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gemini-2.5-flash-preview\":\n",
    "            kwargs['model']=\"openrouter/google/gemini-2.5-flash-preview\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-3.5-instruct\":\n",
    "            kwargs['model']=\"gpt-3.5-turbo-instruct\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-3.5-turbo\":\n",
    "            kwargs['model']=\"gpt-3.5-turbo-0125\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4\":\n",
    "            kwargs['model']=\"gpt-4\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4-turbo\":\n",
    "            kwargs['model']=\"gpt-4-0125-preview\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4o\":\n",
    "            kwargs['model']=\"gpt-4o\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpt-4o-mini\":\n",
    "            kwargs['model']=\"gpt-4o-mini-2024-07-18\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"haiku\":\n",
    "            kwargs['model']=\"claude-3-haiku-20240307\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"sonnet\":\n",
    "            kwargs['model']=\"claude-3-5-sonnet-20240620\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"opus\":\n",
    "            kwargs['model']=\"claude-3-opus-20240229\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"davinci\":\n",
    "            kwargs['model']=\"davinci-002\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"curie\":\n",
    "            kwargs['model']=\"text-curie-001\"\n",
    "            return bolift.AskTellFewShotTopk(**kwargs)\n",
    "        case \"gpr\":\n",
    "            kwargs['selector_k'] = 0\n",
    "            kwargs['pool'] = pool if pool else None\n",
    "            kwargs['n_components'] = 32\n",
    "            return bolift.AskTellGPR(**kwargs)\n",
    "        # knn and krr don't output uncertainties\n",
    "        # case \"knn\":\n",
    "        #     del kwargs['selector_k']\n",
    "        #     kwargs['knn'] = knn\n",
    "        #     return bolift.AskTellNearestNeighbor(**kwargs)\n",
    "        # case \"krr\":\n",
    "        #     kwargs['alpha'] = 0.5\n",
    "        #     return bolift.AskTellRidgeKernelRegression(**kwargs)\n",
    "        case _:\n",
    "            raise ValueError(\"Unknown model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_bkp(path, pool_path, indexes, kwargs):\n",
    "    if os.path.exists(pool_path):\n",
    "        with open(pool_path, \"rb\") as f:\n",
    "            pool = cloudpickle.load(f)\n",
    "        pool.reset()\n",
    "    else:\n",
    "        x = [raw_data[x_name].iloc[i] for i in indexes]\n",
    "        pool = bolift.Pool(list(x), formatter=kwargs['x_formatter'])\n",
    "        # cloudpickle.dump(pool, open(pool_path, \"wb\"))\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        bayesOpts = cloudpickle.load(open(path, \"rb\"))\n",
    "    else:\n",
    "        bayesOpts = {}\n",
    "    \n",
    "    return bayesOpts, pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BayesOpt experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: \n",
      "\t12708\n",
      "Start xs: \n",
      "\t['To synthesize Ce-Na2WO4/SiO2, SiO2 (1.0 g) was impregnated with 4.5 mL of an aqueous solution consisting of Ce (0.37 mol), Na (0.37 mol), W (0.185 mol), at 50 ÂºC for 6 h. Once activated the reaction is ran at 750 ÂºC. The total flow rate was 15 mL/min (Ar: 10.5 mL/min, CH4: 3.4 mL/min, O2: 1.1 mL/min), leading to a contact time of 0.5 s.', 'To synthesize Zn-Na2WO4/SiO2, SiO2 (1.0 g) was impregnated with 4.5 mL of an aqueous solution consisting of Zn (0.37 mol), Na (0.37 mol), W (0.185 mol), at 50 ÂºC for 6 h. Once activated the reaction is ran at 775 ÂºC. The total flow rate was 20 mL/min (Ar: 3.0 mL/min, CH4: 11.3 mL/min, O2: 5.7 mL/min), leading to a contact time of 0.38 s.', 'To synthesize Mn-K2WO4/SiO2, SiO2 (1.0 g) was impregnated with 4.5 mL of an aqueous solution consisting of Mn (0.37 mol), K (0.37 mol), W (0.185 mol), at 50 ÂºC for 6 h. Once activated the reaction is ran at 750 ÂºC. The total flow rate was 20 mL/min (Ar: 14.0 mL/min, CH4: 4.5 mL/min, O2: 1.5 mL/min), leading to a contact time of 0.38 s.', 'To synthesize Pd-Na2WO4/SiO2, SiO2 (1.0 g) was impregnated with 4.5 mL of an aqueous solution consisting of Pd (0.37 mol), Na (0.37 mol), W (0.185 mol), at 50 ÂºC for 6 h. Once activated the reaction is ran at 900 ÂºC. The total flow rate was 15 mL/min (Ar: 10.5 mL/min, CH4: 3.6 mL/min, O2: 0.9 mL/min), leading to a contact time of 0.5 s.', 'To synthesize Mn-FeMoO4/SiO2, SiO2 (1.0 g) was impregnated with 4.5 mL of an aqueous solution consisting of Mn (0.247 mol), Fe (0.185 mol), Mo (0.185 mol), at 50 ÂºC for 6 h. Once activated the reaction is ran at 775 ÂºC. The total flow rate was 15 mL/min (Ar: 10.5 mL/min, CH4: 3.0 mL/min, O2: 1.5 mL/min), leading to a contact time of 0.5 s.', 'To synthesize Mn-Na2WO4/BEA, BEA (1.0 g) was impregnated with 4.5 mL of an aqueous solution consisting of Mn (0.37 mol), Na (0.37 mol), W (0.185 mol), at 50 ÂºC for 6 h. Once activated the reaction is ran at 750 ÂºC. The total flow rate was 15 mL/min (Ar: 10.5 mL/min, CH4: 3.9 mL/min, O2: 0.6 mL/min), leading to a contact time of 0.5 s.']\n",
      "Start ys: \n",
      "\t[6.5, 9.13, 6.28, 1.18, 5.51, 7.37]\n",
      "Start indexes: \n",
      "\tIndex([538, 446, 4502, 5436, 8983, 9049], dtype='int64')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Changing the sparsity structure of a csr_matrix is expensive.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not contained to the unit cube.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not standardized.*')\n",
    "warnings.filterwarnings('ignore', message=\"Keyword arguments .* will be ignored because they are not allowed parameters for function .*\", category=UserWarning)\n",
    "\n",
    "raw_data, starts, indexes, x_name, y_name = get_dataset(dataset, M=M)\n",
    "bayesOpts, pool = read_bkp(path, pool_path, indexes, kwargs)\n",
    "\n",
    "if os.path.exists(system_message_path):\n",
    "    with open(system_message_path, \"r\") as f:\n",
    "        system_message = f.read()\n",
    "\n",
    "if os.path.exists(inv_system_message_path):\n",
    "    with open(inv_system_message_path, \"r\") as f:\n",
    "        inv_system_message = f.read()\n",
    "\n",
    "asktell = get_asktell(model, kwargs=kwargs) #, pool=bolift.Pool(list(pool.sample(500)))) #, knn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper_confidence_bound start: 0,  1,  2,  "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Changing the sparsity structure of a csr_matrix is expensive.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not contained to the unit cube.*')\n",
    "warnings.filterwarnings('ignore', message='Input data is not standardized.*')\n",
    "warnings.filterwarnings('ignore', message=\"Keyword arguments .* will be ignored because they are not allowed parameters for function .*\", category=UserWarning)\n",
    "warnings.filterwarnings('ignore', message=\"Unexpected type for token usage: <class 'NoneType'>\", category=UserWarning)\n",
    "\n",
    "for aq in [\n",
    "    'upper_confidence_bound', \n",
    "    'random_mean', \n",
    "    # 'expected_improvement',\n",
    "    # 'log_expected_improvement',\n",
    "    # 'probability_of_improvement', \n",
    "    'random',\n",
    "    'greedy', \n",
    "    ]:\n",
    "    print(aq, \"start:\", end=\" \")\n",
    "    points = []\n",
    "    for i in range(M):\n",
    "        print(i, end=\",  \")\n",
    "        \n",
    "        point = run_experiment(\n",
    "            get_asktell(model, kwargs=kwargs), #, pool=bolift.Pool(list(pool.sample(500)))), #copy.deepcopy(asktell),\n",
    "            pool, # copy.deepcopy(pool)\n",
    "            raw_data,\n",
    "            indexes=indexes,\n",
    "            x_name=x_name,\n",
    "            y_name=y_name,\n",
    "            N=N,\n",
    "            aq=aq,\n",
    "            start_index=starts[i+initial_train],\n",
    "            calibrate=True,\n",
    "            initial_train=initial_train,\n",
    "            ask_K=ask_K,\n",
    "            lambda_multi=lambda_multi,\n",
    "            system_message=system_message,\n",
    "            inv_system_message=inv_system_message,\n",
    "            transfer_train=initial_transfer_train,\n",
    "            transfer=False,\n",
    "            trans_data=None,\n",
    "            t_indexes=None\n",
    "            )\n",
    "        points.append(point)\n",
    "    points = np.array(points)\n",
    "    bayesOpts[aq] = points\n",
    "    print(aq, \"done\")\n",
    "    # if isinstance(asktell, bolift.AskTellGPR):\n",
    "    #     asktell.save_cache(\"GPR_ada_embed_cache.csv\")\n",
    "    cloudpickle.dump(bayesOpts, open(path, \"wb\"))\n",
    "cloudpickle.dump(bayesOpts, open(path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "# path = \"./out/sol_gpt-3.5-turbo_1_1.pkl\"\n",
    "# path = \"./out/ocm_gpt-3.5-turbo_1_1_0.1.pkl\"\n",
    "# path = \"./out/ocm_gpt-4-turbo_1_1_0.1.pkl\"\n",
    "# path = \"./out/ocm_gpt-4o_1_1_0.1.pkl\"\n",
    "# path = \"./out/ocm_gpr_1_1_0.1.pkl\"\n",
    "\n",
    "title = \"random\" #\"gpt-3.5-turbo-0125\" #f\"{path[6:-4]} \n",
    "d = cloudpickle.load(open(path, \"rb\"))\n",
    "d = bayesOpts\n",
    "data=raw_data[y_name]\n",
    "lim=(data.min()-1, data.max()+1)\n",
    "\n",
    "# name = \"LogS\"\n",
    "name = \"C$_2$ yield\"\n",
    "# name = \"CO STY\"\n",
    "\n",
    "def plot_config():\n",
    "    # plt.title(title)\n",
    "    plt.axhline(y=data.max(), color=\"C15\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.max(), \"max\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.quantile(0.99), color=\"C14\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.quantile(0.99), \"99%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.quantile(0.95), color=\"C13\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.quantile(0.95), \"95%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.mean(), color=\"C12\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.mean(), \"mean\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.quantile(0.05), color=\"C11\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.quantile(0.05), \"5%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.axhline(y=data.min(), color=\"C10\", linestyle=\"--\")\n",
    "    plt.text(plt.xlim()[1]+1, data.min(), \"Min\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    plt.ylim(lim)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5,-0.1),\n",
    "          fancybox=True, shadow=True, ncol=3)\n",
    "\n",
    "#Debugging plots\n",
    "# Plot best value on the entire run\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xlabel(\"Number of samples\")\n",
    "# plt.ylabel(\"Max C$_2$ yield\")\n",
    "plt.ylabel(f\"Max {name}\")\n",
    "\n",
    "for i, acq in enumerate(d.keys()):\n",
    "    if acq in [\"log_expected_improvement\",\"probability_of_improvement\"]:\n",
    "        continue\n",
    "    if acq == \"random_mean\":\n",
    "        plt.plot(d[acq][0,:,0].astype(int), d[acq][:, :, 1].astype(float).mean(axis=0), \n",
    "                 label=f\"random\", color=\"gray\", linestyle=\"dashed\")\n",
    "    else:\n",
    "        for j in range(M):\n",
    "            try:\n",
    "                plt.plot(d[acq][j,:,1].astype(int), d[acq][j,:,2].astype(float), alpha=0.1, color=f\"C{i}\")\n",
    "            except:\n",
    "                continue\n",
    "        plt.plot(d[acq][0,:,1].astype(int), d[acq][:, :, 2].astype(float).mean(axis=0), label=acq, color=f\"C{i}\")\n",
    "plot_config()\n",
    "# plt.ylim(-6, 2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"./out/figs/bo_{title}.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot current values on each iteration\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(f\"{name}\")\n",
    "for acq in d.keys():\n",
    "    if acq in [\"random_mean\", \"log_expected_improvement\"]:\n",
    "        continue\n",
    "    else:\n",
    "        for i in range(M):\n",
    "            plt.plot(d[acq][i,:,1], d[acq][i, :, 3].astype(float), label=f\"{acq}:{i}\", alpha=0.2)\n",
    "        plt.plot(d[acq][0,:,1], d[acq][:, :, 3].astype(float).mean(axis=0), label=f\"{acq}\")\n",
    "plot_config()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in d.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesOpt Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_BO(ax, data_file, title, data, axis_name, lim=None, label=False, M=1):\n",
    "    d = cloudpickle.load(open(data_file, \"rb\"))\n",
    "    for i in range(M):\n",
    "        # if \"expected_improvement\" in d.keys():\n",
    "        #     ax.plot(\n",
    "        #         [int(s) for s in d['expected_improvement'][i, :, 1]],\n",
    "        #         [float(y) for y in d['expected_improvement'][i, :, 2]],\n",
    "        #         color=\"C1\", alpha=0.2\n",
    "        #     )\n",
    "        if \"greedy\" in d.keys():\n",
    "            ax.plot(\n",
    "                [int(s) for s in d['greedy'][i, :, 1]],\n",
    "                [float(y) for y in d['greedy'][i, :, 2]],\n",
    "                color=\"C2\", alpha=0.2\n",
    "            )\n",
    "        if \"upper_confidence_bound\" in d.keys():\n",
    "          ax.plot(\n",
    "            [int(s) for s in d['upper_confidence_bound'][i, :, 1]],\n",
    "            [float(y) for y in d['upper_confidence_bound'][i, :, 2]], \n",
    "            color=\"C3\", alpha=0.2\n",
    "          )\n",
    "        if \"probability_of_improvement\" in d.keys():\n",
    "          ax.plot(\n",
    "            [int(s) for s in d['probability_of_improvement'][i, :, 1]],\n",
    "            [float(y) for y in d['probability_of_improvement'][i, :, 2]], \n",
    "            color=\"C4\", alpha=0.2\n",
    "          )\n",
    "        if \"random\" in d.keys():\n",
    "            ax.plot(\n",
    "                [int(s) for s in d['random'][i, :, 1]],\n",
    "                [float(y) for y in d['random'][i, :, 2]],\n",
    "                color=\"C8\", alpha=0.1\n",
    "            )\n",
    "\n",
    "    # Average curve with shadow (min-max range)\n",
    "    # if \"expected_improvement\" in d.keys():\n",
    "    #     label = \"EI\" if label else None\n",
    "    #     x_vals = d['expected_improvement'][:, :, 1].astype('int').mean(axis=0)\n",
    "    #     y_mean = d['expected_improvement'][:, :, 2].astype('float').mean(axis=0)\n",
    "    #     y_min = d['expected_improvement'][:, :, 2].astype('float').min(axis=0)\n",
    "    #     y_max = d['expected_improvement'][:, :, 2].astype('float').max(axis=0)\n",
    "    #     ax.plot(x_vals, y_mean, color=\"C1\", label=label)\n",
    "    #     ax.fill_between(x_vals, y_min, y_max, color=\"C1\", alpha=0.1)\n",
    "\n",
    "    if \"greedy\" in d.keys():\n",
    "        label = \"Greedy\" if label else None\n",
    "        x_vals = d['greedy'][:, :, 1].astype('int').mean(axis=0)\n",
    "        y_mean = d['greedy'][:, :, 2].astype('float').mean(axis=0)\n",
    "        y_min = d['greedy'][:, :, 2].astype('float').min(axis=0)\n",
    "        y_max = d['greedy'][:, :, 2].astype('float').max(axis=0)\n",
    "        ax.plot(x_vals, y_mean, color=\"C2\", label=label)\n",
    "        ax.fill_between(x_vals, y_min, y_max, color=\"C2\", alpha=0.1)\n",
    "\n",
    "    if \"upper_confidence_bound\" in d.keys():\n",
    "      label = \"UCB\" if label else None\n",
    "      ax.plot(\n",
    "            d['upper_confidence_bound'][:, :, 1].astype('int').mean(axis=0),\n",
    "            d['upper_confidence_bound'][:, :, 2].astype('float').mean(axis=0), \n",
    "            color=\"C3\", label=label\n",
    "          )\n",
    "    if \"probability_of_improvement\" in d.keys():\n",
    "      label = \"POI\" if label else None\n",
    "      ax.plot(\n",
    "            d['probability_of_improvement'][:, :, 1].astype('int').mean(axis=0),\n",
    "            d['probability_of_improvement'][:, :, 2].astype('float').mean(axis=0), \n",
    "            color=\"C4\", label=label\n",
    "          )\n",
    "    if \"random\" in d.keys():\n",
    "      label = \"random\" if label else None\n",
    "      ax.plot(\n",
    "            d['random'][:,:, 1].astype('int').mean(axis=0),\n",
    "            d['random'][:,:, 2].astype('float').mean(axis=0), \n",
    "            color=\"C8\", label=label\n",
    "          )\n",
    "    if \"random_mean\" in d.keys():\n",
    "        label = \"Random Mean\" if label else None\n",
    "        ax.plot(\n",
    "            d['random_mean'][:, :, 0].astype('int').mean(axis=0),\n",
    "            d['random_mean'][:, :, 1].astype('float').mean(axis=0),\n",
    "            color=\"gray\", label=label, linestyle=\"dashed\"\n",
    "        )\n",
    "\n",
    "    # Adding max, quantiles, and mean lines\n",
    "    ax.axhline(y=data.max(), color=\"C15\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.max(), \"max\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.axhline(y=data.quantile(0.99), color=\"C14\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.quantile(0.99), \"99%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.axhline(y=data.quantile(0.95), color=\"C13\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.quantile(0.95), \"95%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    ax.axhline(y=data.mean(), color=\"C12\", linestyle=\"--\")\n",
    "    ax.text(ax.get_xlim()[1]+1, data.mean(), \"mean\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "    if not data_file.startswith(\"./out/sol\"):\n",
    "        ax.axhline(y=data.quantile(0.05), color=\"C11\", linestyle=\"--\")\n",
    "        ax.text(ax.get_xlim()[1]+1, data.quantile(0.05)+0.3, \"5%\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "        ax.axhline(y=data.min(), color=\"C10\", linestyle=\"--\")\n",
    "        ax.text(ax.get_xlim()[1]+1, data.min()-0.3, \"min\", va=\"center\", ha=\"left\", backgroundcolor=\"w\", fontsize=8)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Number of samples\")\n",
    "    ax.set_ylabel(f\"Measured {axis_name}\")\n",
    "    if lim:\n",
    "        ax.set_ylim(lim)\n",
    "    ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"alloy\", M=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12,4), constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(raw_data[y_name].min()-1, raw_data[y_name].max()+1)\n",
    "\n",
    "plot_BO(axs[0], \"./out/alloy_gpt-3.5-turbo_0_1_0.1.pkl\",\"GPT3.5-turbo\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "# plot_BO(axs[1], \"./out/alloy_gpt-4-turbo_1_1_0.1.pkl\", \"GPT4-turbo\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "plot_BO(axs[1], \"./out/alloy_gpt-4o_1_1_0.1.pkl\", \"GPT4o\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, M=5)\n",
    "\n",
    "# plot_BO(axs[2], \"./out/allot_gpr_5_1_0.1.pkl\", \"GPR\",\n",
    "#          raw_data[y_name], \"C$_2$ yield\", lim, label=False, M=5)\n",
    "\n",
    "# fig.suptitle(\"Bayesian Optimization on OCM dataset\")\n",
    "plt.tight_layout()\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"out/figs/ocm_bo\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_curie = cloudpickle.load(open(\"./out/ocm_curie_12744_1_1_16nr.pkl\", \"rb\"))\n",
    "d_davinci = cloudpickle.load(open(\"./out/C2_davinci2_12744_1_16nr.pkl\", \"rb\"))\n",
    "d_gpt4 = cloudpickle.load(open(\"./out/ocm_GPT-4_12744_0_s.pkl\", \"rb\"))\n",
    "d_gpr = cloudpickle.load(open(\"./out/C2_gpr_12744_1_16nr.pkl\", \"rb\"))\n",
    "\n",
    "print(d_curie['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_curie = d_curie['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Curie is top{np.sum(raw_data[y_name] > best_curie)}: {best_curie}\")\n",
    "\n",
    "print(d_davinci['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "print(d_gpt4['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpt4 = d_gpt4['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "print(d_gpr['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpr = d_gpr['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "# print(np.sum(raw_data[y_name] > best))\n",
    "plt.xlabel(\"measured C$_2$ yield\")\n",
    "plt.axvline(best_curie, color='C4', linestyle='--', label=\"Curie\")\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"./out/figs/ocm_hist\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "d_gpt35 = cloudpickle.load(open(\"./out/ocm_gpt-3.5-turbo_1_1_0.1.pkl\", \"rb\"))\n",
    "d_gpt4o = cloudpickle.load(open(\"./out/ocm_gpt-4o_1_1_0.1.pkl\", \"rb\"))\n",
    "d_gpr   = cloudpickle.load(open(\"./out/ocm_gpr_5_1_0.1.pkl\", \"rb\"))\n",
    "\n",
    "v_gpt35 = d_gpt35['upper_confidence_bound'][:, :, 2].astype(float)\n",
    "v_gpt4o = d_gpt4o['upper_confidence_bound'][:, :, 2].astype(float)\n",
    "v_gpr   = d_gpr['upper_confidence_bound'][:, :, 2].astype(float)\n",
    "\n",
    "stat = stats.ttest_ind(v_gpt35, v_gpr)\n",
    "plt.plot([i for i in range(1,31)], stat.pvalue, label=\"gpt-3.5-turbo vs GPR\")\n",
    "stat = stats.ttest_ind(v_gpt4o, v_gpr)\n",
    "plt.plot([i for i in range(1,31)], stat.pvalue, label=\"gpt-4o vs GPR\")\n",
    "stat = stats.ttest_ind(v_gpt35, v_gpt4o)\n",
    "plt.plot([i for i in range(1,31)], stat.pvalue, label=\"gpt-3.5-turbo vs gpt-4o\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.xticks(np.arange(0, 31, 5))\n",
    "plt.axhline(0.05, color='r', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.55), ncol=1)\n",
    "plt.savefig(f\"./out/figs/ocm_bo_pvalue\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_davinci = cloudpickle.load(open(\"./out/sol_davinci_100.pkl\", \"rb\"))\n",
    "d_gpt4 = cloudpickle.load(open(\"./out/sol_gpt4_100.pkl\", \"rb\"))\n",
    "d_gpr = cloudpickle.load(open(\"./out/sol_GPR_100.pkl\", \"rb\"))\n",
    "\n",
    "print(d_davinci['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "print(d_gpt4['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpt4 = d_gpt4['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "print(d_gpr['expected_improvement'][:, -1, 2].astype(float))\n",
    "best_gpr = d_gpr['expected_improvement'][:, :, 2].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci-0.08])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(88)\n",
    "\n",
    "# data_path = \"./dataset/data/12744_ocm_dataset.csv\"\n",
    "# raw_data = pd.read_csv(data_path, sep=\";\")\n",
    "# # raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "# # raw_data['completion'] = - raw_data['completion']\n",
    "\n",
    "# x_name = \"prompt\"\n",
    "# y_name = \"completion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"ocm\", M=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(0,8)\n",
    "raw_data = pd.read_csv(\"dataset/data/1180_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[0], \"./out/C2_davinci_1180_1_tree.pkl\",\"20\", \n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, data_file_random=\"./out/C2 - random - 1180.pkl\")\n",
    "\n",
    "lim=(0,10)\n",
    "raw_data = pd.read_csv(\"dataset/data/2950_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[1], \"./out/C2_davinci_2950_1_tree.pkl\",\"50\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 2950.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/5900_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[2], \"./out/C2_davinci_5900_1_tree.pkl\", \"100\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 5900.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/12744_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[3], \"./out/C2_davinci_12744_1_tree_2.pkl\", \"216\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "fig.suptitle(\"TreePool with davinci\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = [\n",
    "    './out/C2_davinci_1180_1_tree.pkl',\n",
    "    './out/C2_davinci_2950_1_tree.pkl',\n",
    "    './out/C2_davinci_5900_1_tree.pkl',\n",
    "    './out/C2_davinci_12744_1_tree.pkl',\n",
    "]\n",
    "for p in pools:\n",
    "    print(p)\n",
    "    d = cloudpickle.load(open(p, \"rb\"))\n",
    "    for run in d['upper_confidence_bound'][:, :, 0]:\n",
    "        print([r[14:r.find(\",\")] for r in run])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(0,8)\n",
    "raw_data = pd.read_csv(\"dataset/data/1180_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[0], \"./out/C2_davinci_1180_1_16hh.pkl\",\"20\", \n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, data_file_random=\"./out/C2 - random - 1180.pkl\")\n",
    "\n",
    "lim=(0,10)\n",
    "raw_data = pd.read_csv(\"dataset/data/2950_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[1], \"./out/C2_davinci_2950_1_16hh.pkl\",\"50\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 2950.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/5900_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[2], \"./out/C2_davinci_5900_1_16hh.pkl\", \"100\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 5900.pkl\")\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/12744_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[3], \"./out/C2_davinci_12744_1_16hh.pkl\", \"216\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "         \n",
    "\n",
    "fig.suptitle(\"Subpool of 16 with half random samples with davinci\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = [\n",
    "    './out/C2_davinci_1180_1_16hh.pkl',\n",
    "    './out/C2_davinci_2950_1_16hh.pkl',\n",
    "    './out/C2_davinci_5900_1_16hh.pkl',\n",
    "    './out/C2_davinci_12744_1_16hh.pkl',\n",
    "]\n",
    "for p in pools:\n",
    "    print(p)\n",
    "    d = cloudpickle.load(open(p, \"rb\"))\n",
    "    for run in d['upper_confidence_bound'][:, :, 0]:\n",
    "        print([r[14:r.find(\",\")] for r in run])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16,4), constrained_layout=True)\n",
    "# for ax in axs.flat:\n",
    "#     ax.set_aspect(0.6)\n",
    "\n",
    "lim=(0,25)\n",
    "raw_data = pd.read_csv(\"dataset/data/12744_ocm_dataset.csv\", sep=\";\")\n",
    "plot_BO(axs[0], \"./out/C2_davinci_fulldataset_new_subpool_16_allrandom_1init.pkl\",\"all_random\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=True, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "plot_BO(axs[1], \"./out/C2_davinci_12744_1_16hh.pkl\",\"half_random\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "plot_BO(axs[2], \"./out/C2_davinci_fulldataset_subpool_16_no_random_1_init.pkl\", \"no_random\",\n",
    "# plot_BO(axs[2], \"./out/C2_davinci_fulldataset_subpool_16_no_random_newest_seed0_2_init.pkl\", \"no_random\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "\n",
    "plot_BO(axs[3], \"./out/C2_davinci_12744_1_tree_2.pkl\", \"TreePool\",\n",
    "         raw_data[y_name], \"C$_2$ yield\", lim, label=False, data_file_random=\"./out/C2 - random - 12744.pkl\")\n",
    "         \n",
    "\n",
    "fig.suptitle(\"216 samples with each subpool\")\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = [\n",
    "    # './out/C2_davinci_fulldataset_new_subpool_16_allrandom_1init.pkl',\n",
    "    # './out/C2_davinci_12744_1_16hh.pkl',\n",
    "    # './out/C2_davinci_fulldataset_subpool_16_no_random_1_init.pkl',\n",
    "    './out/C2_davinci_fulldataset_subpool_16_no_random_newest_seed0_2_init.pkl',\n",
    "    # './out/C2_davinci_12744_1_tree.pkl',\n",
    "]\n",
    "for p in pools:\n",
    "    print(p)\n",
    "    d = cloudpickle.load(open(p, \"rb\"))\n",
    "    for k in d.keys():\n",
    "        for run in d[k][:, :, 0]:\n",
    "            print(k, [r[14:r.find(\",\")] for r in run])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_davinci = cloudpickle.load(open(\"paper/out/C2_davinci_100.pkl\", \"rb\"))\n",
    "print(d_davinci['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "d_gpt4 = cloudpickle.load(open(\"paper/out/C2_GPT4_100.pkl\", \"rb\"))\n",
    "print(d_gpt4['upper_confidence_bound'][:, -1, 1].astype(float))\n",
    "best_gpt4 = d_gpt4['upper_confidence_bound'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "d_gpr = cloudpickle.load(open(\"paper/out/C2_GPR_100.pkl\", \"rb\"))\n",
    "print(d_gpr['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_gpr = d_gpr['upper_confidence_bound'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "# print(np.sum(raw_data[y_name] > best))\n",
    "plt.xlabel(\"measured C$_2$ yield\")\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_C2\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iupac-sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data_path = \"paper/data/esol_iupac.csv\"\n",
    "raw_data = pd.read_csv(data_path)\n",
    "raw_data = raw_data.dropna()\n",
    "raw_data = raw_data[[\"IUPAC\", \"measured log(solubility:mol/L)\"]]\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# raw_data['measured log(solubility:mol/L)'] = -raw_data['measured log(solubility:mol/L)']\n",
    "x_name = \"IUPAC\"\n",
    "y_name = \"measured log(solubility:mol/L)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, starts, indexes, x_name, y_name = get_dataset(\"sol\", M=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12,4), constrained_layout=True)\n",
    "for ax in axs.flat:\n",
    "    ax.set_aspect(1.8)\n",
    "\n",
    "lim=(-5.5,2)\n",
    "\n",
    "plot_BO(axs[0], \"paper/out/sol_davinci_100.pkl\", \"Topk - davinci\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=False, data_file_random=\"paper/out/sol - random.pkl\")\n",
    "plot_BO(axs[1], \"paper/out/sol_gpt4_100.pkl\", \"Topk - GPT-4\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=False, data_file_random=\"paper/out/sol - random.pkl\")\n",
    "plot_BO(axs[2], \"paper/out/sol_GPR_100.pkl\", \"GPR\",\n",
    "         raw_data[y_name], \"LogS solubility\", lim, label=True, data_file_random=\"paper/out/sol - random.pkl\")\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5,0),\n",
    "          fancybox=True, shadow=True, ncol=6)\n",
    "plt.savefig(f\"figs/BO_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_davinci = cloudpickle.load(open(\"paper/out/sol_davinci_100.pkl\", \"rb\"))\n",
    "d_gpt4 = cloudpickle.load(open(\"paper/out/sol_gpt4_100.pkl\", \"rb\"))\n",
    "d_gpr = cloudpickle.load(open(\"paper/out/sol_GPR_100.pkl\", \"rb\"))\n",
    "\n",
    "print(d_davinci['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_davinci = d_davinci['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"DaVinci is top{np.sum(raw_data[y_name] > best_davinci)}: {best_davinci}\")\n",
    "\n",
    "print(d_gpt4['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_gpt4 = d_gpt4['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"Gpt4 is top{np.sum(raw_data[y_name] > best_gpt4)}: {best_gpt4}\")\n",
    "\n",
    "print(d_gpr['expected_improvement'][:, -1, 1].astype(float))\n",
    "best_gpr = d_gpr['expected_improvement'][:, :, 1].astype(float).mean(axis=0)[-1]\n",
    "print(f\"GPR is top{np.sum(raw_data[y_name] > best_gpr)}: {best_gpr}\")\n",
    "\n",
    "sns.histplot(raw_data[y_name])\n",
    "plt.axvline(best_davinci, color='C1', linestyle='--', label=\"Davinci\")\n",
    "plt.axvline(best_gpt4, color='C2', linestyle='--', label=\"GPT4\")\n",
    "plt.axvline(best_gpr, color='C3', linestyle='--', label=\"GPR\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/hist_sol\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(raw_data[raw_data[y_name] > best_davinci-0.08])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
